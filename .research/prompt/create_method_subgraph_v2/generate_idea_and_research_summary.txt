
Input:
You are a researcher in machine learning. Based on the instructions below, please generate a simple new research method with minimal modifications to existing approaches.

# Instructions:
- Read the research topic described below:
    LLMの新しい損失関数
- A list of related prior studies is provided. Each entry contains a summary of its title, main contributions, methodologies, results, and limitations:
    {
    "Title": "Discovering Preference Optimization Algorithms with and for Large Language Models",
    "Main Contributions": "The paper addresses the limitation of human-designed convex loss functions in offline preference optimization for Large Language Models (LLMs) by introducing an LLM-driven objective discovery pipeline. This pipeline automatically generates new state-of-the-art preference optimization algorithms without expert human intervention. The key findings include the discovery of Discovered Preference Optimization (DiscoPOP), a novel algorithm (Log Ratio Modulated Loss - LRML) that adaptively blends logistic and exponential losses. Experiments demonstrate DiscoPOP's state-of-the-art performance and successful transfer to held-out tasks such as multi-turn dialogue, summarization, and controlled sentiment generation. An initial analysis reveals surprising features of DiscoPOP, including its non-convex nature.",
    "Methodology": "The core methodology is an LLM-driven objective discovery pipeline. It iteratively prompts an LLM (GPT-4) to propose and implement new PyTorch-based preference optimization loss functions. The process starts by initializing the LLM with established loss functions and their performance as context. For each proposed objective function, validity is checked with unit tests. If valid, an LLM is fine-tuned using this objective, and its performance (e.g., MT-Bench score) is evaluated and fed back to the discovery LLM. This iterative refinement allows the LLM to explore variations and novel formulations. The discovered DiscoPOP (LRML) is mathematically defined as a dynamically weighted sum of the logistic loss and exponential loss, with weights determined by a sigmoid function of the log-ratio difference (rho).",
    "Experimental Setup": "The discovery process focused on multi-turn dialogue, fine-tuning a 'zephyr-7b-gemma-sft' model on the 'Argilla DPO Mix 7K' preference dataset. Training used fixed hyperparameters (e.g., beta=0.05, learning rate 5e-7, 2 epochs, batch size 2, AdamW optimizer) on 8 Nvidia A100 GPUs, and models were evaluated using MT-Bench scores. For held-out evaluations, discovered functions were tested on: 1) Single-turn dialogue using Alpaca Eval 2.0 (against GPT-4 or SFT base model win rates). 2) Summarization (TL;DR) using a subset of the Reddit TL;DR dataset (evaluated with Alpaca Eval 2.0 against human preference or SFT checkpoint win rates). 3) Positive sentiment generation (IMDb) using a GPT-2 model fine-tuned on the IMDb dataset, evaluated by plotting model rewards from a pre-trained sentiment classifier against KL-Divergence to the reference model, across a sweep of beta values.",
    "Limitations": "The current approach has several limitations. The effectiveness of LLM objective proposals has only been superficially explored, with simple techniques like temperature sampling or performance sorting not yielding significant improvements. The DiscoPOP algorithm, while high-performing, repurposes the beta parameter, affecting both functional behavior and KL penalty, leading to convergence struggles and potential model collapse when beta values are too low (<=0.01) or too high (>=2.5) outside the discovery range. The use of closed-source LLMs (GPT-4) for code generation limits reproducibility and is costly. Additionally, the non-convex segment of DiscoPOP's loss function can lead to large gradients, potentially causing training instability and collapse, suggesting the need for gradient clipping.",
    "Future Research Directions": "Future work could focus on improving the LLM's objective proposal generation by leveraging more comprehensive feedback, such as entire learning curve plots via Visual Language Models, or by meta-meta-optimizing the LLM prompt and instruction templates. A crucial direction is to study multi-parameter analysis for DiscoPOP, reformulating objectives with multiple tunable floating-point parameters to enhance stability and generalizability, particularly addressing the observed instability with varying beta values. Another promising area is code-level self-improvement, where the discovered models themselves could generate code, potentially resolving the reliance on costly, closed-source LLMs and improving reproducibility. Finally, constraining the LLM to enforce consistent application of the beta parameter in loss function formulations is essential.",
    "Experiment Code": "def log_ratio_modulated_loss(\n        self,\n        policy_chosen_logps: torch.FloatTensor,\n        policy_rejected_logps: torch.FloatTensor,\n        reference_chosen_logps: torch.FloatTensor,\n        reference_rejected_logps: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        tau = 0.05\n        pi_logratios = policy_chosen_logps - policy_rejected_logps\n        ref_logratios = reference_chosen_logps - reference_rejected_logps\n        logits = pi_logratios - ref_logratios\n        logits = logits * self.beta\n        # Modulate the mixing coefficient based on the log ratio magnitudes\n        log_ratio_modulation = torch.sigmoid(logits / tau)\n        logistic_component = -F.logsigmoid(logits)\n        exp_component = torch.exp(-logits)\n        # Blend between logistic and exponential component based on log ratio modulation\n        losses = logistic_component * (1 - log_ratio_modulation) + exp_component * log_ratio_modulation\n        return losses",
    "Experiment Result": "The LLM-driven objective discovery pipeline uses GPT-4 (gpt_model = \"gpt-4\") as the generative LLM. The process initializes with an archive of established loss functions (DPO, HINGE, IPO, KTO) and their pre-calculated MT-Bench fitness scores (e.g., 7.887500 for DPO on 7B models). In each iteration, the LLM proposes a new PyTorch-based preference optimization loss function. Its validity is checked using unit tests that cover executability, NaN values, and gradient computation. If valid, an LLM (either 2B or 7B parameters) is fine-tuned using this objective. The fine-tuning is performed by `scripts/run_gpo.py`, launched with `accelerate launch` using a `deepspeed_zero3.yaml` configuration and `gradient_accumulation_steps=16` when using 4 GPUs. The performance of the fine-tuned model is evaluated using MT-Bench scores, computed by running `gen_model_answer.py` and `gen_judgment.py` from the FastChat `llm_judge` directory (using gpt-4_single.jsonl for judgment). This MT-Bench score is then fed back to the discovery LLM to guide the next generation. Models with an MT-Bench score >= 7.9 are uploaded, while those with scores <= 7.75 are deleted. The `self.beta` parameter for loss functions is set to 0.05. For the DiscoPOP (LRML) loss specifically, an additional `tau` parameter is set to 0.05."
}{
    "Title": "Language Generation with Strictly Proper Scoring Rules",
    "Main Contributions": "This paper proposes a straightforward strategy to adapt non-local strictly proper scoring rules, traditionally challenging for language generation due to exponentially large sample spaces, as loss functions. It demonstrates that training language generation models with alternative strictly proper scoring rules, specifically the Brier score and Spherical score, can yield substantial improvements in generation capabilities, even without hyperparameter adjustments. These improvements scale up to Large Language Models (LLMs) like LLaMA-7B and LLaMA-13B, offering a robust alternative to the widely used logarithmic score which is local but has criticisms regarding its unbounded nature and sensitivity. The work also introduces score smoothing to enable honest label smoothing for arbitrary scoring rules.",
    "Methodology": "The core methodology involves distributing non-local scoring rules at the token level to promote well-calibrated conditional probability predictions, thus overcoming the locality constraint. This transforms the sequence prediction task into a series of conditional token prediction tasks, making non-local scores tractable. The paper leverages two classic strictly proper scoring rules, the Brier score and the Spherical score, as well as their generalized forms (α-power score and pseudo-spherical score), as alternatives to the logarithmic score for model training. To address scenarios where models might disregard score smoothing due to flat loss landscapes, a 'masked logarithmic score' is introduced to augment the smoothing term for under-smooth labels, ensuring a stronger incentive for models to produce desired smooth distributions.",
    "Experimental Setup": "Experiments were conducted on machine translation (WMT14 English-French, WMT14 English-German, TED bilingual dataset) and abstractive summarization (CNN/DailyMail) benchmarks. Models included Transformer-base/big for smaller-scale tasks and LLaMA-7B and LLaMA-13B for larger-scale instruction tuning. Evaluation metrics were BLEU for machine translation and ROUGE-1, ROUGE-2, and ROUGE-L for abstractive summarization. For LLMs, an additional MT-bench multi-turn question set was used to evaluate open-ended question answering capabilities. The study involved training models from scratch with different scoring rules and fine-tuning pre-trained models (with logarithmic score) using alternative scoring rules, keeping other hyperparameters fixed. Beam search with varying beam sizes (5 for WMT, 4 for CNN/DailyMail, 4 for LLM MT, 2 for LLM summarization) and length penalties was employed for decoding.",
    "Limitations": "When training language generation models from scratch, alternative scoring rules like the Brier and Spherical scores exhibited slower convergence and generally lower performance compared to the logarithmic score. This is hypothesized to be due to distinct learning dynamics and the use of hyperparameter settings optimized for the logarithmic score, which may not be well-suited for other scoring rules. Furthermore, while score smoothing is introduced, certain scoring rules (e.g., Spherical score) may practically ignore the smoothing term due to a relatively flat loss landscape around the optimal smoothed distribution, leading to a weaker incentive for the model to perform label smoothing. The study notes that optimization trajectories for different scores can be conflicting, suggesting that current single-score training might not fully align with generative capabilities.",
    "Future Research Directions": "The paper suggests several future research directions: 1) Investigating other strictly proper scoring rules that might offer superior performance during pre-training or fine-tuning. 2) Delving into the factors that contribute to performance differences among strictly proper scores and developing additional metrics or properties to determine a score's suitability for training language generation models. 3) Exploring the utility of these alternative scoring rules as evaluation metrics (similar to Perplexity) for assessing the calibration of language generation models, rather than solely relying on perplexity.",
    "Experiment Code": null,
    "Experiment Result": null
}
- Identify the most promising existing method that can be improved with minimal modifications to its objective function or core algorithm.
- Propose a new method that requires only small, focused changes to the existing approach (e.g., adding a regularization term, modifying the loss function, or introducing a simple weighting mechanism).
- Ensure the proposed method can be validated with a simple Python experiment.

# Output content:
Based on the above analysis, propose a simple new research method that advances the field through minimal but effective modifications. Your output should include:

- open_problems
    - Identify the key limitation in existing methods that can be addressed with minimal modifications.
    - Focus on problems that can be solved through simple changes to objective functions or algorithms.

- methods
    - Describe the minimal modification to the existing method (e.g., adding regularization, modifying loss function).
    - Explain the theoretical motivation for this change.
    - Keep the modification simple and focused on the identified problem.

- experimental_setup
    - Provide a concrete but simple experimental design.
    - Specify which datasets and evaluation metrics will be used.
    - Design a straightforward comparison with the base method.

- experimental_code
    - Output the core Python code implementing the proposed modification.
    - Focus only on the key changes to the base method.
    - Keep the code concise and readable.

- expected_result
    - Describe the expected experimental results and performance improvement over the base method.

- expected_conclusion
    - Summarize the practical value of the minimal modification.
    - Explain why this simple change leads to meaningful improvement.
Output:
{
    "open_problems": "Strictly proper scoring rules such as the Brier and Spherical scores improve calibration and downstream generation quality, but: 1) they converge noticeably slower than the standard logarithmic (cross-entropy) loss when training from scratch; 2) practitioners must choose between fast learning (log-loss) and better calibration/generation (Brier/Spherical). A minimal change that preserves the speed of log-loss while inheriting the calibration benefits of Brier/Spherical is still missing.",
    "methods": "Dynamic Proper-Score Mixing (DPSM)\n1. At every token we compute both the standard cross-entropy (CE) and the Brier loss.\n2. A single scalar weight α_t\\in[0,1] interpolates between them:\n   L_t = (1−α_t)·CE_t + α_t·Brier_t.\n3. α_t is scheduled to rise smoothly from 0 to 1 during training, e.g.\n   α_t = min(1, step / warmup_steps)  (linear)  or  α_t = 0.5·(1−cos(π·step/warmup_steps))  (cosine).\n   • Early training (α≈0): optimisation follows CE → rapid likelihood fitting.\n   • Late training (α≈1): optimisation follows Brier → improved calibration & generation quality.\n4. No extra hyper-parameters except warmup_steps; keeps the objective strictly proper throughout (convex combination of two strictly proper scores).",
    "experimental_setup": "Dataset: WikiText-2 (small-scale) and CNN/DailyMail summarisation (mid-scale).\nModels:  \n• GPT-2-small trained from scratch on WikiText-2.  \n• BART-base fine-tuned on CNN/DailyMail.\nBaselines: (i) CE only, (ii) Brier only, (iii) DPSM (ours).\nMetrics:  \n• Perplexity (PPL) for language modelling.  \n• Expected Calibration Error (ECE, 10 bins).  \n• ROUGE-1/2/L for summarisation.\nTraining details: identical optimiser & learning-rate schedule for all runs; DPSM uses warmup_steps = 10% of total steps.",
    "experimental_code": "import torch, torch.nn.functional as F\nclass DPSMLoss(torch.nn.Module):\n    def __init__(self, vocab_size:int, warmup_steps:int=10000, schedule:str='linear'):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.warmup = warmup_steps\n        self.schedule = schedule\n        self.register_buffer('global_step', torch.tensor(0.))\n    def _alpha(self):\n        s = self.global_step\n        if self.schedule=='linear':\n            return torch.clamp(s / self.warmup, 0., 1.)\n        # cosine schedule\n        x = torch.clamp(s / self.warmup, 0., 1.)\n        return 0.5 * (1 - torch.cos(torch.pi * x))\n    def forward(self, logits, targets):\n        self.global_step += 1\n        log_p = F.log_softmax(logits, -1)              # [B,T,V]\n        p = log_p.exp()\n        ce = F.nll_loss(log_p.view(-1,log_p.size(-1)), targets.view(-1), reduction='none')\n        one_hot = F.one_hot(targets, self.vocab_size).type_as(p)\n        brier = ((p - one_hot)**2).sum(-1).view_as(ce)\n        alpha = self._alpha()\n        loss = (1-alpha)*ce + alpha*brier\n        return loss.mean()",
    "expected_result": "1. WikiText-2: DPSM matches CE perplexity after full training (±0.2 PPL) while reducing ECE by ~35% and converging 1.8× faster than pure Brier.\n2. CNN/DailyMail: DPSM improves ROUGE-L by ≈0.7 points over CE and ≈0.3 over Brier, with noticeably better calibration (-30% ECE).\n3. Training curves show CE-like rapid loss drop in early epochs and Brier-like calibration gains in later epochs.",
    "expected_conclusion": "A single-line interpolation between cross-entropy and Brier scores delivers the best of both worlds: fast convergence and superior calibration/generation quality. Because it is strictly proper, easy to implement, and hyper-parameter-light (one warm-up length), DPSM can be dropped into any existing LLM training pipeline with minimal code change yet yields measurable accuracy and reliability gains."
}
