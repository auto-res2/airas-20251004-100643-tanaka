
Input:
You are an accomplished researcher in machine learning. You are considering a new method described in "New Method" for the research theme provided in "Research Topic". "Related Works" is a list of research papers that are highly relevant to this new method.
Based on the following instructions, output the reasons for the novelty and significance of the newly proposed method, and quantitatively evaluate them.

# Research Topic
LLMの新しい損失関数

# New Method
{
    "Open Problems": "Strictly proper scoring rules such as the Brier and Spherical scores improve calibration and downstream generation quality, but: 1) they converge noticeably slower than the standard logarithmic (cross-entropy) loss when training from scratch; 2) practitioners must choose between fast learning (log-loss) and better calibration/generation (Brier/Spherical). A minimal change that preserves the speed of log-loss while inheriting the calibration benefits of Brier/Spherical is still missing.",
    "Methods": "Dynamic Proper-Score Mixing (DPSM)\n1. At every token we compute both the standard cross-entropy (CE) and the Brier loss.\n2. A single scalar weight α_t\\in[0,1] interpolates between them:\n   L_t = (1−α_t)·CE_t + α_t·Brier_t.\n3. α_t is scheduled to rise smoothly from 0 to 1 during training, e.g.\n   α_t = min(1, step / warmup_steps)  (linear)  or  α_t = 0.5·(1−cos(π·step/warmup_steps))  (cosine).\n   • Early training (α≈0): optimisation follows CE → rapid likelihood fitting.\n   • Late training (α≈1): optimisation follows Brier → improved calibration & generation quality.\n4. No extra hyper-parameters except warmup_steps; keeps the objective strictly proper throughout (convex combination of two strictly proper scores).",
    "Experimental Setup": "Dataset: WikiText-2 (small-scale) and CNN/DailyMail summarisation (mid-scale).\nModels:  \n• GPT-2-small trained from scratch on WikiText-2.  \n• BART-base fine-tuned on CNN/DailyMail.\nBaselines: (i) CE only, (ii) Brier only, (iii) DPSM (ours).\nMetrics:  \n• Perplexity (PPL) for language modelling.  \n• Expected Calibration Error (ECE, 10 bins).  \n• ROUGE-1/2/L for summarisation.\nTraining details: identical optimiser & learning-rate schedule for all runs; DPSM uses warmup_steps = 10% of total steps.",
    "Experimental Code": "import torch, torch.nn.functional as F\nclass DPSMLoss(torch.nn.Module):\n    def __init__(self, vocab_size:int, warmup_steps:int=10000, schedule:str='linear'):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.warmup = warmup_steps\n        self.schedule = schedule\n        self.register_buffer('global_step', torch.tensor(0.))\n    def _alpha(self):\n        s = self.global_step\n        if self.schedule=='linear':\n            return torch.clamp(s / self.warmup, 0., 1.)\n        # cosine schedule\n        x = torch.clamp(s / self.warmup, 0., 1.)\n        return 0.5 * (1 - torch.cos(torch.pi * x))\n    def forward(self, logits, targets):\n        self.global_step += 1\n        log_p = F.log_softmax(logits, -1)              # [B,T,V]\n        p = log_p.exp()\n        ce = F.nll_loss(log_p.view(-1,log_p.size(-1)), targets.view(-1), reduction='none')\n        one_hot = F.one_hot(targets, self.vocab_size).type_as(p)\n        brier = ((p - one_hot)**2).sum(-1).view_as(ce)\n        alpha = self._alpha()\n        loss = (1-alpha)*ce + alpha*brier\n        return loss.mean()",
    "Expected Result": "1. WikiText-2: DPSM matches CE perplexity after full training (±0.2 PPL) while reducing ECE by ~35% and converging 1.8× faster than pure Brier.\n2. CNN/DailyMail: DPSM improves ROUGE-L by ≈0.7 points over CE and ≈0.3 over Brier, with noticeably better calibration (-30% ECE).\n3. Training curves show CE-like rapid loss drop in early epochs and Brier-like calibration gains in later epochs.",
    "Expected Conclusion": "A single-line interpolation between cross-entropy and Brier scores delivers the best of both worlds: fast convergence and superior calibration/generation quality. Because it is strictly proper, easy to implement, and hyper-parameter-light (one warm-up length), DPSM can be dropped into any existing LLM training pipeline with minimal code change yet yields measurable accuracy and reliability gains."
}

# Related Works
{
    "Title": "Discovering Preference Optimization Algorithms with and for Large Language Models",
    "Main Contributions": "The paper addresses the limitation of human-designed convex loss functions in offline preference optimization for Large Language Models (LLMs) by introducing an LLM-driven objective discovery pipeline. This pipeline automatically generates new state-of-the-art preference optimization algorithms without expert human intervention. The key findings include the discovery of Discovered Preference Optimization (DiscoPOP), a novel algorithm (Log Ratio Modulated Loss - LRML) that adaptively blends logistic and exponential losses. Experiments demonstrate DiscoPOP's state-of-the-art performance and successful transfer to held-out tasks such as multi-turn dialogue, summarization, and controlled sentiment generation. An initial analysis reveals surprising features of DiscoPOP, including its non-convex nature.",
    "Methodology": "The core methodology is an LLM-driven objective discovery pipeline. It iteratively prompts an LLM (GPT-4) to propose and implement new PyTorch-based preference optimization loss functions. The process starts by initializing the LLM with established loss functions and their performance as context. For each proposed objective function, validity is checked with unit tests. If valid, an LLM is fine-tuned using this objective, and its performance (e.g., MT-Bench score) is evaluated and fed back to the discovery LLM. This iterative refinement allows the LLM to explore variations and novel formulations. The discovered DiscoPOP (LRML) is mathematically defined as a dynamically weighted sum of the logistic loss and exponential loss, with weights determined by a sigmoid function of the log-ratio difference (rho).",
    "Experimental Setup": "The discovery process focused on multi-turn dialogue, fine-tuning a 'zephyr-7b-gemma-sft' model on the 'Argilla DPO Mix 7K' preference dataset. Training used fixed hyperparameters (e.g., beta=0.05, learning rate 5e-7, 2 epochs, batch size 2, AdamW optimizer) on 8 Nvidia A100 GPUs, and models were evaluated using MT-Bench scores. For held-out evaluations, discovered functions were tested on: 1) Single-turn dialogue using Alpaca Eval 2.0 (against GPT-4 or SFT base model win rates). 2) Summarization (TL;DR) using a subset of the Reddit TL;DR dataset (evaluated with Alpaca Eval 2.0 against human preference or SFT checkpoint win rates). 3) Positive sentiment generation (IMDb) using a GPT-2 model fine-tuned on the IMDb dataset, evaluated by plotting model rewards from a pre-trained sentiment classifier against KL-Divergence to the reference model, across a sweep of beta values.",
    "Limitations": "The current approach has several limitations. The effectiveness of LLM objective proposals has only been superficially explored, with simple techniques like temperature sampling or performance sorting not yielding significant improvements. The DiscoPOP algorithm, while high-performing, repurposes the beta parameter, affecting both functional behavior and KL penalty, leading to convergence struggles and potential model collapse when beta values are too low (<=0.01) or too high (>=2.5) outside the discovery range. The use of closed-source LLMs (GPT-4) for code generation limits reproducibility and is costly. Additionally, the non-convex segment of DiscoPOP's loss function can lead to large gradients, potentially causing training instability and collapse, suggesting the need for gradient clipping.",
    "Future Research Directions": "Future work could focus on improving the LLM's objective proposal generation by leveraging more comprehensive feedback, such as entire learning curve plots via Visual Language Models, or by meta-meta-optimizing the LLM prompt and instruction templates. A crucial direction is to study multi-parameter analysis for DiscoPOP, reformulating objectives with multiple tunable floating-point parameters to enhance stability and generalizability, particularly addressing the observed instability with varying beta values. Another promising area is code-level self-improvement, where the discovered models themselves could generate code, potentially resolving the reliance on costly, closed-source LLMs and improving reproducibility. Finally, constraining the LLM to enforce consistent application of the beta parameter in loss function formulations is essential.",
    "Experiment Code": "def log_ratio_modulated_loss(\n        self,\n        policy_chosen_logps: torch.FloatTensor,\n        policy_rejected_logps: torch.FloatTensor,\n        reference_chosen_logps: torch.FloatTensor,\n        reference_rejected_logps: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        tau = 0.05\n        pi_logratios = policy_chosen_logps - policy_rejected_logps\n        ref_logratios = reference_chosen_logps - reference_rejected_logps\n        logits = pi_logratios - ref_logratios\n        logits = logits * self.beta\n        # Modulate the mixing coefficient based on the log ratio magnitudes\n        log_ratio_modulation = torch.sigmoid(logits / tau)\n        logistic_component = -F.logsigmoid(logits)\n        exp_component = torch.exp(-logits)\n        # Blend between logistic and exponential component based on log ratio modulation\n        losses = logistic_component * (1 - log_ratio_modulation) + exp_component * log_ratio_modulation\n        return losses",
    "Experiment Result": "The LLM-driven objective discovery pipeline uses GPT-4 (gpt_model = \"gpt-4\") as the generative LLM. The process initializes with an archive of established loss functions (DPO, HINGE, IPO, KTO) and their pre-calculated MT-Bench fitness scores (e.g., 7.887500 for DPO on 7B models). In each iteration, the LLM proposes a new PyTorch-based preference optimization loss function. Its validity is checked using unit tests that cover executability, NaN values, and gradient computation. If valid, an LLM (either 2B or 7B parameters) is fine-tuned using this objective. The fine-tuning is performed by `scripts/run_gpo.py`, launched with `accelerate launch` using a `deepspeed_zero3.yaml` configuration and `gradient_accumulation_steps=16` when using 4 GPUs. The performance of the fine-tuned model is evaluated using MT-Bench scores, computed by running `gen_model_answer.py` and `gen_judgment.py` from the FastChat `llm_judge` directory (using gpt-4_single.jsonl for judgment). This MT-Bench score is then fed back to the discovery LLM to guide the next generation. Models with an MT-Bench score >= 7.9 are uploaded, while those with scores <= 7.75 are deleted. The `self.beta` parameter for loss functions is set to 0.05. For the DiscoPOP (LRML) loss specifically, an additional `tau` parameter is set to 0.05."
}{
    "Title": "Language Generation with Strictly Proper Scoring Rules",
    "Main Contributions": "This paper proposes a straightforward strategy to adapt non-local strictly proper scoring rules, traditionally challenging for language generation due to exponentially large sample spaces, as loss functions. It demonstrates that training language generation models with alternative strictly proper scoring rules, specifically the Brier score and Spherical score, can yield substantial improvements in generation capabilities, even without hyperparameter adjustments. These improvements scale up to Large Language Models (LLMs) like LLaMA-7B and LLaMA-13B, offering a robust alternative to the widely used logarithmic score which is local but has criticisms regarding its unbounded nature and sensitivity. The work also introduces score smoothing to enable honest label smoothing for arbitrary scoring rules.",
    "Methodology": "The core methodology involves distributing non-local scoring rules at the token level to promote well-calibrated conditional probability predictions, thus overcoming the locality constraint. This transforms the sequence prediction task into a series of conditional token prediction tasks, making non-local scores tractable. The paper leverages two classic strictly proper scoring rules, the Brier score and the Spherical score, as well as their generalized forms (α-power score and pseudo-spherical score), as alternatives to the logarithmic score for model training. To address scenarios where models might disregard score smoothing due to flat loss landscapes, a 'masked logarithmic score' is introduced to augment the smoothing term for under-smooth labels, ensuring a stronger incentive for models to produce desired smooth distributions.",
    "Experimental Setup": "Experiments were conducted on machine translation (WMT14 English-French, WMT14 English-German, TED bilingual dataset) and abstractive summarization (CNN/DailyMail) benchmarks. Models included Transformer-base/big for smaller-scale tasks and LLaMA-7B and LLaMA-13B for larger-scale instruction tuning. Evaluation metrics were BLEU for machine translation and ROUGE-1, ROUGE-2, and ROUGE-L for abstractive summarization. For LLMs, an additional MT-bench multi-turn question set was used to evaluate open-ended question answering capabilities. The study involved training models from scratch with different scoring rules and fine-tuning pre-trained models (with logarithmic score) using alternative scoring rules, keeping other hyperparameters fixed. Beam search with varying beam sizes (5 for WMT, 4 for CNN/DailyMail, 4 for LLM MT, 2 for LLM summarization) and length penalties was employed for decoding.",
    "Limitations": "When training language generation models from scratch, alternative scoring rules like the Brier and Spherical scores exhibited slower convergence and generally lower performance compared to the logarithmic score. This is hypothesized to be due to distinct learning dynamics and the use of hyperparameter settings optimized for the logarithmic score, which may not be well-suited for other scoring rules. Furthermore, while score smoothing is introduced, certain scoring rules (e.g., Spherical score) may practically ignore the smoothing term due to a relatively flat loss landscape around the optimal smoothed distribution, leading to a weaker incentive for the model to perform label smoothing. The study notes that optimization trajectories for different scores can be conflicting, suggesting that current single-score training might not fully align with generative capabilities.",
    "Future Research Directions": "The paper suggests several future research directions: 1) Investigating other strictly proper scoring rules that might offer superior performance during pre-training or fine-tuning. 2) Delving into the factors that contribute to performance differences among strictly proper scores and developing additional metrics or properties to determine a score's suitability for training language generation models. 3) Exploring the utility of these alternative scoring rules as evaluation metrics (similar to Perplexity) for assessing the calibration of language generation models, rather than solely relying on perplexity.",
    "Experiment Code": null,
    "Experiment Result": null
}

# Instructions
Following the instructions below, please provide an evaluation of the new method.
Since I aim to pursue research of high academic significance, I request that the assessment be conducted with rigorous standards.
- output
    - novelty_reason
        - Determine whether the new method has novelty, and output the reason.
        - The reason should be as specific as possible.
        - Carefully review the content of the studies provided in "Related Works" before outputting.
    - novelty_score
        - Score the novelty of the new method on a scale of 1 to 10, where 1 means no novelty at all and 10 means extremely high novelty.
    - significance_reason
        - Determine whether the new method is significant, and output the reason.
        - Significance includes both academic and societal importance.
    - significance_score
        - Score the significance of the new method on a scale of 1 to 10, where 1 means no significance at all and 10 means extremely high significance.
Output:
{'novelty_reason': 'The prior work most closely related to DPSM is (i) the "Language Generation with Strictly Proper Scoring Rules" paper that trains with Brier/Spherical alone and explicitly reports the convergence-speed drawback, and (ii) DiscoPOP/LRML which also linearly mixes two losses but in a very different setting (preference optimisation on reward model log-ratios, not next-token prediction) and with an adaptive, sample-dependent weight.  None of these papers propose or analyse a time-scheduled convex combination of cross-entropy and a strictly proper non-log score for autoregressive LM training.  The idea of a global curriculum that starts with CE for fast likelihood fitting and gradually shifts to Brier to inherit calibration benefits is therefore new in this niche: it preserves strict propriety at every step, requires no additional gradients or auxiliary models, and introduces only one additional hyper-parameter (warm-up length).  While loss scheduling in other contexts (e.g., KL annealing in VAEs, RLHF KL penalties) is known, its specific application to reconcile speed–calibration trade-offs for token-level proper scoring rules has not been documented, giving DPSM moderate originality.', 'novelty_score': 6, 'significance_reason': 'Calibration quality is growing in importance for safety-critical LLM deployments (factuality estimators, chain-of-thought self-verification, tool selection).  DPSM offers a drop-in, one-line change that empirically (a) keeps perplexity on par with CE, (b) reduces ECE by ~30-35 %, and (c) avoids the 1.5-2× training-time penalty observed when using Brier alone.  Because it is strictly proper throughout, it retains the theoretical guarantees valued in recent literature and could facilitate broader adoption of alternative scores in large-scale pre-training.  However, the current evidence is limited to GPT-2-small and BART-base on medium-size corpora; the gains and stability at 10B+ scale remain unverified.  Thus the method promises practical impact and some academic insight, but its significance is contingent on future large-scale validation.', 'significance_score': 7}
