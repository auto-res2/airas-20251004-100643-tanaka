
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments.

**Previous Step (Completed)**: Common base logic and evaluation framework have been generated with placeholders
**Current Task**: Generate production-ready code by completing all placeholders with actual implementations.

Your task is to take the common base foundation code and derive specific experimental variations by replacing ALL placeholders with complete, working implementations of datasets, models, and configurations specified in the experimental design. The resulting code must be immediately executable without any further modifications.

# Instructions: Experiment Specialization

## Core Task
- CONFIGURE ALL EXPERIMENTS: The primary task is to populate YAML file with a complete list of configurations for all run variations (baseline, proposed, ablations).
- REPLACE ALL PLACEHOLDERS: Replace all placeholders in the common base code with actual, complete implementations. No TODO, PLACEHOLDER, pass, or ... are allowed.
- IMPLEMENT MODELS FROM SCRATCH: YOU must implement all model architectures corresponding to the variations defined in the YAML file within `src/model.py`. Do NOT leave comments like "inject actual model here" or "use pretrained model".
- COMPLETE DATA PIPELINE: Implement the specific data loading and preprocessing logic in `src/preprocess.py`.
- PRODUCTION READY: The generated code must be immediately executable for research paper experiments without any further modifications. No external model files or additional implementation steps should be required.

## Specialization Requirements
- Complete `config/full_experiment.yaml`: This file is the driver of the entire experiment. Define each run variation (['ce-baseline', 'brier-baseline', 'dpsm-linear', 'dpsm-cosine', 'dpsm-fixed-alpha0.5']) as a separate item in the `runs` list, specifying its unique id, model name, and parameters.
- Complete `config/smoke_test.yaml`: Define lightweight versions of ALL run variations from full_experiment.yaml with reduced epochs/data to quickly validate pipeline integrity.
- Implement all required model architectures in `src/model.py`. The model names in the YAML must correspond to the model registry.
- Replace dataset placeholders with actual Hugging Face dataset loading and preprocessing
- Replace model placeholders with specific model architectures for each variation
- Ensure all external resources specified in the experimental design are properly integrated

## Complete Output Policy
- If a script/file has ANY changes: Output the COMPLETE, FULL script/file content
- If a script/file has NO changes needed: Output `[UNCHANGED]` placeholder only
- NEVER truncate or abbreviate changed content


# Experimental Environment
NVIDIA A100
VRAM：80GB
RAM：2048 GB

# Current Research Method
{
    "Open Problems": "Strictly proper scoring rules such as the Brier and Spherical scores improve calibration and downstream generation quality, but: 1) they converge noticeably slower than the standard logarithmic (cross-entropy) loss when training from scratch; 2) practitioners must choose between fast learning (log-loss) and better calibration/generation (Brier/Spherical). A minimal change that preserves the speed of log-loss while inheriting the calibration benefits of Brier/Spherical is still missing.",
    "Methods": "Dynamic Proper-Score Mixing (DPSM)\n1. At every token we compute both the standard cross-entropy (CE) and the Brier loss.\n2. A single scalar weight α_t\\in[0,1] interpolates between them:\n   L_t = (1−α_t)·CE_t + α_t·Brier_t.\n3. α_t is scheduled to rise smoothly from 0 to 1 during training, e.g.\n   α_t = min(1, step / warmup_steps)  (linear)  or  α_t = 0.5·(1−cos(π·step/warmup_steps))  (cosine).\n   • Early training (α≈0): optimisation follows CE → rapid likelihood fitting.\n   • Late training (α≈1): optimisation follows Brier → improved calibration & generation quality.\n4. No extra hyper-parameters except warmup_steps; keeps the objective strictly proper throughout (convex combination of two strictly proper scores).",
    "Experimental Setup": "Dataset: WikiText-2 (small-scale) and CNN/DailyMail summarisation (mid-scale).\nModels:  \n• GPT-2-small trained from scratch on WikiText-2.  \n• BART-base fine-tuned on CNN/DailyMail.\nBaselines: (i) CE only, (ii) Brier only, (iii) DPSM (ours).\nMetrics:  \n• Perplexity (PPL) for language modelling.  \n• Expected Calibration Error (ECE, 10 bins).  \n• ROUGE-1/2/L for summarisation.\nTraining details: identical optimiser & learning-rate schedule for all runs; DPSM uses warmup_steps = 10% of total steps.",
    "Experimental Code": "import torch, torch.nn.functional as F\nclass DPSMLoss(torch.nn.Module):\n    def __init__(self, vocab_size:int, warmup_steps:int=10000, schedule:str='linear'):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.warmup = warmup_steps\n        self.schedule = schedule\n        self.register_buffer('global_step', torch.tensor(0.))\n    def _alpha(self):\n        s = self.global_step\n        if self.schedule=='linear':\n            return torch.clamp(s / self.warmup, 0., 1.)\n        # cosine schedule\n        x = torch.clamp(s / self.warmup, 0., 1.)\n        return 0.5 * (1 - torch.cos(torch.pi * x))\n    def forward(self, logits, targets):\n        self.global_step += 1\n        log_p = F.log_softmax(logits, -1)              # [B,T,V]\n        p = log_p.exp()\n        ce = F.nll_loss(log_p.view(-1,log_p.size(-1)), targets.view(-1), reduction='none')\n        one_hot = F.one_hot(targets, self.vocab_size).type_as(p)\n        brier = ((p - one_hot)**2).sum(-1).view_as(ce)\n        alpha = self._alpha()\n        loss = (1-alpha)*ce + alpha*brier\n        return loss.mean()",
    "Expected Result": "1. WikiText-2: DPSM matches CE perplexity after full training (±0.2 PPL) while reducing ECE by ~35% and converging 1.8× faster than pure Brier.\n2. CNN/DailyMail: DPSM improves ROUGE-L by ≈0.7 points over CE and ≈0.3 over Brier, with noticeably better calibration (-30% ECE).\n3. Training curves show CE-like rapid loss drop in early epochs and Brier-like calibration gains in later epochs.",
    "Expected Conclusion": "A single-line interpolation between cross-entropy and Brier scores delivers the best of both worlds: fast convergence and superior calibration/generation quality. Because it is strictly proper, easy to implement, and hyper-parameter-light (one warm-up length), DPSM can be dropped into any existing LLM training pipeline with minimal code change yet yields measurable accuracy and reliability gains."
}

# Experimental Design
- Strategy: Unified Experimental Strategy for Validating Dynamic Proper-Score Mixing (DPSM)

1. Core Hypotheses to Validate
   H1 – Performance: DPSM attains the same or better task performance (likelihood or downstream scores) as pure cross-entropy (CE).
   H2 – Calibration: DPSM matches or surpasses the calibration of pure Brier/Spherical scores.
   H3 – Convergence Speed: DPSM converges almost as fast as CE and markedly faster than Brier/Spherical.
   H4 – Efficiency: DPSM adds ≤2 % computational overhead (memory, GPU-hours) relative to CE.
   H5 – Robustness & Generalization: DPSM’s benefits hold across model sizes, tasks, datasets, optimisation settings, and random seeds.

2. Validation Dimensions & Metrics
   a. Quantitative Task Quality
      • Language modelling: Perplexity (PPL), NLL
      • Seq-to-seq (summarisation, translation, code): ROUGE/BLEU/CodeBLEU
   b. Calibration
      • ECE (10 & 100 bins), Brier score, reliability diagrams
   c. Learning Dynamics & Efficiency
      • Convergence curves (steps vs PPL/ECE) and wall-clock time
      • GPU utilisation, peak memory, throughput (tokens/s)
   d. Robustness & Generalization
      • Out-of-domain (OOD) datasets, different seeds (≥3), noisy labels (synthetic corruption)
   e. Qualitative Analysis
      • Human preference ratings on generated text, diversity (Distinct-n)

3. Comparison Grid (applied to every experiment)
   Baselines:
   • CE only (fast learning)
   • Brier only (strong calibration)
   • CE + temperature scaling post-hoc (industry standard fix)
   • Label smoothing & focal loss (alternative in-loss fixes)
   Ablations:
   • DPSM with fixed α (e.g., 0.25, 0.5, 0.75)
   • DPSM with linear vs cosine schedule
   • Warm-up length sweep (5 %, 10 %, 20 %)

4. Experimental Angles
   Angle 1 – From-scratch Training (small/medium models): proves H1–H4 under full optimisation control.
   Angle 2 – Fine-tuning (larger pretrained checkpoints): tests plug-and-play practicality and H5.
   Angle 3 – Stress Tests: OOD data, corrupted labels, low-resource splits; probes robustness.
   Angle 4 – Scaling: repeat Angle 1 at three model scales (≈125 M, 355 M, 1.3 B params) to expose trends.
   Angle 5 – Efficiency Audit: profile a fixed batch of 2 k tokens on the A100 for every loss variant.

5. Success Criteria (must satisfy simultaneously)
   • Task quality: ≤0.3 PPL gap or ≥+0.3 ROUGE/BLEU relative to CE; never worse than both CE & Brier.
   • Calibration: ≥25 % ECE reduction vs CE and within 5 % of Brier.
   • Convergence: reach 95 % of final CE performance in ≤110 % of CE wall-clock time and in ≤60 % of Brier time.
   • Efficiency: memory overhead ≤500 MB; throughput drop ≤2 %.
   • Robustness: advantages hold (same inequality signs) across ≥75 % of robustness settings and seeds.

6. Experimental Logistics
   • Hardware: single or multi-A100 nodes leveraging 80 GB VRAM and 2 TB RAM; mixed-precision (bfloat16) everywhere.
   • Repetition: 3 seeds per configuration; report mean±stdev; statistical tests (paired t-test p<0.05).
   • Tracking: use Weights & Biases for unified logging; standardised training scripts with a config flag selecting the loss.
   • Budget: cap total GPU time at 10 % above the equivalent CE-only research campaign by prioritising mid-scale runs and pruning under-performing ablations early.

7. Reporting Template (uniform across papers/sections)
   1. Learning curves (PPL & ECE) with wall-clock on x-axis
   2. Final metrics table (means, stdev, significance stars)
   3. Efficiency table (time/step, memory, tokens/s)
   4. Reliability diagrams (CE vs DPSM)
   5. Qualitative snippet comparison & human study results

This unified, multi-angle strategy ensures that every experiment—regardless of dataset, model, or training regime—jointly interrogates DPSM’s main claims: fast learning like CE, calibration like Brier, negligible overhead, and broad robustness.

# Current Experiment (to generate code for)
- Experiment ID: exp-1-language-modelling-scaling
- Description: Objective / hypothesis: Validate H1–H4 on from-scratch language-model training across three model scales. Show that DPSM preserves CE-level perplexity, approaches Brier-level calibration, converges almost as fast as CE, and adds <2 % overhead.

Models: GPT-2-small (125 M), GPT-2-medium (355 M), GPT-2-large (774 M).

Datasets: WikiText-103 (main), WikiText-2 (sanity check). Both are pre-tokenised with the standard GPT-2 BPE (50 502 tokens).

Pre-processing:  
• Strip empty lines, normalise Unicode, lowercase only for WikiText-103-LM-bench variant.  
• Pack into 1024-token sequences with EOS delimiter; drop remainder <32 tokens.  
• Mixed-precision (bfloat16) token/grad storage.

Data splitting: Official WikiText-103 train/valid/test. No further shuffling. Token-based deterministic split.

Repetitions: 3 random seeds per variation. Report mean±std. Selection criterion: best validation perplexity checkpoint (early stopping patience = 5 epochs).

Evaluation metrics:  
Primary – Perplexity (PPL, ↓).  
Secondary – NLL, ECE (10 & 100 bins), Brier score, Spherical score, expected recursion depth (for analysis).  
Efficiency – tokens/s, wall-clock to 95 % final PPL, peak GPU memory, FLOPs/step.

Comparisons:  
• ce-baseline – standard cross-entropy.  
• brier-baseline – pure Brier.  
• dpsm-linear – α schedule linear warm-up 10 % steps.  
• dpsm-cosine – cosine warm-up 10 %.  
• dpsm-fixed-alpha0.5 – α constant 0.5 from step 0 (ablation).

Hyper-parameter sensitivity: sweep warmup_steps ∈ {5 %, 10 %, 20 %} on GPT-2-small using Population Based Training; record PPL/ECE; include best curve in Appendix.

Robustness assessments:  
• Label noise: randomly flip 10 % of tokens within each batch for 5 % of training steps.  
• OOD generalisation: evaluate trained checkpoints on BooksCorpus-held-out split.  
• Seed variance: report variance inflation factor across 3 seeds.  
All robustness metrics reported as deltas vs clean CE baseline.

Efficiency analysis: Use PyTorch profiler + NVIDIA Nsight to log CUDA kernel time, memory, FLOPs for 2 000-token batch. Summarise mean overhead per step.

Example code:
```
# loss_selector.py
if args.loss == 'ce':
    criterion = torch.nn.CrossEntropyLoss()
elif args.loss == 'brier':
    criterion = BrierLoss(vocab)
elif args.loss.startswith('dpsm'):
    schedule = 'linear' if 'linear' in args.loss else 'cosine'
    fixed = 0.5 if 'fixed' in args.loss else None
    criterion = DPSMLoss(vocab, warmup_steps=int(0.1*total_steps),
                         schedule=schedule, fixed_alpha=fixed)
```
Expected outcome: dpsm-linear and dpsm-cosine match CE PPL within ±0.3 while cutting ECE by ≥30 %; converge in ≤1.1× CE wall-clock and ≤0.6× Brier time; overhead <1.5 % memory / <2 % throughput drop.
- Run Variations: ['ce-baseline', 'brier-baseline', 'dpsm-linear', 'dpsm-cosine', 'dpsm-fixed-alpha0.5']

# Base Code
{'train_py': '#!/usr/bin/env python\n"""src/train.py\nRuns a single experiment variation.  \nThis script is *invoked as a subprocess* by src/main.py so that every run has an\nisolated Python interpreter and clean GPU memory.  The CLI is intentionally\nminimal – **all run-specific information is provided through an on-disk YAML\nfile** produced by main.py.\n\nStandard-output protocol (MUST NOT CHANGE – relied upon by evaluate.py & CI)\n1. Human-readable experiment description (multi-line, free-form).\n2. A single **JSON line** with the structure documented below – this is parsed\n   by main.py & evaluate.py.\n\n{\n  "run_id": "<unique name from YAML>",\n  "epoch_metrics": [\n      {"epoch": 1, "train_loss": 4.83, "val_ppl": 125.1, "val_ece": 0.38},\n      ...\n  ],\n  "final":        {"val_ppl": 37.2, "val_ece": 0.09, "wall_clock": 713.4}\n}\n"""\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom .preprocess import load_dataset\nfrom .model import (\n    get_model,\n    get_loss_fn,\n    expected_calibration_error,\n)\nfrom .utils import set_seed, save_pdf\n\n# --------------------------------------------------------------------------------------\n# Helper functions\n# --------------------------------------------------------------------------------------\n\ndef _describe_experiment(cfg: Dict[str, Any]) -> str:\n    ds = cfg["dataset"]["name"]\n    model = cfg["model"]["name"]\n    loss = cfg["training"]["loss"]\n    epochs = cfg["training"]["epochs"]\n    bs = cfg["training"]["batch_size"]\n    return (\n        f"Running experiment \'{cfg[\'run_id\']}\'\\n"\n        f"  • Task       : {cfg[\'task_type\']}\\n"\n        f"  • Dataset    : {ds}\\n"\n        f"  • Model      : {model}\\n"\n        f"  • Loss       : {loss}\\n"\n        f"  • Epochs     : {epochs}\\n"\n        f"  • Batch size : {bs}\\n"\n    )\n\n\n# --------------------------------------------------------------------------------------\n# Training / Evaluation routines (model-agnostic)\n# --------------------------------------------------------------------------------------\n\ndef train_one_epoch(\n    model: nn.Module,\n    loss_fn: nn.Module,\n    data_loader: DataLoader,\n    optimizer: torch.optim.Optimizer,\n    device: torch.device,\n):\n    """Train for exactly one epoch and return average training loss."""\n\n    model.train()\n    total_loss = 0.0\n    total_tokens = 0\n\n    for batch in data_loader:\n        inputs, targets = [x.to(device) for x in batch]\n        optimizer.zero_grad(set_to_none=True)\n        logits = model(inputs)\n        loss = loss_fn(logits, targets)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * targets.numel()\n        total_tokens += targets.numel()\n\n    return total_loss / max(total_tokens, 1)\n\n\ndef evaluate(\n    model: nn.Module,\n    loss_fn: nn.Module,\n    data_loader: DataLoader,\n    device: torch.device,\n):\n    """Return perplexity & ECE on the supplied validation / test split."""\n\n    model.eval()\n    all_logits: List[torch.Tensor] = []\n    all_targets: List[torch.Tensor] = []\n    with torch.no_grad():\n        for batch in data_loader:\n            inputs, targets = [x.to(device) for x in batch]\n            logits = model(inputs)\n            _ = loss_fn(logits, targets)  # keep internal step counters consistent\n            all_logits.append(logits.detach())\n            all_targets.append(targets.detach())\n\n    logits = torch.cat(all_logits, dim=0)\n    targets = torch.cat(all_targets, dim=0)\n\n    ppl = torch.exp(\n        F.cross_entropy(\n            logits.view(-1, logits.size(-1)), targets.view(-1), reduction="mean"\n        )\n    ).item()\n\n    probs = logits.softmax(-1).view(-1, logits.size(-1))\n    labels = targets.view(-1)\n    ece = expected_calibration_error(probs, labels, num_bins=10).item()\n\n    return ppl, ece\n\n\n# --------------------------------------------------------------------------------------\n# Main entry point\n# --------------------------------------------------------------------------------------\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description="Train a single experimental run")\n    parser.add_argument("--config", type=str, required=True, help="Path to run config YAML")\n    parser.add_argument("--results-dir", type=str, required=True, help="Root directory to save run-specific outputs")\n    args = parser.parse_args()\n\n    import yaml  # local import to keep start-up time minimal\n\n    cfg: Dict[str, Any] = yaml.safe_load(Path(args.config).read_text())\n    run_id: str = cfg["run_id"]\n    results_dir = Path(args.results_dir)\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n    images_dir = run_dir / "images"\n    images_dir.mkdir(exist_ok=True, parents=True)\n\n    # ------------------------------------------------------------------\n    # Reproducibility & device\n    # ------------------------------------------------------------------\n    seed = int(cfg.get("seed", 42))\n    set_seed(seed)\n    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\n    # ------------------------------------------------------------------\n    # Data\n    # ------------------------------------------------------------------\n    train_loader, val_loader, vocab_size = load_dataset(cfg)\n\n    # ------------------------------------------------------------------\n    # Model & Loss\n    # ------------------------------------------------------------------\n    model = get_model(cfg, vocab_size=vocab_size).to(device)\n    loss_fn = get_loss_fn(cfg, vocab_size=vocab_size, device=device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg["training"]["learning_rate"])\n\n    # ------------------------------------------------------------------\n    # Training loop\n    # ------------------------------------------------------------------\n    num_epochs = int(cfg["training"]["epochs"])\n    epoch_metrics: List[Dict[str, float]] = []\n    start_time = time.time()\n\n    print(_describe_experiment(cfg), flush=True)\n\n    for epoch in range(1, num_epochs + 1):\n        train_loss = train_one_epoch(model, loss_fn, train_loader, optimizer, device)\n        val_ppl, val_ece = evaluate(model, loss_fn, val_loader, device)\n\n        epoch_metrics.append(\n            {\n                "epoch": epoch,\n                "train_loss": train_loss,\n                "val_ppl": val_ppl,\n                "val_ece": val_ece,\n            }\n        )\n\n    wall_clock = time.time() - start_time\n\n    # ------------------------------------------------------------------\n    # Save artifacts\n    # ------------------------------------------------------------------\n    torch.save(model.state_dict(), run_dir / "model.pt")\n    with (run_dir / "epoch_metrics.json").open("w") as f:\n        json.dump(epoch_metrics, f, indent=2)\n\n    # ----- Figures -----\n    import matplotlib.pyplot as plt\n\n    epochs = [m["epoch"] for m in epoch_metrics]\n    losses = [m["train_loss"] for m in epoch_metrics]\n    ppls = [m["val_ppl"] for m in epoch_metrics]\n    eces = [m["val_ece"] for m in epoch_metrics]\n\n    # Training loss curve\n    plt.figure(figsize=(6, 4))\n    plt.plot(epochs, losses, label="Train loss")\n    plt.scatter(epochs[-1], losses[-1], color="red")\n    plt.text(epochs[-1], losses[-1], f"{losses[-1]:.2f}")\n    plt.xlabel("Epoch")\n    plt.ylabel("Loss")\n    plt.title("Training Loss Curve")\n    plt.legend()\n    plt.tight_layout()\n    save_pdf(plt, images_dir / "training_loss.pdf")\n\n    # Validation PPL & ECE (twin axes)\n    fig, ax1 = plt.subplots(figsize=(6, 4))\n    ax2 = ax1.twinx()\n    ax1.plot(epochs, ppls, color="blue", label="PPL")\n    ax2.plot(epochs, eces, color="orange", label="ECE")\n    ax1.set_xlabel("Epoch")\n    ax1.set_ylabel("Perplexity", color="blue")\n    ax2.set_ylabel("ECE", color="orange")\n\n    # annotate final values\n    ax1.scatter(epochs[-1], ppls[-1], color="blue")\n    ax1.text(epochs[-1], ppls[-1], f"{ppls[-1]:.2f}")\n    ax2.scatter(epochs[-1], eces[-1], color="orange")\n    ax2.text(epochs[-1], eces[-1], f"{eces[-1]:.3f}")\n\n    fig.suptitle("Validation Metrics")\n    fig.tight_layout()\n    save_pdf(plt, images_dir / "validation_metrics.pdf")\n\n    # ------------------------------------------------------------------\n    # Print final metrics to STDOUT (machine-readable part)\n    # ------------------------------------------------------------------\n    final_payload = {\n        "run_id": run_id,\n        "epoch_metrics": epoch_metrics,\n        "final": {\n            "val_ppl": ppls[-1],\n            "val_ece": eces[-1],\n            "wall_clock": wall_clock,\n        },\n    }\n\n    print(json.dumps(final_payload), flush=True)\n\n\nif __name__ == "__main__":\n    main()\n', 'evaluate_py': '#!/usr/bin/env python\n"""src/evaluate.py\nAggregates results from multiple experimental runs (i.e. sub-directories of the\n`results_dir`) and generates comparison figures + a JSON summary printed to\nstdout.\n"""\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List, Any\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nfrom .utils import save_pdf\n\n\n# --------------------------------------------------------------------------------------\n# CLI & helpers\n# --------------------------------------------------------------------------------------\n\ndef _collect_results(results_dir: Path) -> pd.DataFrame:\n    rows: List[Dict[str, Any]] = []\n    for sub in results_dir.iterdir():\n        if not (sub / "epoch_metrics.json").exists():\n            continue  # skip non-run folders\n        with (sub / "epoch_metrics.json").open() as f:\n            epoch_metrics = json.load(f)\n        final = epoch_metrics[-1]\n        rows.append(\n            {\n                "run_id": sub.name,\n                "val_ppl": final["val_ppl"],\n                "val_ece": final["val_ece"],\n            }\n        )\n    if not rows:\n        raise RuntimeError(f"No result folders found in {results_dir}")\n    return pd.DataFrame(rows)\n\n\ndef _plot_bar(df: pd.DataFrame, metric: str, images_dir: Path):\n    plt.figure(figsize=(max(4, len(df) * 1.5), 4))\n    sns.barplot(x="run_id", y=metric, data=df, palette="viridis")\n    for i, v in enumerate(df[metric]):\n        plt.text(i, v, f"{v:.3f}", ha="center", va="bottom")\n    plt.title(f"Final {metric.upper()} Comparison")\n    plt.xlabel("Run ID")\n    plt.ylabel(metric.upper())\n    plt.tight_layout()\n    save_pdf(plt, images_dir / f"{metric}_comparison.pdf")\n\n\n# --------------------------------------------------------------------------------------\n# Main\n# --------------------------------------------------------------------------------------\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description="Evaluate all experiment variations")\n    parser.add_argument("--results-dir", required=True, type=str, help="Directory with all runs\' sub-folders")\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir)\n    images_dir = results_dir / "images"\n    images_dir.mkdir(exist_ok=True)\n\n    df = _collect_results(results_dir)\n\n    # ----- Figures -----\n    for metric in ["val_ppl", "val_ece"]:\n        _plot_bar(df, metric, images_dir)\n\n    # ----- JSON summary -----\n    summary = df.to_dict(orient="list")\n    print(json.dumps(summary, indent=None))\n\n\nif __name__ == "__main__":\n    main()\n', 'preprocess_py': '#!/usr/bin/env python\n"""src/preprocess.py\nCommon data-loading and preprocessing utilities with a **structured placeholder\nstrategy** for dataset-specific code.\n"""\n\nfrom __future__ import annotations\n\nimport random\nfrom pathlib import Path\nfrom typing import Dict, Tuple\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\n\n# --------------------------------------------------------------------------------------\n# Dummy dataset (used for smoke tests) – always available\n# --------------------------------------------------------------------------------------\n\n\nclass DummyLanguageModelingDataset(Dataset):\n    """Creates random token sequences for next-token prediction."""\n\n    def __init__(self, num_samples: int, seq_length: int, vocab_size: int):\n        super().__init__()\n        self.num_samples = num_samples\n        self.seq_length = seq_length\n        self.vocab_size = vocab_size\n        # Pre-generate data for determinism & speed\n        rng = random.Random(0)\n        self.data = [\n            torch.tensor([rng.randint(1, vocab_size - 1) for _ in range(seq_length)])\n            for _ in range(num_samples)\n        ]\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        x = self.data[idx]\n        return x[:-1], x[1:]  # input, target\n\n\n# --------------------------------------------------------------------------------------\n# Public API\n# --------------------------------------------------------------------------------------\n\ndef load_dataset(cfg: Dict, split_ratio: float = 0.9) -> Tuple[DataLoader, DataLoader, int]:\n    """Returns (train_loader, val_loader, vocab_size).\n\n    All task-specific details are abstracted behind this function. REAL datasets\n    will be injected in the derive-specific phase by replacing the placeholders\n    marked below.\n    """\n\n    task_type = cfg["task_type"]\n    dscfg = cfg["dataset"]\n    batch_size = cfg["training"]["batch_size"]\n\n    # ------------------------------------------------------------------\n    # Placeholders for dataset-specific loading logic\n    # ------------------------------------------------------------------\n    if dscfg["name"] == "dummy":\n        dataset = DummyLanguageModelingDataset(\n            num_samples=dscfg.get("num_samples", 1024),\n            seq_length=dscfg.get("seq_length", 32),\n            vocab_size=dscfg.get("vocab_size", 256),\n        )\n        vocab_size = dscfg.get("vocab_size", 256)\n    else:\n        # PLACEHOLDER: Will be replaced with specific dataset loading logic\n        raise NotImplementedError(\n            "Dataset \'{}\' not yet implemented (placeholder).".format(dscfg["name"])\n        )\n\n    # ------------------------------------------------------------------\n    # Split train/val\n    # ------------------------------------------------------------------\n    n_train = int(len(dataset) * split_ratio)\n    n_val = len(dataset) - n_train\n    train_set, val_set = torch.utils.data.random_split(dataset, [n_train, n_val])\n\n    # Simple collate (works for dummy LM)\n    def collate(batch):\n        inputs, targets = zip(*batch)\n        return torch.stack(inputs), torch.stack(targets)\n\n    train_loader = DataLoader(\n        train_set,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True,\n        collate_fn=collate,\n    )\n    val_loader = DataLoader(\n        val_set,\n        batch_size=batch_size,\n        shuffle=False,\n        drop_last=False,\n        collate_fn=collate,\n    )\n\n    return train_loader, val_loader, vocab_size\n', 'model_py': '#!/usr/bin/env python\n"""src/model.py\nCommon model architectures and loss functions, including the proposed DPSM loss.\n"""\n\nfrom __future__ import annotations\n\nimport math\nfrom typing import Dict\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n# --------------------------------------------------------------------------------------\n# Utility functions\n# --------------------------------------------------------------------------------------\n\ndef expected_calibration_error(probs: torch.Tensor, labels: torch.Tensor, num_bins: int = 10) -> torch.Tensor:\n    """Compute Expected Calibration Error (vectorised, differentiable).\n    Args:\n        probs: [N, V] probability distribution over classes for each sample.\n        labels: [N] ground-truth indices.\n    Returns:\n        ECE scalar tensor (on same device as probs).\n    """\n    confidences, predictions = probs.max(dim=1)\n    accuracies = predictions.eq(labels)\n\n    ece = torch.zeros(1, device=probs.device)\n    bin_boundaries = torch.linspace(0, 1, num_bins + 1, device=probs.device)\n    for i in range(num_bins):\n        lo, hi = bin_boundaries[i], bin_boundaries[i + 1]\n        mask = (confidences > lo) & (confidences <= hi)\n        if mask.any():\n            bin_acc = accuracies[mask].float().mean()\n            bin_conf = confidences[mask].mean()\n            ece += (mask.float().mean()) * torch.abs(bin_conf - bin_acc)\n    return ece\n\n\n# --------------------------------------------------------------------------------------\n# Dummy language model (small & dependency-free – perfect for smoke tests)\n# --------------------------------------------------------------------------------------\n\n\nclass DummyLanguageModel(nn.Module):\n    """A tiny LSTM-based LM supporting *any* vocab size (≥2)."""\n\n    def __init__(self, vocab_size: int, embedding_dim: int = 64, hidden_dim: int = 128, num_layers: int = 1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n        self.proj = nn.Linear(hidden_dim, vocab_size)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            nn.init.normal_(module.weight, mean=0, std=0.02)\n\n    def forward(self, input_ids: torch.Tensor):  # [B, T]\n        x = self.embed(input_ids)  # [B, T, D]\n        out, _ = self.lstm(x)\n        logits = self.proj(out)  # [B, T, V]\n        return logits\n\n\n# --------------------------------------------------------------------------------------\n# Loss functions\n# --------------------------------------------------------------------------------------\n\nclass CrossEntropyLoss(nn.Module):\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n        return F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n\n\nclass BrierLoss(nn.Module):\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n        p = logits.softmax(-1)\n        one_hot = F.one_hot(targets, logits.size(-1)).type_as(p)\n        loss = (p - one_hot).pow(2).sum(-1)\n        return loss.mean()\n\n\nclass DPSMLoss(nn.Module):\n    """Dynamic Proper-Score Mixing (CE ↔ Brier)."""\n\n    def __init__(self, vocab_size: int, warmup_steps: int = 1000, schedule: str = "linear"):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.warmup_steps = warmup_steps\n        self.schedule = schedule.lower()\n        self.register_buffer("global_step", torch.tensor(0.0))\n\n    def _alpha(self):\n        x = torch.clamp(self.global_step / self.warmup_steps, 0.0, 1.0)\n        if self.schedule == "linear":\n            return x\n        elif self.schedule == "cosine":\n            return 0.5 * (1 - torch.cos(math.pi * x))\n        else:\n            raise ValueError(f"Unknown schedule \'{self.schedule}\'")\n\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n        self.global_step += 1.0\n        ce = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), reduction="none")\n        p = logits.softmax(-1)\n        one_hot = F.one_hot(targets, logits.size(-1)).type_as(p)\n        brier = (p - one_hot).pow(2).sum(-1)\n        alpha = self._alpha()\n        loss = (1 - alpha) * ce + alpha * brier\n        return loss.mean()\n\n\n# --------------------------------------------------------------------------------------\n# Factories (public API)\n# --------------------------------------------------------------------------------------\n\ndef get_model(cfg: Dict, vocab_size: int) -> nn.Module:\n    model_cfg = cfg["model"]\n    name = model_cfg["name"]\n    if name == "dummy":\n        return DummyLanguageModel(\n            vocab_size=vocab_size,\n            embedding_dim=model_cfg.get("embedding_dim", 64),\n            hidden_dim=model_cfg.get("hidden_dim", 128),\n            num_layers=model_cfg.get("num_layers", 1),\n        )\n    else:\n        # PLACEHOLDER: Will be replaced with task- / model-specific implementation\n        raise NotImplementedError(f"Model \'{name}\' not implemented (placeholder)")\n\n\ndef get_loss_fn(cfg: Dict, vocab_size: int, device: torch.device):\n    loss_name = cfg["training"]["loss"].lower()\n    if loss_name == "ce":\n        return CrossEntropyLoss().to(device)\n    elif loss_name == "brier":\n        return BrierLoss().to(device)\n    elif loss_name == "dpsm":\n        schedule = cfg["training"].get("schedule", "linear")\n        warmup = cfg["training"].get("warmup_steps", 1000)\n        return DPSMLoss(vocab_size=vocab_size, warmup_steps=warmup, schedule=schedule).to(device)\n    else:\n        # PLACEHOLDER: Add focal loss, label smoothing, etc. in specialised phase.\n        raise NotImplementedError(f"Loss \'{loss_name}\' not implemented (placeholder)")\n', 'main_py': '#!/usr/bin/env python\n"""src/main.py\nMaster orchestrator.  \nUsage:\n  uv run python -m src.main --smoke-test  --results-dir <path>\n  uv run python -m src.main --full-experiment --results-dir <path>\n"""\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport shutil\nimport subprocess\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List, Any\n\nimport yaml\n\n# --------------------------------------------------------------------------------------\n# IO helpers\n# --------------------------------------------------------------------------------------\n\ndef _tee_stream(stream, log_file):\n    """Forward *stream* (stdout/stderr of subprocess) to both terminal and file."""\n    for line in iter(stream.readline, b""):\n        sys.stdout.buffer.write(line)\n        log_file.buffer.write(line)\n        sys.stdout.flush()\n        log_file.flush()\n\n\ndef _run_subprocess(cmd: List[str], env: Dict[str, str], stdout_path: Path, stderr_path: Path):\n    with stdout_path.open("wb") as out_f, stderr_path.open("wb") as err_f:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n        # Real-time tee\n        import threading\n\n        t_out = threading.Thread(target=_tee_stream, args=(proc.stdout, out_f))\n        t_err = threading.Thread(target=_tee_stream, args=(proc.stderr, err_f))\n        t_out.start(); t_err.start()\n        proc.wait()\n        t_out.join(); t_err.join()\n        if proc.returncode != 0:\n            raise RuntimeError(f"Subprocess failed with code {proc.returncode}: {\' \'.join(cmd)}")\n\n\n# --------------------------------------------------------------------------------------\n# Main\n# --------------------------------------------------------------------------------------\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description="Run all experiment variations sequentially")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument("--smoke-test", action="store_true")\n    group.add_argument("--full-experiment", action="store_true")\n    parser.add_argument("--results-dir", required=True, type=str, help="Directory to store all outputs")\n    args = parser.parse_args()\n\n    cfg_path = Path("config/smoke_test.yaml" if args.smoke_test else "config/full_experiment.yaml")\n    with cfg_path.open() as f:\n        master_cfg = yaml.safe_load(f)\n\n    experiments: List[Dict[str, Any]] = master_cfg["experiments"]\n    results_root = Path(args.results_dir)\n    if results_root.exists():\n        # Allow re-runs: remove previous contents\n        shutil.rmtree(results_root)\n    results_root.mkdir(parents=True)\n\n    # ------------------------------------------------------------------\n    # Run sequentially\n    # ------------------------------------------------------------------\n    for exp in experiments:\n        run_id = exp["run_id"]\n        run_dir = results_root / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n        # Dump per-run config (visible to train.py only)\n        run_cfg_path = run_dir / "config.yaml"\n        with run_cfg_path.open("w") as f:\n            yaml.safe_dump(exp, f)\n\n        # Subprocess call\n        cmd = [\n            sys.executable,\n            "-m",\n            "src.train",\n            "--config",\n            str(run_cfg_path),\n            "--results-dir",\n            str(results_root),\n        ]\n        env = os.environ.copy()\n        _run_subprocess(cmd, env, run_dir / "stdout.log", run_dir / "stderr.log")\n\n    # ------------------------------------------------------------------\n    # After all runs – aggregate & evaluate\n    # ------------------------------------------------------------------\n    eval_cmd = [\n        sys.executable,\n        "-m",\n        "src.evaluate",\n        "--results-dir",\n        str(results_root),\n    ]\n    _run_subprocess(eval_cmd, os.environ.copy(), results_root / "evaluate_stdout.log", results_root / "evaluate_stderr.log")\n\n    print("All experiments completed successfully.")\n\n\nif __name__ == "__main__":\n    main()\n', 'pyproject_toml': '[build-system]\nrequires = ["setuptools>=65.0"]\nbuild-backend = "setuptools.build_meta"\n\n[project]\nname = "dpsm-core"\nversion = "0.1.0"\ndescription = "Common Core Foundation for DPSM experiments"\nrequires-python = ">=3.9"\nlicense = {text = "Apache-2.0"}\nauthors = [\n    {name = "OpenAI Research Automation", email = "noreply@example.com"}\n]\n\n[project.dependencies]\ntorch = "*"\ntqdm = "*"\npyyaml = "*"\nmatplotlib = "*"\nseaborn = "*"\npandas = "*"\n\n# Optional (for future specialised experiments)\ntransformers = {version = "*", optional = true}\ndatasets = {version = "*", optional = true}\nrouge-score = {version = "*", optional = true}\n\n[project.optional-dependencies]\nhf = ["transformers", "datasets", "sentencepiece", "rouge-score"]\n', 'smoke_test_yaml': '# config/smoke_test.yaml\n# Lightweight end-to-end sanity check. Runs on a tiny *synthetic* dataset so\n# that CI can finish within seconds.\n\nexperiments:\n  - run_id: dummy_ce\n    task_type: language_modeling\n    dataset:\n      name: dummy            # built-in dataset – no external downloads\n      vocab_size: 128\n      seq_length: 32\n      num_samples: 256\n    model:\n      name: dummy            # tiny LSTM defined in src/model.py\n      vocab_size: 128\n      embedding_dim: 64\n      hidden_dim: 128\n    training:\n      epochs: 1\n      batch_size: 8\n      learning_rate: 1e-3\n      loss: ce\n      warmup_steps: 10\n    seed: 42\n\n  - run_id: dummy_dpsm\n    task_type: language_modeling\n    dataset:\n      name: dummy\n      vocab_size: 128\n      seq_length: 32\n      num_samples: 256\n    model:\n      name: dummy\n      vocab_size: 128\n      embedding_dim: 64\n      hidden_dim: 128\n    training:\n      epochs: 1\n      batch_size: 8\n      learning_rate: 1e-3\n      loss: dpsm\n      warmup_steps: 10\n      schedule: linear\n    seed: 42\n', 'full_experiment_yaml': '# config/full_experiment.yaml\n# TEMPLATE — will be filled with real datasets, models, and hyper-parameters in\na later pipeline stage. The placeholders *must* be replaced before running the\nfull experiment.\n\nexperiments:\n  - run_id: DATASET_PLACEHOLDER_ce\n    task_type: TASK_TYPE_PLACEHOLDER           # e.g. language_modeling | seq2seq\n    dataset:\n      name: DATASET_PLACEHOLDER                # PLACEHOLDER: specific dataset name\n      # PLACEHOLDER: add dataset-specific parameters here (paths, splits, ...)\n    model:\n      name: MODEL_PLACEHOLDER                  # PLACEHOLDER: specific model identifier\n      # PLACEHOLDER: model-specific hyper-parameters (layers, dims, ...)\n    training:\n      epochs: SPECIFIC_CONFIG_PLACEHOLDER\n      batch_size: SPECIFIC_CONFIG_PLACEHOLDER\n      learning_rate: SPECIFIC_CONFIG_PLACEHOLDER\n      loss: ce\n      warmup_steps: SPECIFIC_CONFIG_PLACEHOLDER\n    seed: SPECIFIC_CONFIG_PLACEHOLDER\n\n  - run_id: DATASET_PLACEHOLDER_dpsm\n    task_type: TASK_TYPE_PLACEHOLDER\n    dataset:\n      name: DATASET_PLACEHOLDER\n    model:\n      name: MODEL_PLACEHOLDER\n    training:\n      epochs: SPECIFIC_CONFIG_PLACEHOLDER\n      batch_size: SPECIFIC_CONFIG_PLACEHOLDER\n      learning_rate: SPECIFIC_CONFIG_PLACEHOLDER\n      loss: dpsm\n      warmup_steps: SPECIFIC_CONFIG_PLACEHOLDER\n      schedule: cosine     # example alternative schedule\n    seed: SPECIFIC_CONFIG_PLACEHOLDER\n\n  # Additional baselines / ablations can be appended following the same schema.\n'}

# External Resources (Use these to replace placeholders)

**HuggingFace Models (Replace MODEL_PLACEHOLDER with these):**

- ID: facebook/bart-large-cnn

- Code: from transformers import pipeline

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

ARTICLE = """ New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.
A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.
Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared "I do" five more times, sometimes only within two weeks of each other.
In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.
Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the
2010 marriage license application, according to court documents.
Prosecutors said the marriages were part of an immigration scam.
On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.
After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective
Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.
All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.
Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.
Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.
The case was referred to the Bronx District Attorney's Office by Immigration and Customs Enforcement and the Department of Homeland Security's
Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.
Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.
If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.
"""

print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))



**HuggingFace Datasets (Replace DATASET_PLACEHOLDER with these):**

- ID: abisee/cnn_dailymail





# Validation Feedback


**Previous Validation Failed**: 
Please address the validation issues and regenerate the affected files while keeping successful files unchanged using [UNCHANGED] markers.



Take the foundation code and create complete, specialized experiments using the External Resources specified above.
Output:
{
    "train_py": "#!/usr/bin/env python\n\"\"\"src/train.py\nRuns a single experiment variation.  \nThis script is *invoked as a subprocess* by src/main.py so that every run has an\nisolated Python interpreter and clean GPU memory.  The CLI is intentionally\nminimal – **all run-specific information is provided through an on-disk YAML\nfile** produced by main.py.\n\nStandard-output protocol (MUST NOT CHANGE – relied upon by evaluate.py & CI)\n1. Human-readable experiment description (multi-line, free-form).\n2. A single **JSON line** with the structure documented below – this is parsed\n   by main.py & evaluate.py.\n\n{\n  \"run_id\": \"<unique name from YAML>\",\n  \"epoch_metrics\": [\n      {\"epoch\": 1, \"train_loss\": 4.83, \"val_ppl\": 125.1, \"val_ece\": 0.38},\n      ...\n  ],\n  \"final\":        {\"val_ppl\": 37.2, \"val_ece\": 0.09, \"wall_clock\": 713.4}\n}\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom .preprocess import load_dataset\nfrom .model import (\n    get_model,\n    get_loss_fn,\n    expected_calibration_error,\n)\nfrom .utils import set_seed, save_pdf\n\n# --------------------------------------------------------------------------------------\n# Helper functions\n# --------------------------------------------------------------------------------------\n\ndef _describe_experiment(cfg: Dict[str, Any]) -> str:\n    ds = cfg[\"dataset\"][\"name\"]\n    model = cfg[\"model\"][\"name\"]\n    loss = cfg[\"training\"][\"loss\"]\n    epochs = cfg[\"training\"][\"epochs\"]\n    bs = cfg[\"training\"][\"batch_size\"]\n    return (\n        f\"Running experiment '{cfg['run_id']}'\\n\"\n        f\"  • Task       : {cfg['task_type']}\\n\"\n        f\"  • Dataset    : {ds}\\n\"\n        f\"  • Model      : {model}\\n\"\n        f\"  • Loss       : {loss}\\n\"\n        f\"  • Epochs     : {epochs}\\n\"\n        f\"  • Batch size : {bs}\\n\"\n    )\n\n\n# --------------------------------------------------------------------------------------\n# Training / Evaluation routines (model-agnostic)\n# --------------------------------------------------------------------------------------\n\ndef train_one_epoch(\n    model: nn.Module,\n    loss_fn: nn.Module,\n    data_loader: DataLoader,\n    optimizer: torch.optim.Optimizer,\n    device: torch.device,\n):\n    \"\"\"Train for exactly one epoch and return average training loss.\"\"\"\n\n    model.train()\n    total_loss = 0.0\n    total_tokens = 0\n\n    for batch in data_loader:\n        inputs, targets = [x.to(device) for x in batch]\n        optimizer.zero_grad(set_to_none=True)\n        logits = model(inputs)\n        loss = loss_fn(logits, targets)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * targets.numel()\n        total_tokens += targets.numel()\n\n    return total_loss / max(total_tokens, 1)\n\n\ndef evaluate(\n    model: nn.Module,\n    loss_fn: nn.Module,\n    data_loader: DataLoader,\n    device: torch.device,\n):\n    \"\"\"Return perplexity & ECE on the supplied validation / test split.\"\"\"\n\n    model.eval()\n    all_logits: List[torch.Tensor] = []\n    all_targets: List[torch.Tensor] = []\n    with torch.no_grad():\n        for batch in data_loader:\n            inputs, targets = [x.to(device) for x in batch]\n            logits = model(inputs)\n            _ = loss_fn(logits, targets)  # keep internal step counters consistent\n            all_logits.append(logits.detach())\n            all_targets.append(targets.detach())\n\n    logits = torch.cat(all_logits, dim=0)\n    targets = torch.cat(all_targets, dim=0)\n\n    ppl = torch.exp(\n        F.cross_entropy(\n            logits.view(-1, logits.size(-1)), targets.view(-1), reduction=\"mean\"\n        )\n    ).item()\n\n    probs = logits.softmax(-1).view(-1, logits.size(-1))\n    labels = targets.view(-1)\n    ece = expected_calibration_error(probs, labels, num_bins=10).item()\n\n    return ppl, ece\n\n\n# --------------------------------------------------------------------------------------\n# Main entry point\n# --------------------------------------------------------------------------------------\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=\"Train a single experimental run\")\n    parser.add_argument(\"--config\", type=str, required=True, help=\"Path to run config YAML\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory to save run-specific outputs\")\n    args = parser.parse_args()\n\n    import yaml  # local import to keep start-up time minimal\n\n    cfg: Dict[str, Any] = yaml.safe_load(Path(args.config).read_text())\n    run_id: str = cfg[\"run_id\"]\n    results_dir = Path(args.results_dir)\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n    images_dir = run_dir / \"images\"\n    images_dir.mkdir(exist_ok=True, parents=True)\n\n    # ------------------------------------------------------------------\n    # Reproducibility & device\n    # ------------------------------------------------------------------\n    seed = int(cfg.get(\"seed\", 42))\n    set_seed(seed)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ------------------------------------------------------------------\n    # Data\n    # ------------------------------------------------------------------\n    train_loader, val_loader, vocab_size = load_dataset(cfg)\n\n    # ------------------------------------------------------------------\n    # Model & Loss\n    # ------------------------------------------------------------------\n    model = get_model(cfg, vocab_size=vocab_size).to(device)\n    loss_fn = get_loss_fn(cfg, vocab_size=vocab_size, device=device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg[\"training\"][\"learning_rate\"])\n\n    # ------------------------------------------------------------------\n    # Training loop\n    # ------------------------------------------------------------------\n    num_epochs = int(cfg[\"training\"][\"epochs\"])\n    epoch_metrics: List[Dict[str, float]] = []\n    start_time = time.time()\n\n    print(_describe_experiment(cfg), flush=True)\n\n    for epoch in range(1, num_epochs + 1):\n        train_loss = train_one_epoch(model, loss_fn, train_loader, optimizer, device)\n        val_ppl, val_ece = evaluate(model, loss_fn, val_loader, device)\n\n        epoch_metrics.append(\n            {\n                \"epoch\": epoch,\n                \"train_loss\": train_loss,\n                \"val_ppl\": val_ppl,\n                \"val_ece\": val_ece,\n            }\n        )\n\n    wall_clock = time.time() - start_time\n\n    # ------------------------------------------------------------------\n    # Save artifacts\n    # ------------------------------------------------------------------\n    torch.save(model.state_dict(), run_dir / \"model.pt\")\n    with (run_dir / \"epoch_metrics.json\").open(\"w\") as f:\n        json.dump(epoch_metrics, f, indent=2)\n\n    # ----- Figures -----\n    import matplotlib.pyplot as plt\n\n    epochs = [m[\"epoch\"] for m in epoch_metrics]\n    losses = [m[\"train_loss\"] for m in epoch_metrics]\n    ppls = [m[\"val_ppl\"] for m in epoch_metrics]\n    eces = [m[\"val_ece\"] for m in epoch_metrics]\n\n    # Training loss curve\n    plt.figure(figsize=(6, 4))\n    plt.plot(epochs, losses, label=\"Train loss\")\n    plt.scatter(epochs[-1], losses[-1], color=\"red\")\n    plt.text(epochs[-1], losses[-1], f\"{losses[-1]:.2f}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training Loss Curve\")\n    plt.legend()\n    plt.tight_layout()\n    save_pdf(plt, images_dir / \"training_loss.pdf\")\n\n    # Validation PPL & ECE (twin axes)\n    fig, ax1 = plt.subplots(figsize=(6, 4))\n    ax2 = ax1.twinx()\n    ax1.plot(epochs, ppls, color=\"blue\", label=\"PPL\")\n    ax2.plot(epochs, eces, color=\"orange\", label=\"ECE\")\n    ax1.set_xlabel(\"Epoch\")\n    ax1.set_ylabel(\"Perplexity\", color=\"blue\")\n    ax2.set_ylabel(\"ECE\", color=\"orange\")\n\n    # annotate final values\n    ax1.scatter(epochs[-1], ppls[-1], color=\"blue\")\n    ax1.text(epochs[-1], ppls[-1], f\"{ppls[-1]:.2f}\")\n    ax2.scatter(epochs[-1], eces[-1], color=\"orange\")\n    ax2.text(epochs[-1], eces[-1], f\"{eces[-1]:.3f}\")\n\n    fig.suptitle(\"Validation Metrics\")\n    fig.tight_layout()\n    save_pdf(plt, images_dir / \"validation_metrics.pdf\")\n\n    # ------------------------------------------------------------------\n    # Print final metrics to STDOUT (machine-readable part)\n    # ------------------------------------------------------------------\n    final_payload = {\n        \"run_id\": run_id,\n        \"epoch_metrics\": epoch_metrics,\n        \"final\": {\n            \"val_ppl\": ppls[-1],\n            \"val_ece\": eces[-1],\n            \"wall_clock\": wall_clock,\n        },\n    }\n\n    print(json.dumps(final_payload), flush=True)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "#!/usr/bin/env python\n\"\"\"src/evaluate.py\nAggregates results from multiple experimental runs (i.e. sub-directories of the\n`results_dir`) and generates comparison figures + a JSON summary printed to\nstdout.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List, Any\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nfrom .utils import save_pdf\n\n\n# --------------------------------------------------------------------------------------\n# CLI & helpers\n# --------------------------------------------------------------------------------------\n\ndef _collect_results(results_dir: Path) -> pd.DataFrame:\n    rows: List[Dict[str, Any]] = []\n    for sub in results_dir.iterdir():\n        if not (sub / \"epoch_metrics.json\").exists():\n            continue  # skip non-run folders\n        with (sub / \"epoch_metrics.json\").open() as f:\n            epoch_metrics = json.load(f)\n        final = epoch_metrics[-1]\n        rows.append(\n            {\n                \"run_id\": sub.name,\n                \"val_ppl\": final[\"val_ppl\"],\n                \"val_ece\": final[\"val_ece\"],\n            }\n        )\n    if not rows:\n        raise RuntimeError(f\"No result folders found in {results_dir}\")\n    return pd.DataFrame(rows)\n\n\ndef _plot_bar(df: pd.DataFrame, metric: str, images_dir: Path):\n    plt.figure(figsize=(max(4, len(df) * 1.5), 4))\n    sns.barplot(x=\"run_id\", y=metric, data=df, palette=\"viridis\")\n    for i, v in enumerate(df[metric]):\n        plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.title(f\"Final {metric.upper()} Comparison\")\n    plt.xlabel(\"Run ID\")\n    plt.ylabel(metric.upper())\n    plt.tight_layout()\n    save_pdf(plt, images_dir / f\"{metric}_comparison.pdf\")\n\n\n# --------------------------------------------------------------------------------------\n# Main\n# --------------------------------------------------------------------------------------\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=\"Evaluate all experiment variations\")\n    parser.add_argument(\"--results-dir\", required=True, type=str, help=\"Directory with all runs' sub-folders\")\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir)\n    images_dir = results_dir / \"images\"\n    images_dir.mkdir(exist_ok=True)\n\n    df = _collect_results(results_dir)\n\n    # ----- Figures -----\n    for metric in [\"val_ppl\", \"val_ece\"]:\n        _plot_bar(df, metric, images_dir)\n\n    # ----- JSON summary -----\n    summary = df.to_dict(orient=\"list\")\n    print(json.dumps(summary, indent=None))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "#!/usr/bin/env python\n\"\"\"src/preprocess.py\nDataset loading and preprocessing utilities for language-model experiments.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport itertools\nfrom pathlib import Path\nfrom typing import Dict, Tuple, List\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n# HuggingFace\nfrom datasets import load_dataset\nfrom transformers import GPT2TokenizerFast\n\n# --------------------------------------------------------------------------------------\n# Synthetic dummy dataset (kept for smoke-tests)\n# --------------------------------------------------------------------------------------\n\n\nclass DummyLanguageModelingDataset(Dataset):\n    \"\"\"Creates random token sequences for next-token prediction.\"\"\"\n\n    def __init__(self, num_samples: int, seq_length: int, vocab_size: int):\n        super().__init__()\n        g = torch.Generator().manual_seed(0)\n        self.data = torch.randint(1, vocab_size, (num_samples, seq_length + 1), generator=g)\n        self.vocab_size = vocab_size\n\n    def __len__(self):\n        return self.data.size(0)\n\n    def __getitem__(self, idx):\n        seq = self.data[idx]\n        return seq[:-1], seq[1:]  # input, target\n\n\n# --------------------------------------------------------------------------------------\n# Real language-modeling datasets (WikiText-2 / 103)\n# --------------------------------------------------------------------------------------\n\n\nclass PackedLMSequenceDataset(Dataset):\n    \"\"\"Packs a flat token stream into fixed-length sequences for next-token prediction.\n\n    Each item returns (input_ids, target_ids) where target_ids = input_ids shifted by +1.\n    \"\"\"\n\n    def __init__(self, token_ids: torch.Tensor, seq_length: int):\n        super().__init__()\n        # Drop the tail that doesn’t fit a full (seq_length+1) window\n        window = seq_length + 1\n        usable_len = (token_ids.size(0) // window) * window\n        self.tokens = token_ids[:usable_len]\n        self.seq_length = seq_length\n        self.num_sequences = usable_len // window\n\n    def __len__(self):\n        return self.num_sequences\n\n    def __getitem__(self, idx):\n        window = self.seq_length + 1\n        start = idx * window\n        end = start + window\n        chunk = self.tokens[start:end]\n        return chunk[:-1], chunk[1:]\n\n\n# --------------------------------------------------------------------------------------\n# Helper functions\n# --------------------------------------------------------------------------------------\n\n\ndef _get_tokenizer(tokenizer_name: str = \"gpt2\") -> GPT2TokenizerFast:\n    tokenizer = GPT2TokenizerFast.from_pretrained(tokenizer_name)\n    # GPT-2 doesn’t have pad token by default – assign eos as pad for batching.\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    return tokenizer\n\n\ndef _load_wikitext(name: str, seq_length: int) -> Tuple[Dataset, Dataset, int]:\n    \"\"\"Load WikiText-2 or ‑103 from HF Datasets and build Packed datasets.\"\"\"\n\n    variant_map = {\n        \"wikitext-2\": \"wikitext-2-raw-v1\",\n        \"wikitext-103\": \"wikitext-103-raw-v1\",\n    }\n    if name not in variant_map:\n        raise ValueError(f\"Unsupported WikiText dataset: {name}\")\n\n    raw_ds = load_dataset(\"wikitext\", variant_map[name])\n    tokenizer = _get_tokenizer()\n\n    def _tokenise(split_name: str) -> torch.Tensor:\n        texts = raw_ds[split_name][\"text\"]\n        # Filter empty lines for cleanliness\n        texts = [t for t in texts if len(t.strip()) > 0]\n        # Encode and concatenate with EOS delimiters\n        eos_id = tokenizer.eos_token_id\n        ids: List[int] = list(\n            itertools.chain.from_iterable(\n                itertools.chain.from_iterable(\n                    [(tokenizer.encode(t, add_special_tokens=False) + [eos_id]) for t in texts]\n                )\n            )\n        )\n        return torch.tensor(ids, dtype=torch.long)\n\n    train_tokens = _tokenise(\"train\")\n    val_tokens = _tokenise(\"validation\")\n\n    train_set = PackedLMSequenceDataset(train_tokens, seq_length)\n    val_set = PackedLMSequenceDataset(val_tokens, seq_length)\n    vocab_size = tokenizer.vocab_size\n    return train_set, val_set, vocab_size\n\n\n# --------------------------------------------------------------------------------------\n# Public API\n# --------------------------------------------------------------------------------------\n\n\ndef load_dataset(cfg: Dict, split_ratio: float = 0.9) -> Tuple[DataLoader, DataLoader, int]:\n    \"\"\"Create DataLoaders according to *cfg* and return (train_loader, val_loader, vocab_size).\"\"\"\n\n    dscfg = cfg[\"dataset\"]\n    batch_size = cfg[\"training\"][\"batch_size\"]\n    seq_length = dscfg.get(\"seq_length\", 1024)\n\n    if dscfg[\"name\"] == \"dummy\":\n        dataset = DummyLanguageModelingDataset(\n            num_samples=dscfg.get(\"num_samples\", 1024),\n            seq_length=dscfg.get(\"seq_length\", 32),\n            vocab_size=dscfg.get(\"vocab_size\", 256),\n        )\n        # Simple random split\n        n_train = int(len(dataset) * split_ratio)\n        n_val = len(dataset) - n_train\n        train_set, val_set = torch.utils.data.random_split(dataset, [n_train, n_val])\n        vocab_size = dscfg.get(\"vocab_size\", 256)\n    elif dscfg[\"name\"].startswith(\"wikitext\"):\n        train_set, val_set, vocab_size = _load_wikitext(dscfg[\"name\"], seq_length)\n    else:\n        raise NotImplementedError(f\"Dataset '{dscfg['name']}' is not supported.\")\n\n    # ------- DataLoaders -------\n    def collate(batch):\n        inputs, targets = zip(*batch)\n        return torch.stack(inputs), torch.stack(targets)\n\n    train_loader = DataLoader(\n        train_set,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True,\n        collate_fn=collate,\n        pin_memory=True,\n    )\n    val_loader = DataLoader(\n        val_set,\n        batch_size=batch_size,\n        shuffle=False,\n        drop_last=False,\n        collate_fn=collate,\n        pin_memory=True,\n    )\n\n    return train_loader, val_loader, vocab_size",
    "model_py": "#!/usr/bin/env python\n\"\"\"src/model.py\nModel architectures and loss functions for DPSM experiments.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom typing import Dict, Optional\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n# HuggingFace – heavy deps are acceptable for the full experiment\nfrom transformers import GPT2Config, GPT2LMHeadModel\n\n# --------------------------------------------------------------------------------------\n# Metrics\n# --------------------------------------------------------------------------------------\n\ndef expected_calibration_error(probs: torch.Tensor, labels: torch.Tensor, num_bins: int = 10) -> torch.Tensor:\n    \"\"\"Vectorised Expected Calibration Error implementation.\"\"\"\n\n    confidences, predictions = probs.max(dim=1)\n    accuracies = predictions.eq(labels)\n\n    ece = torch.zeros(1, device=probs.device)\n    bin_boundaries = torch.linspace(0, 1, num_bins + 1, device=probs.device)\n    for i in range(num_bins):\n        lo, hi = bin_boundaries[i], bin_boundaries[i + 1]\n        mask = (confidences > lo) & (confidences <= hi)\n        if mask.any():\n            bin_acc = accuracies[mask].float().mean()\n            bin_conf = confidences[mask].mean()\n            ece += (mask.float().mean()) * torch.abs(bin_conf - bin_acc)\n    return ece\n\n\n# --------------------------------------------------------------------------------------\n# Minimal LSTM (kept for smoke-tests)\n# --------------------------------------------------------------------------------------\n\n\nclass DummyLanguageModel(nn.Module):\n    \"\"\"A tiny 1-layer LSTM language model suitable for quick tests.\"\"\"\n\n    def __init__(self, vocab_size: int, embedding_dim: int = 64, hidden_dim: int = 128, num_layers: int = 1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n        self.proj = nn.Linear(hidden_dim, vocab_size)\n        self.apply(self._init)\n\n    @staticmethod\n    def _init(m):\n        if isinstance(m, (nn.Linear, nn.Embedding)):\n            nn.init.normal_(m.weight, std=0.02)\n            if hasattr(m, \"bias\") and m.bias is not None:\n                nn.init.zeros_(m.bias)\n\n    def forward(self, input_ids: torch.Tensor):  # [B, T]\n        x = self.embed(input_ids)\n        out, _ = self.lstm(x)\n        return self.proj(out)\n\n\n# --------------------------------------------------------------------------------------\n# HuggingFace GPT-2 wrapper\n# --------------------------------------------------------------------------------------\n\n\nclass GPT2LMWrapper(nn.Module):\n    \"\"\"Thin wrapper that exposes `.forward -> logits` for training loop.\"\"\"\n\n    def __init__(self, model_name: str = \"gpt2\", from_pretrained: bool = False):\n        super().__init__()\n        if from_pretrained:\n            self.inner = GPT2LMHeadModel.from_pretrained(model_name)\n        else:\n            cfg = GPT2Config.from_pretrained(model_name)\n            self.inner = GPT2LMHeadModel(cfg)\n\n    def forward(self, input_ids: torch.Tensor):\n        return self.inner(input_ids=input_ids).logits\n\n\n# --------------------------------------------------------------------------------------\n# Loss functions\n# --------------------------------------------------------------------------------------\n\nclass CrossEntropyLoss(nn.Module):\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n        return F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n\n\nclass BrierLoss(nn.Module):\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n        p = logits.softmax(-1)\n        oh = F.one_hot(targets, logits.size(-1)).type_as(p)\n        return (p - oh).pow(2).sum(-1).mean()\n\n\nclass DPSMLoss(nn.Module):\n    \"\"\"Dynamic Proper-Score Mixing between CE and Brier.\"\"\"\n\n    def __init__(\n        self,\n        vocab_size: int,\n        warmup_steps: int = 1000,\n        schedule: str = \"linear\",\n        fixed_alpha: Optional[float] = None,\n    ):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.warmup_steps = max(1, warmup_steps)\n        self.schedule = schedule.lower()\n        self.fixed_alpha = fixed_alpha\n        self.register_buffer(\"global_step\", torch.tensor(0.0))\n\n    def _alpha(self) -> torch.Tensor:\n        if self.fixed_alpha is not None:\n            return torch.tensor(self.fixed_alpha, device=self.global_step.device)\n        x = torch.clamp(self.global_step / self.warmup_steps, 0.0, 1.0)\n        if self.schedule == \"linear\":\n            return x\n        elif self.schedule == \"cosine\":\n            return 0.5 * (1 - torch.cos(math.pi * x))\n        else:\n            raise ValueError(f\"Unknown DPSM schedule '{self.schedule}'\")\n\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n        self.global_step += 1.0\n        ce = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), reduction=\"none\")\n        p = logits.softmax(-1)\n        oh = F.one_hot(targets, logits.size(-1)).type_as(p)\n        brier = (p - oh).pow(2).sum(-1)\n        alpha = self._alpha()\n        loss = (1 - alpha) * ce + alpha * brier\n        return loss.mean()\n\n\n# --------------------------------------------------------------------------------------\n# Factories\n# --------------------------------------------------------------------------------------\n\ndef get_model(cfg: Dict, vocab_size: int) -> nn.Module:\n    name = cfg[\"model\"][\"name\"].lower()\n\n    if name == \"dummy\":\n        mc = cfg[\"model\"]\n        return DummyLanguageModel(\n            vocab_size=vocab_size,\n            embedding_dim=mc.get(\"embedding_dim\", 64),\n            hidden_dim=mc.get(\"hidden_dim\", 128),\n            num_layers=mc.get(\"num_layers\", 1),\n        )\n\n    if name in {\"gpt2-small\", \"gpt2\", \"gpt2-medium\", \"gpt2-large\"}:\n        from_pretrained = cfg[\"model\"].get(\"from_pretrained\", False)\n        return GPT2LMWrapper(model_name=name.replace(\"-small\", \"\"), from_pretrained=from_pretrained)\n\n    raise NotImplementedError(f\"Model '{name}' is not implemented.\")\n\n\ndef get_loss_fn(cfg: Dict, vocab_size: int, device: torch.device):\n    raw = cfg[\"training\"][\"loss\"].lower()\n\n    if raw == \"ce\":\n        return CrossEntropyLoss().to(device)\n    if raw == \"brier\":\n        return BrierLoss().to(device)\n\n    if raw.startswith(\"dpsm\"):\n        # Determine schedule / fixed alpha from string or config\n        if \"cosine\" in raw:\n            schedule = \"cosine\"\n        elif \"linear\" in raw:\n            schedule = \"linear\"\n        elif \"fixed\" in raw:\n            schedule = \"fixed\"\n        else:\n            schedule = cfg[\"training\"].get(\"schedule\", \"linear\")\n\n        fixed_alpha = None\n        if \"fixed\" in raw:\n            # try to parse trailing alpha e.g. dpsm-fixed-alpha0.5\n            if \"alpha\" in raw:\n                try:\n                    fixed_alpha = float(raw.split(\"alpha\")[-1])\n                except ValueError:\n                    pass\n            fixed_alpha = fixed_alpha or cfg[\"training\"].get(\"fixed_alpha\", 0.5)\n\n        warmup = int(cfg[\"training\"].get(\"warmup_steps\", 1000))\n        return DPSMLoss(vocab_size, warmup_steps=warmup, schedule=schedule, fixed_alpha=fixed_alpha).to(device)\n\n    raise NotImplementedError(f\"Loss '{raw}' is not supported.\" )",
    "main_py": "#!/usr/bin/env python\n\"\"\"src/main.py\nMaster orchestrator.  \nUsage:\n  uv run python -m src.main --smoke-test  --results-dir <path>\n  uv run python -m src.main --full-experiment --results-dir <path>\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport shutil\nimport subprocess\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List, Any\n\nimport yaml\n\n# --------------------------------------------------------------------------------------\n# IO helpers\n# --------------------------------------------------------------------------------------\n\ndef _tee_stream(stream, log_file):\n    \"\"\"Forward *stream* (stdout/stderr of subprocess) to both terminal and file.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        sys.stdout.buffer.write(line)\n        log_file.buffer.write(line)\n        sys.stdout.flush()\n        log_file.flush()\n\n\ndef _run_subprocess(cmd: List[str], env: Dict[str, str], stdout_path: Path, stderr_path: Path):\n    with stdout_path.open(\"wb\") as out_f, stderr_path.open(\"wb\") as err_f:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n        # Real-time tee\n        import threading\n\n        t_out = threading.Thread(target=_tee_stream, args=(proc.stdout, out_f))\n        t_err = threading.Thread(target=_tee_stream, args=(proc.stderr, err_f))\n        t_out.start(); t_err.start()\n        proc.wait()\n        t_out.join(); t_err.join()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Subprocess failed with code {proc.returncode}: {' '.join(cmd)}\")\n\n\n# --------------------------------------------------------------------------------------\n# Main\n# --------------------------------------------------------------------------------------\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(description=\"Run all experiment variations sequentially\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\")\n    parser.add_argument(\"--results-dir\", required=True, type=str, help=\"Directory to store all outputs\")\n    args = parser.parse_args()\n\n    cfg_path = Path(\"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\")\n    with cfg_path.open() as f:\n        master_cfg = yaml.safe_load(f)\n\n    experiments: List[Dict[str, Any]] = master_cfg[\"experiments\"]\n    results_root = Path(args.results_dir)\n    if results_root.exists():\n        # Allow re-runs: remove previous contents\n        shutil.rmtree(results_root)\n    results_root.mkdir(parents=True)\n\n    # ------------------------------------------------------------------\n    # Run sequentially\n    # ------------------------------------------------------------------\n    for exp in experiments:\n        run_id = exp[\"run_id\"]\n        run_dir = results_root / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n        # Dump per-run config (visible to train.py only)\n        run_cfg_path = run_dir / \"config.yaml\"\n        with run_cfg_path.open(\"w\") as f:\n            yaml.safe_dump(exp, f)\n\n        # Subprocess call\n        cmd = [\n            sys.executable,\n            \"-m\",\n            \"src.train\",\n            \"--config\",\n            str(run_cfg_path),\n            \"--results-dir\",\n            str(results_root),\n        ]\n        env = os.environ.copy()\n        _run_subprocess(cmd, env, run_dir / \"stdout.log\", run_dir / \"stderr.log\")\n\n    # ------------------------------------------------------------------\n    # After all runs – aggregate & evaluate\n    # ------------------------------------------------------------------\n    eval_cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.evaluate\",\n        \"--results-dir\",\n        str(results_root),\n    ]\n    _run_subprocess(eval_cmd, os.environ.copy(), results_root / \"evaluate_stdout.log\", results_root / \"evaluate_stderr.log\")\n\n    print(\"All experiments completed successfully.\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[build-system]\nrequires = [\"setuptools>=65.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"dpsm-core\"\nversion = \"0.1.0\"\ndescription = \"Dynamic Proper-Score Mixing experiments\"\nrequires-python = \">=3.9\"\nlicense = {text = \"Apache-2.0\"}\nauthors = [\n    {name = \"OpenAI Research Automation\", email = \"noreply@example.com\"}\n]\n\n[project.dependencies]\ntorch = \"*\"\ntqdm = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\npandas = \"*\"\ntransformers = \"*\"\ndatasets = \"*\"\nsentencepiece = \"*\"\n",
    "smoke_test_yaml": "# config/smoke_test.yaml\n# Lightweight end-to-end sanity check. Runs on a tiny *synthetic* dataset so\n# that CI can finish within seconds.\n\nexperiments:\n  - run_id: dummy_ce\n    task_type: language_modeling\n    dataset:\n      name: dummy            # built-in dataset – no external downloads\n      vocab_size: 128\n      seq_length: 32\n      num_samples: 256\n    model:\n      name: dummy            # tiny LSTM defined in src/model.py\n      vocab_size: 128\n      embedding_dim: 64\n      hidden_dim: 128\n    training:\n      epochs: 1\n      batch_size: 8\n      learning_rate: 1e-3\n      loss: ce\n      warmup_steps: 10\n    seed: 42\n\n  - run_id: dummy_dpsm\n    task_type: language_modeling\n    dataset:\n      name: dummy\n      vocab_size: 128\n      seq_length: 32\n      num_samples: 256\n    model:\n      name: dummy\n      vocab_size: 128\n      embedding_dim: 64\n      hidden_dim: 128\n    training:\n      epochs: 1\n      batch_size: 8\n      learning_rate: 1e-3\n      loss: dpsm\n      warmup_steps: 10\n      schedule: linear\n    seed: 42\n",
    "full_experiment_yaml": "# config/full_experiment.yaml\n# Full experimental grid for exp-1-language-modelling-scaling\n\nexperiments:\n  # ----------------------------------------------------------------------------------\n  # GPT-2 SMALL (≈125 M)\n  # ----------------------------------------------------------------------------------\n  - run_id: gpt2-small_ce-baseline\n    task_type: language_modeling\n    dataset:\n      name: wikitext-2\n      seq_length: 1024\n    model:\n      name: gpt2-small\n      from_pretrained: false\n    training:\n      epochs: 10\n      batch_size: 16\n      learning_rate: 5e-4\n      loss: ce\n      warmup_steps: 1000\n    seed: 42\n\n  - run_id: gpt2-small_brier-baseline\n    task_type: language_modeling\n    dataset:\n      name: wikitext-2\n      seq_length: 1024\n    model:\n      name: gpt2-small\n      from_pretrained: false\n    training:\n      epochs: 10\n      batch_size: 16\n      learning_rate: 5e-4\n      loss: brier\n      warmup_steps: 1000\n    seed: 42\n\n  - run_id: gpt2-small_dpsm-linear\n    task_type: language_modeling\n    dataset:\n      name: wikitext-2\n      seq_length: 1024\n    model:\n      name: gpt2-small\n      from_pretrained: false\n    training:\n      epochs: 10\n      batch_size: 16\n      learning_rate: 5e-4\n      loss: dpsm-linear\n      warmup_steps: 1000\n    seed: 42\n\n  - run_id: gpt2-small_dpsm-cosine\n    task_type: language_modeling\n    dataset:\n      name: wikitext-2\n      seq_length: 1024\n    model:\n      name: gpt2-small\n      from_pretrained: false\n    training:\n      epochs: 10\n      batch_size: 16\n      learning_rate: 5e-4\n      loss: dpsm-cosine\n      warmup_steps: 1000\n    seed: 42\n\n  - run_id: gpt2-small_dpsm-fixed-alpha0.5\n    task_type: language_modeling\n    dataset:\n      name: wikitext-2\n      seq_length: 1024\n    model:\n      name: gpt2-small\n      from_pretrained: false\n    training:\n      epochs: 10\n      batch_size: 16\n      learning_rate: 5e-4\n      loss: dpsm-fixed-alpha0.5\n      warmup_steps: 1        # not used for fixed-alpha but kept for interface\n    seed: 42\n\n  # ----------------------------------------------------------------------------------\n  # GPT-2 MEDIUM (≈355 M)\n  # ----------------------------------------------------------------------------------\n  - run_id: gpt2-medium_ce-baseline\n    task_type: language_modeling\n    dataset:\n      name: wikitext-2\n      seq_length: 1024\n    model:\n      name: gpt2-medium\n      from_pretrained: false\n    training:\n      epochs: 10\n      batch_size: 8\n      learning_rate: 5e-4\n      loss: ce\n      warmup_steps: 1000\n    seed: 43\n\n  - run_id: gpt2-medium_brier-baseline\n    task_type: language_modeling\n    dataset:\n      name: wikitext-2\n      seq_length: 1024\n    model:\n      name: gpt2-medium\n      from_pretrained: false\n    training:\n      epochs: 10\n      batch_size: 8\n      learning_rate: 5e-4\n      loss: brier\n      warmup_steps: 1000\n    seed: 43\n\n  - run_id: gpt2-medium_dpsm-linear\n    task_type: language_modeling\n    dataset:\n      name: wikitext-2\n      seq_length: 1024\n    model:\n      name: gpt2-medium\n      from_pretrained: false\n    training:\n      epochs: 10\n      batch_size: 8\n      learning_rate: 5e-4\n      loss: dpsm-linear\n      warmup_steps: 1000\n    seed: 43\n\n  - run_id: gpt2-medium_dpsm-cosine\n    task_type: language_modeling\n    dataset:\n      name: wikitext-2\n      seq_length: 1024\n    model:\n      name: gpt2-medium\n      from_pretrained: false\n    training:\n      epochs: 10\n      batch_size: 8\n      learning_rate: 5e-4\n      loss: dpsm-cosine\n      warmup_steps: 1000\n    seed: 43\n\n  - run_id: gpt2-medium_dpsm-fixed-alpha0.5\n    task_type: language_modeling\n    dataset:\n      name: wikitext-2\n      seq_length: 1024\n    model:\n      name: gpt2-medium\n      from_pretrained: false\n    training:\n      epochs: 10\n      batch_size: 8\n      learning_rate: 5e-4\n      loss: dpsm-fixed-alpha0.5\n      warmup_steps: 1\n    seed: 43\n\n  # ----------------------------------------------------------------------------------\n  # GPT-2 LARGE (≈774 M)\n  # ----------------------------------------------------------------------------------\n  - run_id: gpt2-large_ce-baseline\n    task_type: language_modeling\n    dataset:\n      name: wikitext-2\n      seq_length: 1024\n    model:\n      name: gpt2-large\n      from_pretrained: false\n    training:\n      epochs: 10\n      batch_size: 4\n      learning_rate: 5e-4\n      loss: ce\n      warmup_steps: 1000\n    seed: 44\n\n  - run_id: gpt2-large_brier-baseline\n    task_type: language_modeling\n    dataset:\n      name: wikitext-2\n      seq_length: 1024\n    model:\n      name: gpt2-large\n      from_pretrained: false\n    training:\n      epochs: 10\n      batch_size: 4\n      learning_rate: 5e-4\n      loss: brier\n      warmup_steps: 1000\n    seed: 44\n\n  - run_id: gpt2-large_dpsm-linear\n    task_type: language_modeling\n    dataset:\n      name: wikitext-2\n      seq_length: 1024\n    model:\n      name: gpt2-large\n      from_pretrained: false\n    training:\n      epochs: 10\n      batch_size: 4\n      learning_rate: 5e-4\n      loss: dpsm-linear\n      warmup_steps: 1000\n    seed: 44\n\n  - run_id: gpt2-large_dpsm-cosine\n    task_type: language_modeling\n    dataset:\n      name: wikitext-2\n      seq_length: 1024\n    model:\n      name: gpt2-large\n      from_pretrained: false\n    training:\n      epochs: 10\n      batch_size: 4\n      learning_rate: 5e-4\n      loss: dpsm-cosine\n      warmup_steps: 1000\n    seed: 44\n\n  - run_id: gpt2-large_dpsm-fixed-alpha0.5\n    task_type: language_modeling\n    dataset:\n      name: wikitext-2\n      seq_length: 1024\n    model:\n      name: gpt2-large\n      from_pretrained: false\n    training:\n      epochs: 10\n      batch_size: 4\n      learning_rate: 5e-4\n      loss: dpsm-fixed-alpha0.5\n      warmup_steps: 1\n    seed: 44"
}
