
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Your task is to compare the derived experiment_code with the original base_code to ensure that:
1. No important functionality has been omitted or truncated
2. All placeholders have been completely replaced with working implementations (no TODO, PLACEHOLDER, pass, or ... allowed)
3. The code is immediately executable and ready for research paper experiments
4. The derived code maintains the quality and completeness of the base foundation

# Instructions

## Core Validation Criteria
Check if the derived experiment code meets ALL of the following requirements:

1. **Complete Implementation Preservation**:
   - All functionality from base_code is preserved or properly enhanced
   - No code sections have been omitted or significantly shortened
   - Core algorithms and logic remain intact and functional
   - No reduction in code quality or completeness

2. **Complete Placeholder Replacement and Variation Implementation**:
   - All `DATASET_PLACEHOLDER` entries replaced with complete, working Hugging Face dataset loading
   - All `MODEL_PLACEHOLDER` entries replaced with complete, working model architectures
   - All `SPECIFIC_CONFIG_PLACEHOLDER` entries replaced with actual parameters
   - All run_variations are defined in both `config/smoke_test.yaml` and `config/full_experiment.yaml`
   - All run_variations are implemented in `src/model.py`
   - `config/smoke_test.yaml` contains ALL run variations in lightweight form
   - No TODO, PLACEHOLDER, pass, ..., or any incomplete implementations remain

3. **Functional Enhancement**:
   - Dataset-specific preprocessing is properly implemented
   - Model-specific configurations are correctly applied
   - Evaluation metrics are adapted for the specific experimental setup
   - All external resources are properly integrated

4. **Code Completeness**:
   - No truncated functions or incomplete implementations
   - All imports and dependencies are properly specified
   - Configuration files contain real experimental parameters
   - No "[UNCHANGED]" markers or similar placeholders remain

5. **Consistency with Base Code**:
   - Same file structure and organization
   - Consistent coding style and patterns
   - Proper error handling and logging maintained
   - All base functionality enhanced, not removed

## Detection of Common Issues
Flag the following problems if found:

- **Truncation**: Code sections that are significantly shorter than base_code equivalents
- **Omission**: Missing functions, classes, or important code blocks from base_code
- **Incomplete Replacement**: TODO, PLACEHOLDER, pass, ..., or any placeholder patterns that haven't been fully replaced with working code
- **Quality Degradation**: Simplified logic that reduces functionality
- **Structural Changes**: Unexpected modifications to the core architecture
- **Not Executable**: Code that cannot be run immediately due to missing implementations

## Output Format
Respond with a JSON object containing:
- `is_experiment_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `experiment_code_issue`: string - specific issues found if any criteria are not met

# Current Research Method
{
    "Open Problems": "Strictly proper scoring rules such as the Brier and Spherical scores improve calibration and downstream generation quality, but: 1) they converge noticeably slower than the standard logarithmic (cross-entropy) loss when training from scratch; 2) practitioners must choose between fast learning (log-loss) and better calibration/generation (Brier/Spherical). A minimal change that preserves the speed of log-loss while inheriting the calibration benefits of Brier/Spherical is still missing.",
    "Methods": "Dynamic Proper-Score Mixing (DPSM)\n1. At every token we compute both the standard cross-entropy (CE) and the Brier loss.\n2. A single scalar weight α_t\\in[0,1] interpolates between them:\n   L_t = (1−α_t)·CE_t + α_t·Brier_t.\n3. α_t is scheduled to rise smoothly from 0 to 1 during training, e.g.\n   α_t = min(1, step / warmup_steps)  (linear)  or  α_t = 0.5·(1−cos(π·step/warmup_steps))  (cosine).\n   • Early training (α≈0): optimisation follows CE → rapid likelihood fitting.\n   • Late training (α≈1): optimisation follows Brier → improved calibration & generation quality.\n4. No extra hyper-parameters except warmup_steps; keeps the objective strictly proper throughout (convex combination of two strictly proper scores).",
    "Experimental Setup": "Dataset: WikiText-2 (small-scale) and CNN/DailyMail summarisation (mid-scale).\nModels:  \n• GPT-2-small trained from scratch on WikiText-2.  \n• BART-base fine-tuned on CNN/DailyMail.\nBaselines: (i) CE only, (ii) Brier only, (iii) DPSM (ours).\nMetrics:  \n• Perplexity (PPL) for language modelling.  \n• Expected Calibration Error (ECE, 10 bins).  \n• ROUGE-1/2/L for summarisation.\nTraining details: identical optimiser & learning-rate schedule for all runs; DPSM uses warmup_steps = 10% of total steps.",
    "Experimental Code": "import torch, torch.nn.functional as F\nclass DPSMLoss(torch.nn.Module):\n    def __init__(self, vocab_size:int, warmup_steps:int=10000, schedule:str='linear'):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.warmup = warmup_steps\n        self.schedule = schedule\n        self.register_buffer('global_step', torch.tensor(0.))\n    def _alpha(self):\n        s = self.global_step\n        if self.schedule=='linear':\n            return torch.clamp(s / self.warmup, 0., 1.)\n        # cosine schedule\n        x = torch.clamp(s / self.warmup, 0., 1.)\n        return 0.5 * (1 - torch.cos(torch.pi * x))\n    def forward(self, logits, targets):\n        self.global_step += 1\n        log_p = F.log_softmax(logits, -1)              # [B,T,V]\n        p = log_p.exp()\n        ce = F.nll_loss(log_p.view(-1,log_p.size(-1)), targets.view(-1), reduction='none')\n        one_hot = F.one_hot(targets, self.vocab_size).type_as(p)\n        brier = ((p - one_hot)**2).sum(-1).view_as(ce)\n        alpha = self._alpha()\n        loss = (1-alpha)*ce + alpha*brier\n        return loss.mean()",
    "Expected Result": "1. WikiText-2: DPSM matches CE perplexity after full training (±0.2 PPL) while reducing ECE by ~35% and converging 1.8× faster than pure Brier.\n2. CNN/DailyMail: DPSM improves ROUGE-L by ≈0.7 points over CE and ≈0.3 over Brier, with noticeably better calibration (-30% ECE).\n3. Training curves show CE-like rapid loss drop in early epochs and Brier-like calibration gains in later epochs.",
    "Expected Conclusion": "A single-line interpolation between cross-entropy and Brier scores delivers the best of both worlds: fast convergence and superior calibration/generation quality. Because it is strictly proper, easy to implement, and hyper-parameter-light (one warm-up length), DPSM can be dropped into any existing LLM training pipeline with minimal code change yet yields measurable accuracy and reliability gains."
}

# Experimental Design
## Experiment Strategy
Unified Experimental Strategy for Validating Dynamic Proper-Score Mixing (DPSM)

1. Core Hypotheses to Validate
   H1 – Performance: DPSM attains the same or better task performance (likelihood or downstream scores) as pure cross-entropy (CE).
   H2 – Calibration: DPSM matches or surpasses the calibration of pure Brier/Spherical scores.
   H3 – Convergence Speed: DPSM converges almost as fast as CE and markedly faster than Brier/Spherical.
   H4 – Efficiency: DPSM adds ≤2 % computational overhead (memory, GPU-hours) relative to CE.
   H5 – Robustness & Generalization: DPSM’s benefits hold across model sizes, tasks, datasets, optimisation settings, and random seeds.

2. Validation Dimensions & Metrics
   a. Quantitative Task Quality
      • Language modelling: Perplexity (PPL), NLL
      • Seq-to-seq (summarisation, translation, code): ROUGE/BLEU/CodeBLEU
   b. Calibration
      • ECE (10 & 100 bins), Brier score, reliability diagrams
   c. Learning Dynamics & Efficiency
      • Convergence curves (steps vs PPL/ECE) and wall-clock time
      • GPU utilisation, peak memory, throughput (tokens/s)
   d. Robustness & Generalization
      • Out-of-domain (OOD) datasets, different seeds (≥3), noisy labels (synthetic corruption)
   e. Qualitative Analysis
      • Human preference ratings on generated text, diversity (Distinct-n)

3. Comparison Grid (applied to every experiment)
   Baselines:
   • CE only (fast learning)
   • Brier only (strong calibration)
   • CE + temperature scaling post-hoc (industry standard fix)
   • Label smoothing & focal loss (alternative in-loss fixes)
   Ablations:
   • DPSM with fixed α (e.g., 0.25, 0.5, 0.75)
   • DPSM with linear vs cosine schedule
   • Warm-up length sweep (5 %, 10 %, 20 %)

4. Experimental Angles
   Angle 1 – From-scratch Training (small/medium models): proves H1–H4 under full optimisation control.
   Angle 2 – Fine-tuning (larger pretrained checkpoints): tests plug-and-play practicality and H5.
   Angle 3 – Stress Tests: OOD data, corrupted labels, low-resource splits; probes robustness.
   Angle 4 – Scaling: repeat Angle 1 at three model scales (≈125 M, 355 M, 1.3 B params) to expose trends.
   Angle 5 – Efficiency Audit: profile a fixed batch of 2 k tokens on the A100 for every loss variant.

5. Success Criteria (must satisfy simultaneously)
   • Task quality: ≤0.3 PPL gap or ≥+0.3 ROUGE/BLEU relative to CE; never worse than both CE & Brier.
   • Calibration: ≥25 % ECE reduction vs CE and within 5 % of Brier.
   • Convergence: reach 95 % of final CE performance in ≤110 % of CE wall-clock time and in ≤60 % of Brier time.
   • Efficiency: memory overhead ≤500 MB; throughput drop ≤2 %.
   • Robustness: advantages hold (same inequality signs) across ≥75 % of robustness settings and seeds.

6. Experimental Logistics
   • Hardware: single or multi-A100 nodes leveraging 80 GB VRAM and 2 TB RAM; mixed-precision (bfloat16) everywhere.
   • Repetition: 3 seeds per configuration; report mean±stdev; statistical tests (paired t-test p<0.05).
   • Tracking: use Weights & Biases for unified logging; standardised training scripts with a config flag selecting the loss.
   • Budget: cap total GPU time at 10 % above the equivalent CE-only research campaign by prioritising mid-scale runs and pruning under-performing ablations early.

7. Reporting Template (uniform across papers/sections)
   1. Learning curves (PPL & ECE) with wall-clock on x-axis
   2. Final metrics table (means, stdev, significance stars)
   3. Efficiency table (time/step, memory, tokens/s)
   4. Reliability diagrams (CE vs DPSM)
   5. Qualitative snippet comparison & human study results

This unified, multi-angle strategy ensures that every experiment—regardless of dataset, model, or training regime—jointly interrogates DPSM’s main claims: fast learning like CE, calibration like Brier, negligible overhead, and broad robustness.

# Base Code (Reference Foundation)
{"evaluate_py": "#!/usr/bin/env python\n\"\"\"src/evaluate.py\nAggregates results from multiple experimental runs (i.e. sub-directories of the\n`results_dir`) and generates comparison figures + a JSON summary printed to\nstdout.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List, Any\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nfrom .utils import save_pdf\n\n\n# --------------------------------------------------------------------------------------\n# CLI \u0026 helpers\n# --------------------------------------------------------------------------------------\n\ndef _collect_results(results_dir: Path) -\u003e pd.DataFrame:\n    rows: List[Dict[str, Any]] = []\n    for sub in results_dir.iterdir():\n        if not (sub / \"epoch_metrics.json\").exists():\n            continue  # skip non-run folders\n        with (sub / \"epoch_metrics.json\").open() as f:\n            epoch_metrics = json.load(f)\n        final = epoch_metrics[-1]\n        rows.append(\n            {\n                \"run_id\": sub.name,\n                \"val_ppl\": final[\"val_ppl\"],\n                \"val_ece\": final[\"val_ece\"],\n            }\n        )\n    if not rows:\n        raise RuntimeError(f\"No result folders found in {results_dir}\")\n    return pd.DataFrame(rows)\n\n\ndef _plot_bar(df: pd.DataFrame, metric: str, images_dir: Path):\n    plt.figure(figsize=(max(4, len(df) * 1.5), 4))\n    sns.barplot(x=\"run_id\", y=metric, data=df, palette=\"viridis\")\n    for i, v in enumerate(df[metric]):\n        plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.title(f\"Final {metric.upper()} Comparison\")\n    plt.xlabel(\"Run ID\")\n    plt.ylabel(metric.upper())\n    plt.tight_layout()\n    save_pdf(plt, images_dir / f\"{metric}_comparison.pdf\")\n\n\n# --------------------------------------------------------------------------------------\n# Main\n# --------------------------------------------------------------------------------------\n\ndef main() -\u003e None:\n    parser = argparse.ArgumentParser(description=\"Evaluate all experiment variations\")\n    parser.add_argument(\"--results-dir\", required=True, type=str, help=\"Directory with all runs\u0027 sub-folders\")\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir)\n    images_dir = results_dir / \"images\"\n    images_dir.mkdir(exist_ok=True)\n\n    df = _collect_results(results_dir)\n\n    # ----- Figures -----\n    for metric in [\"val_ppl\", \"val_ece\"]:\n        _plot_bar(df, metric, images_dir)\n\n    # ----- JSON summary -----\n    summary = df.to_dict(orient=\"list\")\n    print(json.dumps(summary, indent=None))\n\n\nif __name__ == \"__main__\":\n    main()\n", "full_experiment_yaml": "# config/full_experiment.yaml\n# TEMPLATE \u2014 will be filled with real datasets, models, and hyper-parameters in\na later pipeline stage. The placeholders *must* be replaced before running the\nfull experiment.\n\nexperiments:\n  - run_id: DATASET_PLACEHOLDER_ce\n    task_type: TASK_TYPE_PLACEHOLDER           # e.g. language_modeling | seq2seq\n    dataset:\n      name: DATASET_PLACEHOLDER                # PLACEHOLDER: specific dataset name\n      # PLACEHOLDER: add dataset-specific parameters here (paths, splits, ...)\n    model:\n      name: MODEL_PLACEHOLDER                  # PLACEHOLDER: specific model identifier\n      # PLACEHOLDER: model-specific hyper-parameters (layers, dims, ...)\n    training:\n      epochs: SPECIFIC_CONFIG_PLACEHOLDER\n      batch_size: SPECIFIC_CONFIG_PLACEHOLDER\n      learning_rate: SPECIFIC_CONFIG_PLACEHOLDER\n      loss: ce\n      warmup_steps: SPECIFIC_CONFIG_PLACEHOLDER\n    seed: SPECIFIC_CONFIG_PLACEHOLDER\n\n  - run_id: DATASET_PLACEHOLDER_dpsm\n    task_type: TASK_TYPE_PLACEHOLDER\n    dataset:\n      name: DATASET_PLACEHOLDER\n    model:\n      name: MODEL_PLACEHOLDER\n    training:\n      epochs: SPECIFIC_CONFIG_PLACEHOLDER\n      batch_size: SPECIFIC_CONFIG_PLACEHOLDER\n      learning_rate: SPECIFIC_CONFIG_PLACEHOLDER\n      loss: dpsm\n      warmup_steps: SPECIFIC_CONFIG_PLACEHOLDER\n      schedule: cosine     # example alternative schedule\n    seed: SPECIFIC_CONFIG_PLACEHOLDER\n\n  # Additional baselines / ablations can be appended following the same schema.\n", "main_py": "#!/usr/bin/env python\n\"\"\"src/main.py\nMaster orchestrator.  \nUsage:\n  uv run python -m src.main --smoke-test  --results-dir \u003cpath\u003e\n  uv run python -m src.main --full-experiment --results-dir \u003cpath\u003e\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport shutil\nimport subprocess\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List, Any\n\nimport yaml\n\n# --------------------------------------------------------------------------------------\n# IO helpers\n# --------------------------------------------------------------------------------------\n\ndef _tee_stream(stream, log_file):\n    \"\"\"Forward *stream* (stdout/stderr of subprocess) to both terminal and file.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        sys.stdout.buffer.write(line)\n        log_file.buffer.write(line)\n        sys.stdout.flush()\n        log_file.flush()\n\n\ndef _run_subprocess(cmd: List[str], env: Dict[str, str], stdout_path: Path, stderr_path: Path):\n    with stdout_path.open(\"wb\") as out_f, stderr_path.open(\"wb\") as err_f:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n        # Real-time tee\n        import threading\n\n        t_out = threading.Thread(target=_tee_stream, args=(proc.stdout, out_f))\n        t_err = threading.Thread(target=_tee_stream, args=(proc.stderr, err_f))\n        t_out.start(); t_err.start()\n        proc.wait()\n        t_out.join(); t_err.join()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Subprocess failed with code {proc.returncode}: {\u0027 \u0027.join(cmd)}\")\n\n\n# --------------------------------------------------------------------------------------\n# Main\n# --------------------------------------------------------------------------------------\n\ndef main() -\u003e None:\n    parser = argparse.ArgumentParser(description=\"Run all experiment variations sequentially\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\")\n    parser.add_argument(\"--results-dir\", required=True, type=str, help=\"Directory to store all outputs\")\n    args = parser.parse_args()\n\n    cfg_path = Path(\"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\")\n    with cfg_path.open() as f:\n        master_cfg = yaml.safe_load(f)\n\n    experiments: List[Dict[str, Any]] = master_cfg[\"experiments\"]\n    results_root = Path(args.results_dir)\n    if results_root.exists():\n        # Allow re-runs: remove previous contents\n        shutil.rmtree(results_root)\n    results_root.mkdir(parents=True)\n\n    # ------------------------------------------------------------------\n    # Run sequentially\n    # ------------------------------------------------------------------\n    for exp in experiments:\n        run_id = exp[\"run_id\"]\n        run_dir = results_root / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n        # Dump per-run config (visible to train.py only)\n        run_cfg_path = run_dir / \"config.yaml\"\n        with run_cfg_path.open(\"w\") as f:\n            yaml.safe_dump(exp, f)\n\n        # Subprocess call\n        cmd = [\n            sys.executable,\n            \"-m\",\n            \"src.train\",\n            \"--config\",\n            str(run_cfg_path),\n            \"--results-dir\",\n            str(results_root),\n        ]\n        env = os.environ.copy()\n        _run_subprocess(cmd, env, run_dir / \"stdout.log\", run_dir / \"stderr.log\")\n\n    # ------------------------------------------------------------------\n    # After all runs \u2013 aggregate \u0026 evaluate\n    # ------------------------------------------------------------------\n    eval_cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.evaluate\",\n        \"--results-dir\",\n        str(results_root),\n    ]\n    _run_subprocess(eval_cmd, os.environ.copy(), results_root / \"evaluate_stdout.log\", results_root / \"evaluate_stderr.log\")\n\n    print(\"All experiments completed successfully.\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "#!/usr/bin/env python\n\"\"\"src/model.py\nCommon model architectures and loss functions, including the proposed DPSM loss.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom typing import Dict\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n# --------------------------------------------------------------------------------------\n# Utility functions\n# --------------------------------------------------------------------------------------\n\ndef expected_calibration_error(probs: torch.Tensor, labels: torch.Tensor, num_bins: int = 10) -\u003e torch.Tensor:\n    \"\"\"Compute Expected Calibration Error (vectorised, differentiable).\n    Args:\n        probs: [N, V] probability distribution over classes for each sample.\n        labels: [N] ground-truth indices.\n    Returns:\n        ECE scalar tensor (on same device as probs).\n    \"\"\"\n    confidences, predictions = probs.max(dim=1)\n    accuracies = predictions.eq(labels)\n\n    ece = torch.zeros(1, device=probs.device)\n    bin_boundaries = torch.linspace(0, 1, num_bins + 1, device=probs.device)\n    for i in range(num_bins):\n        lo, hi = bin_boundaries[i], bin_boundaries[i + 1]\n        mask = (confidences \u003e lo) \u0026 (confidences \u003c= hi)\n        if mask.any():\n            bin_acc = accuracies[mask].float().mean()\n            bin_conf = confidences[mask].mean()\n            ece += (mask.float().mean()) * torch.abs(bin_conf - bin_acc)\n    return ece\n\n\n# --------------------------------------------------------------------------------------\n# Dummy language model (small \u0026 dependency-free \u2013 perfect for smoke tests)\n# --------------------------------------------------------------------------------------\n\n\nclass DummyLanguageModel(nn.Module):\n    \"\"\"A tiny LSTM-based LM supporting *any* vocab size (\u22652).\"\"\"\n\n    def __init__(self, vocab_size: int, embedding_dim: int = 64, hidden_dim: int = 128, num_layers: int = 1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n        self.proj = nn.Linear(hidden_dim, vocab_size)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            nn.init.normal_(module.weight, mean=0, std=0.02)\n\n    def forward(self, input_ids: torch.Tensor):  # [B, T]\n        x = self.embed(input_ids)  # [B, T, D]\n        out, _ = self.lstm(x)\n        logits = self.proj(out)  # [B, T, V]\n        return logits\n\n\n# --------------------------------------------------------------------------------------\n# Loss functions\n# --------------------------------------------------------------------------------------\n\nclass CrossEntropyLoss(nn.Module):\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n        return F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n\n\nclass BrierLoss(nn.Module):\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n        p = logits.softmax(-1)\n        one_hot = F.one_hot(targets, logits.size(-1)).type_as(p)\n        loss = (p - one_hot).pow(2).sum(-1)\n        return loss.mean()\n\n\nclass DPSMLoss(nn.Module):\n    \"\"\"Dynamic Proper-Score Mixing (CE \u2194 Brier).\"\"\"\n\n    def __init__(self, vocab_size: int, warmup_steps: int = 1000, schedule: str = \"linear\"):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.warmup_steps = warmup_steps\n        self.schedule = schedule.lower()\n        self.register_buffer(\"global_step\", torch.tensor(0.0))\n\n    def _alpha(self):\n        x = torch.clamp(self.global_step / self.warmup_steps, 0.0, 1.0)\n        if self.schedule == \"linear\":\n            return x\n        elif self.schedule == \"cosine\":\n            return 0.5 * (1 - torch.cos(math.pi * x))\n        else:\n            raise ValueError(f\"Unknown schedule \u0027{self.schedule}\u0027\")\n\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n        self.global_step += 1.0\n        ce = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), reduction=\"none\")\n        p = logits.softmax(-1)\n        one_hot = F.one_hot(targets, logits.size(-1)).type_as(p)\n        brier = (p - one_hot).pow(2).sum(-1)\n        alpha = self._alpha()\n        loss = (1 - alpha) * ce + alpha * brier\n        return loss.mean()\n\n\n# --------------------------------------------------------------------------------------\n# Factories (public API)\n# --------------------------------------------------------------------------------------\n\ndef get_model(cfg: Dict, vocab_size: int) -\u003e nn.Module:\n    model_cfg = cfg[\"model\"]\n    name = model_cfg[\"name\"]\n    if name == \"dummy\":\n        return DummyLanguageModel(\n            vocab_size=vocab_size,\n            embedding_dim=model_cfg.get(\"embedding_dim\", 64),\n            hidden_dim=model_cfg.get(\"hidden_dim\", 128),\n            num_layers=model_cfg.get(\"num_layers\", 1),\n        )\n    else:\n        # PLACEHOLDER: Will be replaced with task- / model-specific implementation\n        raise NotImplementedError(f\"Model \u0027{name}\u0027 not implemented (placeholder)\")\n\n\ndef get_loss_fn(cfg: Dict, vocab_size: int, device: torch.device):\n    loss_name = cfg[\"training\"][\"loss\"].lower()\n    if loss_name == \"ce\":\n        return CrossEntropyLoss().to(device)\n    elif loss_name == \"brier\":\n        return BrierLoss().to(device)\n    elif loss_name == \"dpsm\":\n        schedule = cfg[\"training\"].get(\"schedule\", \"linear\")\n        warmup = cfg[\"training\"].get(\"warmup_steps\", 1000)\n        return DPSMLoss(vocab_size=vocab_size, warmup_steps=warmup, schedule=schedule).to(device)\n    else:\n        # PLACEHOLDER: Add focal loss, label smoothing, etc. in specialised phase.\n        raise NotImplementedError(f\"Loss \u0027{loss_name}\u0027 not implemented (placeholder)\")\n", "preprocess_py": "#!/usr/bin/env python\n\"\"\"src/preprocess.py\nCommon data-loading and preprocessing utilities with a **structured placeholder\nstrategy** for dataset-specific code.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport random\nfrom pathlib import Path\nfrom typing import Dict, Tuple\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\n\n# --------------------------------------------------------------------------------------\n# Dummy dataset (used for smoke tests) \u2013 always available\n# --------------------------------------------------------------------------------------\n\n\nclass DummyLanguageModelingDataset(Dataset):\n    \"\"\"Creates random token sequences for next-token prediction.\"\"\"\n\n    def __init__(self, num_samples: int, seq_length: int, vocab_size: int):\n        super().__init__()\n        self.num_samples = num_samples\n        self.seq_length = seq_length\n        self.vocab_size = vocab_size\n        # Pre-generate data for determinism \u0026 speed\n        rng = random.Random(0)\n        self.data = [\n            torch.tensor([rng.randint(1, vocab_size - 1) for _ in range(seq_length)])\n            for _ in range(num_samples)\n        ]\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        x = self.data[idx]\n        return x[:-1], x[1:]  # input, target\n\n\n# --------------------------------------------------------------------------------------\n# Public API\n# --------------------------------------------------------------------------------------\n\ndef load_dataset(cfg: Dict, split_ratio: float = 0.9) -\u003e Tuple[DataLoader, DataLoader, int]:\n    \"\"\"Returns (train_loader, val_loader, vocab_size).\n\n    All task-specific details are abstracted behind this function. REAL datasets\n    will be injected in the derive-specific phase by replacing the placeholders\n    marked below.\n    \"\"\"\n\n    task_type = cfg[\"task_type\"]\n    dscfg = cfg[\"dataset\"]\n    batch_size = cfg[\"training\"][\"batch_size\"]\n\n    # ------------------------------------------------------------------\n    # Placeholders for dataset-specific loading logic\n    # ------------------------------------------------------------------\n    if dscfg[\"name\"] == \"dummy\":\n        dataset = DummyLanguageModelingDataset(\n            num_samples=dscfg.get(\"num_samples\", 1024),\n            seq_length=dscfg.get(\"seq_length\", 32),\n            vocab_size=dscfg.get(\"vocab_size\", 256),\n        )\n        vocab_size = dscfg.get(\"vocab_size\", 256)\n    else:\n        # PLACEHOLDER: Will be replaced with specific dataset loading logic\n        raise NotImplementedError(\n            \"Dataset \u0027{}\u0027 not yet implemented (placeholder).\".format(dscfg[\"name\"])\n        )\n\n    # ------------------------------------------------------------------\n    # Split train/val\n    # ------------------------------------------------------------------\n    n_train = int(len(dataset) * split_ratio)\n    n_val = len(dataset) - n_train\n    train_set, val_set = torch.utils.data.random_split(dataset, [n_train, n_val])\n\n    # Simple collate (works for dummy LM)\n    def collate(batch):\n        inputs, targets = zip(*batch)\n        return torch.stack(inputs), torch.stack(targets)\n\n    train_loader = DataLoader(\n        train_set,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True,\n        collate_fn=collate,\n    )\n    val_loader = DataLoader(\n        val_set,\n        batch_size=batch_size,\n        shuffle=False,\n        drop_last=False,\n        collate_fn=collate,\n    )\n\n    return train_loader, val_loader, vocab_size\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=65.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"dpsm-core\"\nversion = \"0.1.0\"\ndescription = \"Common Core Foundation for DPSM experiments\"\nrequires-python = \"\u003e=3.9\"\nlicense = {text = \"Apache-2.0\"}\nauthors = [\n    {name = \"OpenAI Research Automation\", email = \"noreply@example.com\"}\n]\n\n[project.dependencies]\ntorch = \"*\"\ntqdm = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\npandas = \"*\"\n\n# Optional (for future specialised experiments)\ntransformers = {version = \"*\", optional = true}\ndatasets = {version = \"*\", optional = true}\nrouge-score = {version = \"*\", optional = true}\n\n[project.optional-dependencies]\nhf = [\"transformers\", \"datasets\", \"sentencepiece\", \"rouge-score\"]\n", "smoke_test_yaml": "# config/smoke_test.yaml\n# Lightweight end-to-end sanity check. Runs on a tiny *synthetic* dataset so\n# that CI can finish within seconds.\n\nexperiments:\n  - run_id: dummy_ce\n    task_type: language_modeling\n    dataset:\n      name: dummy            # built-in dataset \u2013 no external downloads\n      vocab_size: 128\n      seq_length: 32\n      num_samples: 256\n    model:\n      name: dummy            # tiny LSTM defined in src/model.py\n      vocab_size: 128\n      embedding_dim: 64\n      hidden_dim: 128\n    training:\n      epochs: 1\n      batch_size: 8\n      learning_rate: 1e-3\n      loss: ce\n      warmup_steps: 10\n    seed: 42\n\n  - run_id: dummy_dpsm\n    task_type: language_modeling\n    dataset:\n      name: dummy\n      vocab_size: 128\n      seq_length: 32\n      num_samples: 256\n    model:\n      name: dummy\n      vocab_size: 128\n      embedding_dim: 64\n      hidden_dim: 128\n    training:\n      epochs: 1\n      batch_size: 8\n      learning_rate: 1e-3\n      loss: dpsm\n      warmup_steps: 10\n      schedule: linear\n    seed: 42\n", "train_py": "#!/usr/bin/env python\n\"\"\"src/train.py\nRuns a single experiment variation.  \nThis script is *invoked as a subprocess* by src/main.py so that every run has an\nisolated Python interpreter and clean GPU memory.  The CLI is intentionally\nminimal \u2013 **all run-specific information is provided through an on-disk YAML\nfile** produced by main.py.\n\nStandard-output protocol (MUST NOT CHANGE \u2013 relied upon by evaluate.py \u0026 CI)\n1. Human-readable experiment description (multi-line, free-form).\n2. A single **JSON line** with the structure documented below \u2013 this is parsed\n   by main.py \u0026 evaluate.py.\n\n{\n  \"run_id\": \"\u003cunique name from YAML\u003e\",\n  \"epoch_metrics\": [\n      {\"epoch\": 1, \"train_loss\": 4.83, \"val_ppl\": 125.1, \"val_ece\": 0.38},\n      ...\n  ],\n  \"final\":        {\"val_ppl\": 37.2, \"val_ece\": 0.09, \"wall_clock\": 713.4}\n}\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom .preprocess import load_dataset\nfrom .model import (\n    get_model,\n    get_loss_fn,\n    expected_calibration_error,\n)\nfrom .utils import set_seed, save_pdf\n\n# --------------------------------------------------------------------------------------\n# Helper functions\n# --------------------------------------------------------------------------------------\n\ndef _describe_experiment(cfg: Dict[str, Any]) -\u003e str:\n    ds = cfg[\"dataset\"][\"name\"]\n    model = cfg[\"model\"][\"name\"]\n    loss = cfg[\"training\"][\"loss\"]\n    epochs = cfg[\"training\"][\"epochs\"]\n    bs = cfg[\"training\"][\"batch_size\"]\n    return (\n        f\"Running experiment \u0027{cfg[\u0027run_id\u0027]}\u0027\\n\"\n        f\"  \u2022 Task       : {cfg[\u0027task_type\u0027]}\\n\"\n        f\"  \u2022 Dataset    : {ds}\\n\"\n        f\"  \u2022 Model      : {model}\\n\"\n        f\"  \u2022 Loss       : {loss}\\n\"\n        f\"  \u2022 Epochs     : {epochs}\\n\"\n        f\"  \u2022 Batch size : {bs}\\n\"\n    )\n\n\n# --------------------------------------------------------------------------------------\n# Training / Evaluation routines (model-agnostic)\n# --------------------------------------------------------------------------------------\n\ndef train_one_epoch(\n    model: nn.Module,\n    loss_fn: nn.Module,\n    data_loader: DataLoader,\n    optimizer: torch.optim.Optimizer,\n    device: torch.device,\n):\n    \"\"\"Train for exactly one epoch and return average training loss.\"\"\"\n\n    model.train()\n    total_loss = 0.0\n    total_tokens = 0\n\n    for batch in data_loader:\n        inputs, targets = [x.to(device) for x in batch]\n        optimizer.zero_grad(set_to_none=True)\n        logits = model(inputs)\n        loss = loss_fn(logits, targets)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * targets.numel()\n        total_tokens += targets.numel()\n\n    return total_loss / max(total_tokens, 1)\n\n\ndef evaluate(\n    model: nn.Module,\n    loss_fn: nn.Module,\n    data_loader: DataLoader,\n    device: torch.device,\n):\n    \"\"\"Return perplexity \u0026 ECE on the supplied validation / test split.\"\"\"\n\n    model.eval()\n    all_logits: List[torch.Tensor] = []\n    all_targets: List[torch.Tensor] = []\n    with torch.no_grad():\n        for batch in data_loader:\n            inputs, targets = [x.to(device) for x in batch]\n            logits = model(inputs)\n            _ = loss_fn(logits, targets)  # keep internal step counters consistent\n            all_logits.append(logits.detach())\n            all_targets.append(targets.detach())\n\n    logits = torch.cat(all_logits, dim=0)\n    targets = torch.cat(all_targets, dim=0)\n\n    ppl = torch.exp(\n        F.cross_entropy(\n            logits.view(-1, logits.size(-1)), targets.view(-1), reduction=\"mean\"\n        )\n    ).item()\n\n    probs = logits.softmax(-1).view(-1, logits.size(-1))\n    labels = targets.view(-1)\n    ece = expected_calibration_error(probs, labels, num_bins=10).item()\n\n    return ppl, ece\n\n\n# --------------------------------------------------------------------------------------\n# Main entry point\n# --------------------------------------------------------------------------------------\n\ndef main() -\u003e None:\n    parser = argparse.ArgumentParser(description=\"Train a single experimental run\")\n    parser.add_argument(\"--config\", type=str, required=True, help=\"Path to run config YAML\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory to save run-specific outputs\")\n    args = parser.parse_args()\n\n    import yaml  # local import to keep start-up time minimal\n\n    cfg: Dict[str, Any] = yaml.safe_load(Path(args.config).read_text())\n    run_id: str = cfg[\"run_id\"]\n    results_dir = Path(args.results_dir)\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n    images_dir = run_dir / \"images\"\n    images_dir.mkdir(exist_ok=True, parents=True)\n\n    # ------------------------------------------------------------------\n    # Reproducibility \u0026 device\n    # ------------------------------------------------------------------\n    seed = int(cfg.get(\"seed\", 42))\n    set_seed(seed)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ------------------------------------------------------------------\n    # Data\n    # ------------------------------------------------------------------\n    train_loader, val_loader, vocab_size = load_dataset(cfg)\n\n    # ------------------------------------------------------------------\n    # Model \u0026 Loss\n    # ------------------------------------------------------------------\n    model = get_model(cfg, vocab_size=vocab_size).to(device)\n    loss_fn = get_loss_fn(cfg, vocab_size=vocab_size, device=device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg[\"training\"][\"learning_rate\"])\n\n    # ------------------------------------------------------------------\n    # Training loop\n    # ------------------------------------------------------------------\n    num_epochs = int(cfg[\"training\"][\"epochs\"])\n    epoch_metrics: List[Dict[str, float]] = []\n    start_time = time.time()\n\n    print(_describe_experiment(cfg), flush=True)\n\n    for epoch in range(1, num_epochs + 1):\n        train_loss = train_one_epoch(model, loss_fn, train_loader, optimizer, device)\n        val_ppl, val_ece = evaluate(model, loss_fn, val_loader, device)\n\n        epoch_metrics.append(\n            {\n                \"epoch\": epoch,\n                \"train_loss\": train_loss,\n                \"val_ppl\": val_ppl,\n                \"val_ece\": val_ece,\n            }\n        )\n\n    wall_clock = time.time() - start_time\n\n    # ------------------------------------------------------------------\n    # Save artifacts\n    # ------------------------------------------------------------------\n    torch.save(model.state_dict(), run_dir / \"model.pt\")\n    with (run_dir / \"epoch_metrics.json\").open(\"w\") as f:\n        json.dump(epoch_metrics, f, indent=2)\n\n    # ----- Figures -----\n    import matplotlib.pyplot as plt\n\n    epochs = [m[\"epoch\"] for m in epoch_metrics]\n    losses = [m[\"train_loss\"] for m in epoch_metrics]\n    ppls = [m[\"val_ppl\"] for m in epoch_metrics]\n    eces = [m[\"val_ece\"] for m in epoch_metrics]\n\n    # Training loss curve\n    plt.figure(figsize=(6, 4))\n    plt.plot(epochs, losses, label=\"Train loss\")\n    plt.scatter(epochs[-1], losses[-1], color=\"red\")\n    plt.text(epochs[-1], losses[-1], f\"{losses[-1]:.2f}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training Loss Curve\")\n    plt.legend()\n    plt.tight_layout()\n    save_pdf(plt, images_dir / \"training_loss.pdf\")\n\n    # Validation PPL \u0026 ECE (twin axes)\n    fig, ax1 = plt.subplots(figsize=(6, 4))\n    ax2 = ax1.twinx()\n    ax1.plot(epochs, ppls, color=\"blue\", label=\"PPL\")\n    ax2.plot(epochs, eces, color=\"orange\", label=\"ECE\")\n    ax1.set_xlabel(\"Epoch\")\n    ax1.set_ylabel(\"Perplexity\", color=\"blue\")\n    ax2.set_ylabel(\"ECE\", color=\"orange\")\n\n    # annotate final values\n    ax1.scatter(epochs[-1], ppls[-1], color=\"blue\")\n    ax1.text(epochs[-1], ppls[-1], f\"{ppls[-1]:.2f}\")\n    ax2.scatter(epochs[-1], eces[-1], color=\"orange\")\n    ax2.text(epochs[-1], eces[-1], f\"{eces[-1]:.3f}\")\n\n    fig.suptitle(\"Validation Metrics\")\n    fig.tight_layout()\n    save_pdf(plt, images_dir / \"validation_metrics.pdf\")\n\n    # ------------------------------------------------------------------\n    # Print final metrics to STDOUT (machine-readable part)\n    # ------------------------------------------------------------------\n    final_payload = {\n        \"run_id\": run_id,\n        \"epoch_metrics\": epoch_metrics,\n        \"final\": {\n            \"val_ppl\": ppls[-1],\n            \"val_ece\": eces[-1],\n            \"wall_clock\": wall_clock,\n        },\n    }\n\n    print(json.dumps(final_payload), flush=True)\n\n\nif __name__ == \"__main__\":\n    main()\n"}

# Current Experiment (To be validated)
- Experiment ID: exp-2-seq2seq-finetune-robustness
- Description: Objective / hypothesis: Demonstrate plug-and-play benefit of DPSM during fine-tuning (H1, H2, H5) and test robustness to domain shift and noisy references.

Models: BART-large (406 M) and PEGASUS-large (568 M) pretrained checkpoints.

Datasets: CNN/DailyMail (train for fine-tune), XSum (zero-shot transfer), and distorted CNN/DM with 15 % summary-swap noise for robustness.

Pre-processing:  
• SentencePiece tokenisation (pretrained model vocab, 1024 tokens max src, 128 tokens max tgt).  
• Remove articles shorter than 50 words.  
• For noise study, with probability 0.15 replace reference summary with another random summary in the same batch.

Data split: CNN/DM official splits. XSum only used for evaluation (OOD).  
Cross-validation: 5-fold on training set for hyper-param searches, but main results use full train.

Repetitions: 3 seeds. Selection: last checkpoint (no early stopping) to mimic production; report also best-val for completeness.

Evaluation metrics:  
Primary – ROUGE-1/2/L (F1, ↑).  
Secondary – BERTScore, ECE (token-level), Brier score.  
Human eval – 200 random articles: pairwise preference test (fluency + informativeness).  
Efficiency – sec/step, memory, sentences/s.

Comparisons & run_variations:  
1. ce-finetune – vanilla negative log-likelihood.  
2. ce+tempscale – CE followed by post-hoc temperature on validation (industry baseline).  
3. brier-finetune – optimise pure Brier.  
4. dpsm-warm10 – DPSM, 10 % linear warm-up.  
5. dpsm-warm20 – DPSM, 20 % linear warm-up (sensitivity).

Hyper-parameter analysis: grid over learning_rate {1e-5, 3e-5, 5e-5}, label_smoothing {0, 0.1}, and warmup_steps {10 %, 20 %, 30 %} for dpsm-warm*. Record ROUGE/ECE heatmaps.

Robustness:  
• Domain shift – evaluate CNN/DM-trained models on XSum (zero-shot).  
• Noisy labels – train on 15 % swapped summaries; measure ROUGE drop Δ.  
• Adversarial input – apply TextFooler attacks on 500 CNN/DM validation articles; compute ROUGE and calibration under attack.

Efficiency audit: identical batch size (64 × 1024 tokens) on single A100; profile training for 1 000 steps. Report average forward+backward time, GPU utilisation, energy (via nvidia-smi).

Example code:
```
model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')
loss_variant = args.variant  # 'ce', 'brier', 'dpsm'
if loss_variant == 'ce+tempscale':
    model, T = finetune_ce_then_calibrate(model, train_loader, val_loader)
else:
    criterion = pick_loss(loss_variant, vocab_size=model.config.vocab_size)
    trainer = Seq2SeqTrainer(model, criterion, ...)
```
Expected outcome: dpsm-warm10 improves ROUGE-L by ≥0.5 over ce-finetune and ≥0.2 over brier-finetune, cuts ECE by ≥25 % vs CE, and maintains ≤2 % throughput drop. In noisy and OOD settings, DPSM variants degrade ≤70 % as much as CE, confirming robustness.
- Run Variations: ['ce-finetune', 'ce+tempscale', 'brier-finetune', 'dpsm-warm10', 'dpsm-warm20']

# Derived Experiment Code (To be validated)

{"evaluate_py": "#!/usr/bin/env python\n\"\"\"src/evaluate.py\nAggregates results from multiple experimental runs (i.e. sub-directories of the\n`results_dir`) and generates comparison figures + a JSON summary printed to\nstdout.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List, Any\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nfrom .utils import save_pdf\n\n\n# --------------------------------------------------------------------------------------\n# CLI \u0026 helpers\n# --------------------------------------------------------------------------------------\n\ndef _collect_results(results_dir: Path) -\u003e pd.DataFrame:\n    rows: List[Dict[str, Any]] = []\n    for sub in results_dir.iterdir():\n        if not (sub / \"epoch_metrics.json\").exists():\n            continue  # skip non-run folders\n        with (sub / \"epoch_metrics.json\").open() as f:\n            epoch_metrics = json.load(f)\n        final = epoch_metrics[-1]\n        rows.append(\n            {\n                \"run_id\": sub.name,\n                \"val_ppl\": final[\"val_ppl\"],\n                \"val_ece\": final[\"val_ece\"],\n            }\n        )\n    if not rows:\n        raise RuntimeError(f\"No result folders found in {results_dir}\")\n    return pd.DataFrame(rows)\n\n\ndef _plot_bar(df: pd.DataFrame, metric: str, images_dir: Path):\n    plt.figure(figsize=(max(4, len(df) * 1.5), 4))\n    sns.barplot(x=\"run_id\", y=metric, data=df, palette=\"viridis\")\n    for i, v in enumerate(df[metric]):\n        plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.title(f\"Final {metric.upper()} Comparison\")\n    plt.xlabel(\"Run ID\")\n    plt.ylabel(metric.upper())\n    plt.tight_layout()\n    save_pdf(plt, images_dir / f\"{metric}_comparison.pdf\")\n\n\n# --------------------------------------------------------------------------------------\n# Main\n# --------------------------------------------------------------------------------------\n\ndef main() -\u003e None:\n    parser = argparse.ArgumentParser(description=\"Evaluate all experiment variations\")\n    parser.add_argument(\"--results-dir\", required=True, type=str, help=\"Directory with all runs\u0027 sub-folders\")\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir)\n    images_dir = results_dir / \"images\"\n    images_dir.mkdir(exist_ok=True)\n\n    df = _collect_results(results_dir)\n\n    # ----- Figures -----\n    for metric in [\"val_ppl\", \"val_ece\"]:\n        _plot_bar(df, metric, images_dir)\n\n    # ----- JSON summary -----\n    summary = df.to_dict(orient=\"list\")\n    print(json.dumps(summary, indent=None))\n\n\nif __name__ == \"__main__\":\n    main()\n", "full_experiment_yaml": "# config/full_experiment.yaml\n# Full experimental grid for exp-2-seq2seq-finetune-robustness\n\nexperiments:\n  - run_id: ce-finetune\n    task_type: summarization\n    dataset:\n      name: cnn_dailymail\n      version: \"3.0.0\"\n      max_source_length: 512\n      max_target_length: 128\n      min_article_words: 50\n      noise_frac: 0.0\n      tokenizer_name: facebook/bart-large-cnn\n    model:\n      name: facebook/bart-large-cnn\n    training:\n      epochs: 3\n      batch_size: 2          # fits on a single A100 with fp16\n      learning_rate: 3e-5\n      loss: ce\n      warmup_steps: 500\n    seed: 42\n\n  - run_id: ce+tempscale\n    task_type: summarization\n    dataset:\n      name: cnn_dailymail\n      version: \"3.0.0\"\n      max_source_length: 512\n      max_target_length: 128\n      min_article_words: 50\n      noise_frac: 0.0\n      tokenizer_name: facebook/bart-large-cnn\n    model:\n      name: facebook/bart-large-cnn\n    training:\n      epochs: 3\n      batch_size: 2\n      learning_rate: 3e-5\n      loss: ce+tempscale\n      warmup_steps: 500\n    seed: 42\n\n  - run_id: brier-finetune\n    task_type: summarization\n    dataset:\n      name: cnn_dailymail\n      version: \"3.0.0\"\n      max_source_length: 512\n      max_target_length: 128\n      min_article_words: 50\n      noise_frac: 0.0\n      tokenizer_name: facebook/bart-large-cnn\n    model:\n      name: facebook/bart-large-cnn\n    training:\n      epochs: 3\n      batch_size: 2\n      learning_rate: 3e-5\n      loss: brier\n      warmup_steps: 500\n    seed: 42\n\n  - run_id: dpsm-warm10\n    task_type: summarization\n    dataset:\n      name: cnn_dailymail\n      version: \"3.0.0\"\n      max_source_length: 512\n      max_target_length: 128\n      min_article_words: 50\n      noise_frac: 0.0\n      tokenizer_name: facebook/bart-large-cnn\n    model:\n      name: facebook/bart-large-cnn\n    training:\n      epochs: 3\n      batch_size: 2\n      learning_rate: 3e-5\n      loss: dpsm\n      warmup_steps: 1000        # 10% of approx. 10k steps in full training\n      schedule: linear\n    seed: 42\n\n  - run_id: dpsm-warm20\n    task_type: summarization\n    dataset:\n      name: cnn_dailymail\n      version: \"3.0.0\"\n      max_source_length: 512\n      max_target_length: 128\n      min_article_words: 50\n      noise_frac: 0.0\n      tokenizer_name: facebook/bart-large-cnn\n    model:\n      name: facebook/bart-large-cnn\n    training:\n      epochs: 3\n      batch_size: 2\n      learning_rate: 3e-5\n      loss: dpsm\n      warmup_steps: 2000        # 20% warm-up\n      schedule: linear\n    seed: 42\n\n  # You can append variants for robustness (e.g., noise_frac=0.15) or OOD evaluation as needed.\n  # The above 5 runs constitute the core comparison grid requested.\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n", "main_py": "#!/usr/bin/env python\n\"\"\"src/main.py\nMaster orchestrator.  \nUsage:\n  uv run python -m src.main --smoke-test  --results-dir \u003cpath\u003e\n  uv run python -m src.main --full-experiment --results-dir \u003cpath\u003e\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport shutil\nimport subprocess\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List, Any\n\nimport yaml\n\n# --------------------------------------------------------------------------------------\n# IO helpers\n# --------------------------------------------------------------------------------------\n\ndef _tee_stream(stream, log_file):\n    \"\"\"Forward *stream* (stdout/stderr of subprocess) to both terminal and file.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        sys.stdout.buffer.write(line)\n        log_file.buffer.write(line)\n        sys.stdout.flush()\n        log_file.flush()\n\n\ndef _run_subprocess(cmd: List[str], env: Dict[str, str], stdout_path: Path, stderr_path: Path):\n    with stdout_path.open(\"wb\") as out_f, stderr_path.open(\"wb\") as err_f:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n        # Real-time tee\n        import threading\n\n        t_out = threading.Thread(target=_tee_stream, args=(proc.stdout, out_f))\n        t_err = threading.Thread(target=_tee_stream, args=(proc.stderr, err_f))\n        t_out.start(); t_err.start()\n        proc.wait()\n        t_out.join(); t_err.join()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Subprocess failed with code {proc.returncode}: {\u0027 \u0027.join(cmd)}\")\n\n\n# --------------------------------------------------------------------------------------\n# Main\n# --------------------------------------------------------------------------------------\n\ndef main() -\u003e None:\n    parser = argparse.ArgumentParser(description=\"Run all experiment variations sequentially\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\")\n    parser.add_argument(\"--results-dir\", required=True, type=str, help=\"Directory to store all outputs\")\n    args = parser.parse_args()\n\n    cfg_path = Path(\"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\")\n    with cfg_path.open() as f:\n        master_cfg = yaml.safe_load(f)\n\n    experiments: List[Dict[str, Any]] = master_cfg[\"experiments\"]\n    results_root = Path(args.results_dir)\n    if results_root.exists():\n        # Allow re-runs: remove previous contents\n        shutil.rmtree(results_root)\n    results_root.mkdir(parents=True)\n\n    # ------------------------------------------------------------------\n    # Run sequentially\n    # ------------------------------------------------------------------\n    for exp in experiments:\n        run_id = exp[\"run_id\"]\n        run_dir = results_root / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n        # Dump per-run config (visible to train.py only)\n        run_cfg_path = run_dir / \"config.yaml\"\n        with run_cfg_path.open(\"w\") as f:\n            yaml.safe_dump(exp, f)\n\n        # Subprocess call\n        cmd = [\n            sys.executable,\n            \"-m\",\n            \"src.train\",\n            \"--config\",\n            str(run_cfg_path),\n            \"--results-dir\",\n            str(results_root),\n        ]\n        env = os.environ.copy()\n        _run_subprocess(cmd, env, run_dir / \"stdout.log\", run_dir / \"stderr.log\")\n\n    # ------------------------------------------------------------------\n    # After all runs \u2013 aggregate \u0026 evaluate\n    # ------------------------------------------------------------------\n    eval_cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.evaluate\",\n        \"--results-dir\",\n        str(results_root),\n    ]\n    _run_subprocess(eval_cmd, os.environ.copy(), results_root / \"evaluate_stdout.log\", results_root / \"evaluate_stderr.log\")\n\n    print(\"All experiments completed successfully.\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "#!/usr/bin/env python\n\"\"\"src/model.py\nModel architectures \u0026 loss functions (fully implemented).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom typing import Dict, Union\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\ntry:\n    from transformers import AutoModelForSeq2SeqLM\nexcept ImportError:  # pragma: no cover \u2013 handled via optional deps\n    AutoModelForSeq2SeqLM = None  # type: ignore\n\n# --------------------------------------------------------------------------------------\n# Calibration metric\n# --------------------------------------------------------------------------------------\n\ndef expected_calibration_error(probs: torch.Tensor, labels: torch.Tensor, num_bins: int = 10) -\u003e torch.Tensor:\n    confidences, predictions = probs.max(dim=1)\n    accuracies = predictions.eq(labels)\n\n    ece = torch.zeros(1, device=probs.device)\n    bin_boundaries = torch.linspace(0, 1, num_bins + 1, device=probs.device)\n    for i in range(num_bins):\n        lo, hi = bin_boundaries[i], bin_boundaries[i + 1]\n        mask = (confidences \u003e lo) \u0026 (confidences \u003c= hi)\n        if mask.any():\n            bin_acc = accuracies[mask].float().mean()\n            bin_conf = confidences[mask].mean()\n            ece += (mask.float().mean()) * torch.abs(bin_conf - bin_acc)\n    return ece\n\n\n# --------------------------------------------------------------------------------------\n# Dummy language model (used for smoke tests)\n# --------------------------------------------------------------------------------------\n\nclass DummyLanguageModel(nn.Module):\n    \"\"\"A tiny LSTM-based LM supporting *any* vocab size.\"\"\"\n\n    def __init__(self, vocab_size: int, embedding_dim: int = 64, hidden_dim: int = 128, num_layers: int = 1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n        self.proj = nn.Linear(hidden_dim, vocab_size)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            nn.init.normal_(module.weight, mean=0, std=0.02)\n\n    def forward(self, input_ids: torch.Tensor):\n        x = self.embed(input_ids)\n        out, _ = self.lstm(x)\n        return self.proj(out)\n\n\n# --------------------------------------------------------------------------------------\n# HuggingFace Seq2Seq wrapper\n# --------------------------------------------------------------------------------------\n\nclass Seq2SeqModelWrapper(nn.Module):\n    \"\"\"Thin wrapper around HuggingFace AutoModelForSeq2SeqLM that exposes logits.\"\"\"\n\n    def __init__(self, pretrained_name: str):\n        super().__init__()\n        if AutoModelForSeq2SeqLM is None:\n            raise ImportError(\"transformers must be installed to use Seq2Seq models\")\n        self.model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_name)\n\n    def forward(self, inputs):  # type: ignore[override]\n        outputs = self.model(\n            input_ids=inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n            decoder_input_ids=inputs[\"decoder_input_ids\"],\n            use_cache=False,\n        )\n        return outputs.logits\n\n\n# --------------------------------------------------------------------------------------\n# Loss functions\n# --------------------------------------------------------------------------------------\n\nclass CrossEntropyLoss(nn.Module):\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n        return F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n\n\nclass BrierLoss(nn.Module):\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n        p = logits.softmax(-1)\n        oh = F.one_hot(targets, logits.size(-1)).type_as(p)\n        return (p - oh).pow(2).sum(-1).mean()\n\n\nclass DPSMLoss(nn.Module):\n    \"\"\"Dynamic Proper-Score Mixing (CE \u2194 Brier).\"\"\"\n\n    def __init__(self, vocab_size: int, warmup_steps: int = 1000, schedule: str = \"linear\"):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.warmup_steps = warmup_steps\n        self.schedule = schedule.lower()\n        self.register_buffer(\"global_step\", torch.tensor(0.0))\n\n    def _alpha(self):\n        x = torch.clamp(self.global_step / self.warmup_steps, 0.0, 1.0)\n        if self.schedule == \"linear\":\n            return x\n        elif self.schedule == \"cosine\":\n            return 0.5 * (1 - torch.cos(math.pi * x))\n        else:\n            raise ValueError(f\"Unknown schedule \u0027{self.schedule}\u0027\")\n\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n        self.global_step += 1.0\n        ce = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), reduction=\"none\")\n        p = logits.softmax(-1)\n        oh = F.one_hot(targets, logits.size(-1)).type_as(p)\n        brier = (p - oh).pow(2).sum(-1)\n        alpha = self._alpha()\n        loss = (1 - alpha) * ce + alpha * brier\n        return loss.mean()\n\n\n# --------------------------------------------------------------------------------------\n# Factories (public API)\n# --------------------------------------------------------------------------------------\n\ndef get_model(cfg: Dict, vocab_size: int) -\u003e nn.Module:\n    name = cfg[\"model\"][\"name\"].lower()\n    if name == \"dummy\":\n        return DummyLanguageModel(\n            vocab_size=vocab_size,\n            embedding_dim=cfg[\"model\"].get(\"embedding_dim\", 64),\n            hidden_dim=cfg[\"model\"].get(\"hidden_dim\", 128),\n            num_layers=cfg[\"model\"].get(\"num_layers\", 1),\n        )\n    else:\n        # Assume HuggingFace identifier\n        return Seq2SeqModelWrapper(pretrained_name=cfg[\"model\"][\"name\"])\n\n\ndef get_loss_fn(cfg: Dict, vocab_size: int, device: torch.device):\n    loss_name = cfg[\"training\"][\"loss\"].lower()\n    if loss_name in {\"ce\", \"cross_entropy\", \"ce+tempscale\", \"ce_tempscale\"}:\n        return CrossEntropyLoss().to(device)\n    elif loss_name == \"brier\":\n        return BrierLoss().to(device)\n    elif loss_name == \"dpsm\":\n        warmup = cfg[\"training\"].get(\"warmup_steps\", 1000)\n        schedule = cfg[\"training\"].get(\"schedule\", \"linear\")\n        return DPSMLoss(vocab_size=vocab_size, warmup_steps=warmup, schedule=schedule).to(device)\n    else:\n        raise NotImplementedError(f\"Loss \u0027{loss_name}\u0027 not implemented.\")\n", "preprocess_py": "#!/usr/bin/env python\n\"\"\"src/preprocess.py\nDataset loading \u0026 preprocessing utilities (fully implemented).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport random\nfrom functools import partial\nfrom pathlib import Path\nfrom typing import Dict, Tuple, List, Any\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\n\n# Optional heavy dependencies \u2013 import lazily\ntry:\n    from datasets import load_dataset\n    from transformers import AutoTokenizer\nexcept ImportError:  # pragma: no cover \u2013 handled via project optional deps\n    load_dataset = None  # type: ignore\n    AutoTokenizer = None  # type: ignore\n\n# --------------------------------------------------------------------------------------\n# Dummy dataset (used for smoke tests)\n# --------------------------------------------------------------------------------------\n\n\nclass DummyLanguageModelingDataset(Dataset):\n    \"\"\"Creates random token sequences for next-token prediction.\"\"\"\n\n    def __init__(self, num_samples: int, seq_length: int, vocab_size: int):\n        super().__init__()\n        self.num_samples = num_samples\n        self.seq_length = seq_length\n        self.vocab_size = vocab_size\n        rng = random.Random(0)\n        self.data = [\n            torch.tensor([rng.randint(1, vocab_size - 1) for _ in range(seq_length)])\n            for _ in range(num_samples)\n        ]\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        x = self.data[idx]\n        return x[:-1], x[1:]\n\n\n# --------------------------------------------------------------------------------------\n# CNN/DailyMail summarisation dataset\n# --------------------------------------------------------------------------------------\n\n\ndef _shift_right(labels: torch.Tensor, pad_id: int, start_id: int) -\u003e torch.Tensor:\n    shifted = labels.new_full(labels.shape, pad_id)\n    shifted[..., 1:] = labels[..., :-1]\n    shifted[..., 0] = start_id\n    return shifted\n\n\nclass CNNDailyMailDataset(Dataset):\n    \"\"\"Pre-tokenised CNN/DailyMail split (train/val).\"\"\"\n\n    def __init__(\n        self,\n        split: str,\n        tokenizer_name: str,\n        version: str = \"3.0.0\",\n        max_source_length: int = 512,\n        max_target_length: int = 128,\n        min_article_words: int = 50,\n        noise_frac: float = 0.0,\n    ) -\u003e None:\n        if load_dataset is None:\n            raise ImportError(\"datasets \u0026 transformers must be installed for real datasets\")\n\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n        # Load HF dataset\n        ds = load_dataset(\"cnn_dailymail\", version, split=split)\n        # Filter short articles\n        ds = ds.filter(lambda x: len(x[\"article\"].split()) \u003e= min_article_words)\n        self.ds = ds\n        self.max_source_length = max_source_length\n        self.max_target_length = max_target_length\n        self.noise_frac = noise_frac\n        self.pad_id = self.tokenizer.pad_token_id\n        # decoder_start_token_id fallbacks to BOS if not defined\n        self.start_id = (\n            self.tokenizer.convert_tokens_to_ids(self.tokenizer.bos_token)\n            if self.tokenizer.bos_token_id is None\n            else self.tokenizer.bos_token_id\n        )\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx: int):\n        example = self.ds[int(idx)]\n        article = example[\"article\"]\n        summary = example[\"highlights\"]\n\n        # Optional noise injection: sample random summary from within the batch\n        if self.noise_frac \u003e 0 and random.random() \u003c self.noise_frac:\n            rand_idx = random.randint(0, len(self.ds) - 1)\n            summary = self.ds[rand_idx][\"highlights\"]\n\n        tok = self.tokenizer\n        model_inputs = tok(\n            article,\n            max_length=self.max_source_length,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n        with tok.as_target_tokenizer():\n            labels_enc = tok(\n                summary,\n                max_length=self.max_target_length,\n                truncation=True,\n                padding=\"max_length\",\n                return_tensors=\"pt\",\n            )\n        labels = labels_enc[\"input_ids\"].squeeze(0)\n        decoder_input_ids = _shift_right(labels, self.pad_id, self.start_id)\n        model_inputs = {\n            \"input_ids\": model_inputs[\"input_ids\"].squeeze(0),\n            \"attention_mask\": model_inputs[\"attention_mask\"].squeeze(0),\n            \"decoder_input_ids\": decoder_input_ids,\n        }\n        return model_inputs, labels\n\n\n# --------------------------------------------------------------------------------------\n# Public API\n# --------------------------------------------------------------------------------------\n\ndef load_dataset(cfg: Dict, split_ratio: float = 0.9) -\u003e Tuple[DataLoader, DataLoader, int]:\n    \"\"\"Returns (train_loader, val_loader, vocab_size).\"\"\"\n\n    dscfg = cfg[\"dataset\"]\n    batch_size = cfg[\"training\"][\"batch_size\"]\n    task_type = cfg[\"task_type\"].lower()\n\n    # ------------------------------------------------------------------\n    # Language modelling dummy task (used for CI / smoke)\n    # ------------------------------------------------------------------\n    if dscfg[\"name\"] == \"dummy\":\n        dataset = DummyLanguageModelingDataset(\n            num_samples=dscfg.get(\"num_samples\", 1024),\n            seq_length=dscfg.get(\"seq_length\", 32),\n            vocab_size=dscfg.get(\"vocab_size\", 256),\n        )\n        vocab_size = dscfg.get(\"vocab_size\", 256)\n        n_train = int(len(dataset) * split_ratio)\n        n_val = len(dataset) - n_train\n        train_set, val_set = torch.utils.data.random_split(dataset, [n_train, n_val])\n\n        def collate(batch):\n            inputs, targets = zip(*batch)\n            return torch.stack(inputs), torch.stack(targets)\n\n    # ------------------------------------------------------------------\n    # CNN/DailyMail summarisation (seq2seq)\n    # ------------------------------------------------------------------\n    elif dscfg[\"name\"].lower() == \"cnn_dailymail\":\n        tokenizer_name = dscfg.get(\"tokenizer_name\", cfg[\"model\"][\"name\"])\n        train_set = CNNDailyMailDataset(\n            split=\"train\",\n            tokenizer_name=tokenizer_name,\n            version=dscfg.get(\"version\", \"3.0.0\"),\n            max_source_length=dscfg.get(\"max_source_length\", 512),\n            max_target_length=dscfg.get(\"max_target_length\", 128),\n            min_article_words=dscfg.get(\"min_article_words\", 50),\n            noise_frac=dscfg.get(\"noise_frac\", 0.0),\n        )\n        val_set = CNNDailyMailDataset(\n            split=\"validation\",\n            tokenizer_name=tokenizer_name,\n            version=dscfg.get(\"version\", \"3.0.0\"),\n            max_source_length=dscfg.get(\"max_source_length\", 512),\n            max_target_length=dscfg.get(\"max_target_length\", 128),\n            min_article_words=dscfg.get(\"min_article_words\", 50),\n            noise_frac=0.0,  # no noise for validation\n        )\n        vocab_size = train_set.tokenizer.vocab_size\n\n        def collate(batch: List[Any]):\n            inputs_list, labels_list = zip(*batch)\n            batch_inputs = {k: torch.stack([d[k] for d in inputs_list]) for k in inputs_list[0].keys()}\n            batch_labels = torch.stack(labels_list)\n            return batch_inputs, batch_labels\n\n    else:\n        raise NotImplementedError(f\"Dataset \u0027{dscfg[\u0027name\u0027]}\u0027 not implemented.\")\n\n    # ------------------------------------------------------------------\n    # DataLoaders\n    # ------------------------------------------------------------------\n    train_loader = DataLoader(\n        train_set,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True,\n        collate_fn=collate,\n    )\n    val_loader = DataLoader(\n        val_set,\n        batch_size=batch_size,\n        shuffle=False,\n        drop_last=False,\n        collate_fn=collate,\n    )\n\n    return train_loader, val_loader, vocab_size\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=65.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"dpsm-core\"\nversion = \"0.1.0\"\ndescription = \"Common Core Foundation for DPSM experiments\"\nrequires-python = \"\u003e=3.9\"\nlicense = {text = \"Apache-2.0\"}\nauthors = [\n    {name = \"OpenAI Research Automation\", email = \"noreply@example.com\"}\n]\n\n[project.dependencies]\ntorch = \"*\"\ntqdm = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\npandas = \"*\"\ntransformers = \"*\"\ndatasets = \"*\"\nsentencepiece = \"*\"\nrouge-score = \"*\"\n", "smoke_test_yaml": "# config/smoke_test.yaml\n# Lightweight smoke test replicating all run variations on a tiny synthetic dataset.\n\nexperiments:\n  - run_id: ce-finetune_smoke\n    task_type: language_modeling\n    dataset:\n      name: dummy\n      vocab_size: 128\n      seq_length: 32\n      num_samples: 256\n    model:\n      name: dummy\n      vocab_size: 128\n      embedding_dim: 64\n      hidden_dim: 128\n    training:\n      epochs: 1\n      batch_size: 8\n      learning_rate: 1e-3\n      loss: ce\n    seed: 42\n\n  - run_id: ce+tempscale_smoke\n    task_type: language_modeling\n    dataset:\n      name: dummy\n      vocab_size: 128\n      seq_length: 32\n      num_samples: 256\n    model:\n      name: dummy\n      vocab_size: 128\n      embedding_dim: 64\n      hidden_dim: 128\n    training:\n      epochs: 1\n      batch_size: 8\n      learning_rate: 1e-3\n      loss: ce+tempscale\n    seed: 42\n\n  - run_id: brier-finetune_smoke\n    task_type: language_modeling\n    dataset:\n      name: dummy\n      vocab_size: 128\n      seq_length: 32\n      num_samples: 256\n    model:\n      name: dummy\n      vocab_size: 128\n      embedding_dim: 64\n      hidden_dim: 128\n    training:\n      epochs: 1\n      batch_size: 8\n      learning_rate: 1e-3\n      loss: brier\n    seed: 42\n\n  - run_id: dpsm-warm10_smoke\n    task_type: language_modeling\n    dataset:\n      name: dummy\n      vocab_size: 128\n      seq_length: 32\n      num_samples: 256\n    model:\n      name: dummy\n      vocab_size: 128\n      embedding_dim: 64\n      hidden_dim: 128\n    training:\n      epochs: 1\n      batch_size: 8\n      learning_rate: 1e-3\n      loss: dpsm\n      warmup_steps: 5\n      schedule: linear\n    seed: 42\n\n  - run_id: dpsm-warm20_smoke\n    task_type: language_modeling\n    dataset:\n      name: dummy\n      vocab_size: 128\n      seq_length: 32\n      num_samples: 256\n    model:\n      name: dummy\n      vocab_size: 128\n      embedding_dim: 64\n      hidden_dim: 128\n    training:\n      epochs: 1\n      batch_size: 8\n      learning_rate: 1e-3\n      loss: dpsm\n      warmup_steps: 10\n      schedule: linear\n    seed: 42\n", "train_py": "#!/usr/bin/env python\n\"\"\"src/train.py\nRuns a single experiment variation.\nSee module-level docstring in the common base for the STDOUT protocol.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom .preprocess import load_dataset\nfrom .model import (\n    get_model,\n    get_loss_fn,\n    expected_calibration_error,\n)\nfrom .utils import set_seed, save_pdf\n\n# --------------------------------------------------------------------------------------\n# Helper functions\n# --------------------------------------------------------------------------------------\n\ndef _describe_experiment(cfg: Dict[str, Any]) -\u003e str:\n    ds = cfg[\"dataset\"][\"name\"]\n    model = cfg[\"model\"][\"name\"]\n    loss = cfg[\"training\"][\"loss\"]\n    epochs = cfg[\"training\"][\"epochs\"]\n    bs = cfg[\"training\"][\"batch_size\"]\n    return (\n        f\"Running experiment \u0027{cfg[\u0027run_id\u0027]}\u0027\\n\"\n        f\"  \u2022 Task       : {cfg[\u0027task_type\u0027]}\\n\"\n        f\"  \u2022 Dataset    : {ds}\\n\"\n        f\"  \u2022 Model      : {model}\\n\"\n        f\"  \u2022 Loss       : {loss}\\n\"\n        f\"  \u2022 Epochs     : {epochs}\\n\"\n        f\"  \u2022 Batch size : {bs}\\n\"\n    )\n\n\n# --------------------------------------------------------------------------------------\n# Utility helpers for dict-aware tensor handling\n# --------------------------------------------------------------------------------------\n\ndef _move_inputs_to_device(inputs: Union[torch.Tensor, Dict[str, torch.Tensor]], device: torch.device):\n    if isinstance(inputs, dict):\n        return {k: v.to(device) for k, v in inputs.items()}\n    return inputs.to(device)\n\n\n# --------------------------------------------------------------------------------------\n# Temperature scaling (for CE+TempScale baseline)\n# --------------------------------------------------------------------------------------\n\nclass TemperatureScaledModel(nn.Module):\n    \"\"\"Wraps any model and divides its logits by a learned temperature.\"\"\"\n\n    def __init__(self, model: nn.Module, temperature: torch.Tensor):\n        super().__init__()\n        self.model = model\n        self.temperature = nn.Parameter(temperature)\n\n    def forward(self, inputs):  # type: ignore[override]\n        logits = self.model(inputs)\n        return logits / self.temperature\n\n\ndef _tune_temperature(model: nn.Module, val_loader: DataLoader, device: torch.device) -\u003e torch.Tensor:\n    \"\"\"Optimise a single scalar temperature on the validation set.\"\"\"\n\n    model.eval()\n    logits_list: List[torch.Tensor] = []\n    targets_list: List[torch.Tensor] = []\n\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs = _move_inputs_to_device(inputs, device)\n            targets = targets.to(device)\n            logits = model(inputs)\n            logits_list.append(logits.view(-1, logits.size(-1)))\n            targets_list.append(targets.view(-1))\n\n    logits_all = torch.cat(logits_list, dim=0)\n    targets_all = torch.cat(targets_list, dim=0)\n\n    temperature = torch.nn.Parameter(torch.ones([], device=device) * 1.5)\n    optimizer = torch.optim.LBFGS([temperature], max_iter=50, line_search_fn=\"strong_wolfe\")\n    nll_criterion = torch.nn.CrossEntropyLoss()\n\n    def _eval():\n        optimizer.zero_grad()\n        loss = nll_criterion(logits_all / temperature, targets_all)\n        loss.backward()\n        return loss\n\n    optimizer.step(_eval)\n    return temperature.detach()\n\n\n# --------------------------------------------------------------------------------------\n# Training / Evaluation routines (model-agnostic)\n# --------------------------------------------------------------------------------------\n\ndef train_one_epoch(\n    model: nn.Module,\n    loss_fn: nn.Module,\n    data_loader: DataLoader,\n    optimizer: torch.optim.Optimizer,\n    device: torch.device,\n):\n    \"\"\"Train for exactly one epoch and return average training loss.\"\"\"\n\n    model.train()\n    total_loss = 0.0\n    total_tokens = 0\n\n    for inputs, targets in data_loader:\n        inputs = _move_inputs_to_device(inputs, device)\n        targets = targets.to(device)\n\n        optimizer.zero_grad(set_to_none=True)\n        logits = model(inputs)\n        loss = loss_fn(logits, targets)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * targets.numel()\n        total_tokens += targets.numel()\n\n    return total_loss / max(total_tokens, 1)\n\n\ndef evaluate(\n    model: nn.Module,\n    loss_fn: nn.Module,\n    data_loader: DataLoader,\n    device: torch.device,\n) -\u003e Tuple[float, float]:\n    \"\"\"Return perplexity \u0026 ECE on the supplied validation / test split.\"\"\"\n\n    model.eval()\n    all_logits: List[torch.Tensor] = []\n    all_targets: List[torch.Tensor] = []\n    with torch.no_grad():\n        for inputs, targets in data_loader:\n            inputs = _move_inputs_to_device(inputs, device)\n            targets = targets.to(device)\n            logits = model(inputs)\n            _ = loss_fn(logits, targets)  # keep internal step counters consistent\n            all_logits.append(logits.detach())\n            all_targets.append(targets.detach())\n\n    logits = torch.cat(all_logits, dim=0)\n    targets = torch.cat(all_targets, dim=0)\n\n    ppl = torch.exp(\n        F.cross_entropy(\n            logits.view(-1, logits.size(-1)), targets.view(-1), reduction=\"mean\"\n        )\n    ).item()\n\n    probs = logits.softmax(-1).view(-1, logits.size(-1))\n    labels = targets.view(-1)\n    ece = expected_calibration_error(probs, labels, num_bins=10).item()\n\n    return ppl, ece\n\n\n# --------------------------------------------------------------------------------------\n# Main entry point\n# --------------------------------------------------------------------------------------\n\ndef main() -\u003e None:\n    parser = argparse.ArgumentParser(description=\"Train a single experimental run\")\n    parser.add_argument(\"--config\", type=str, required=True, help=\"Path to run config YAML\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory to save run-specific outputs\")\n    args = parser.parse_args()\n\n    import yaml  # local import to keep start-up time minimal\n\n    cfg: Dict[str, Any] = yaml.safe_load(Path(args.config).read_text())\n    run_id: str = cfg[\"run_id\"]\n    results_dir = Path(args.results_dir)\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n    images_dir = run_dir / \"images\"\n    images_dir.mkdir(exist_ok=True, parents=True)\n\n    # ------------------------------------------------------------------\n    # Reproducibility \u0026 device\n    # ------------------------------------------------------------------\n    seed = int(cfg.get(\"seed\", 42))\n    set_seed(seed)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ------------------------------------------------------------------\n    # Data\n    # ------------------------------------------------------------------\n    train_loader, val_loader, vocab_size = load_dataset(cfg)\n\n    # ------------------------------------------------------------------\n    # Model \u0026 Loss\n    # ------------------------------------------------------------------\n    model = get_model(cfg, vocab_size=vocab_size).to(device)\n    loss_fn = get_loss_fn(cfg, vocab_size=vocab_size, device=device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg[\"training\"][\"learning_rate\"])\n\n    # ------------------------------------------------------------------\n    # Training loop\n    # ------------------------------------------------------------------\n    num_epochs = int(cfg[\"training\"][\"epochs\"])\n    epoch_metrics: List[Dict[str, float]] = []\n    start_time = time.time()\n\n    print(_describe_experiment(cfg), flush=True)\n\n    for epoch in range(1, num_epochs + 1):\n        train_loss = train_one_epoch(model, loss_fn, train_loader, optimizer, device)\n        val_ppl, val_ece = evaluate(model, loss_fn, val_loader, device)\n\n        epoch_metrics.append(\n            {\n                \"epoch\": epoch,\n                \"train_loss\": train_loss,\n                \"val_ppl\": val_ppl,\n                \"val_ece\": val_ece,\n            }\n        )\n\n    # ------------------------------------------------------------------\n    # Temperature scaling baseline (post-hoc calibration)\n    # ------------------------------------------------------------------\n    loss_variant = cfg[\"training\"][\"loss\"].lower()\n    if loss_variant in {\"ce+tempscale\", \"ce_tempscale\"}:\n        temperature = _tune_temperature(model, val_loader, device)\n        model = TemperatureScaledModel(model, temperature).to(device)\n        val_ppl, val_ece = evaluate(model, loss_fn, val_loader, device)\n        # Overwrite / append final metrics\n        epoch_metrics[-1][\"val_ppl\"] = val_ppl\n        epoch_metrics[-1][\"val_ece\"] = val_ece\n        epoch_metrics[-1][\"temperature\"] = float(temperature.cpu())\n\n    wall_clock = time.time() - start_time\n\n    # ------------------------------------------------------------------\n    # Save artifacts\n    # ------------------------------------------------------------------\n    torch.save(model.state_dict(), run_dir / \"model.pt\")\n    with (run_dir / \"epoch_metrics.json\").open(\"w\") as f:\n        json.dump(epoch_metrics, f, indent=2)\n\n    # ----- Figures -----\n    import matplotlib.pyplot as plt\n\n    epochs = [m[\"epoch\"] for m in epoch_metrics]\n    losses = [m[\"train_loss\"] for m in epoch_metrics]\n    ppls = [m[\"val_ppl\"] for m in epoch_metrics]\n    eces = [m[\"val_ece\"] for m in epoch_metrics]\n\n    # Training loss curve\n    plt.figure(figsize=(6, 4))\n    plt.plot(epochs, losses, label=\"Train loss\")\n    plt.scatter(epochs[-1], losses[-1], color=\"red\")\n    plt.text(epochs[-1], losses[-1], f\"{losses[-1]:.2f}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training Loss Curve\")\n    plt.legend()\n    plt.tight_layout()\n    save_pdf(plt, images_dir / \"training_loss.pdf\")\n\n    # Validation PPL \u0026 ECE (twin axes)\n    fig, ax1 = plt.subplots(figsize=(6, 4))\n    ax2 = ax1.twinx()\n    ax1.plot(epochs, ppls, color=\"blue\", label=\"PPL\")\n    ax2.plot(epochs, eces, color=\"orange\", label=\"ECE\")\n    ax1.set_xlabel(\"Epoch\")\n    ax1.set_ylabel(\"Perplexity\", color=\"blue\")\n    ax2.set_ylabel(\"ECE\", color=\"orange\")\n\n    # annotate final values\n    ax1.scatter(epochs[-1], ppls[-1], color=\"blue\")\n    ax1.text(epochs[-1], ppls[-1], f\"{ppls[-1]:.2f}\")\n    ax2.scatter(epochs[-1], eces[-1], color=\"orange\")\n    ax2.text(epochs[-1], eces[-1], f\"{eces[-1]:.3f}\")\n\n    fig.suptitle(\"Validation Metrics\")\n    fig.tight_layout()\n    save_pdf(plt, images_dir / \"validation_metrics.pdf\")\n\n    # ------------------------------------------------------------------\n    # Print final metrics to STDOUT (machine-readable part)\n    # ------------------------------------------------------------------\n    final_payload = {\n        \"run_id\": run_id,\n        \"epoch_metrics\": epoch_metrics,\n        \"final\": {\n            \"val_ppl\": ppls[-1],\n            \"val_ece\": eces[-1],\n            \"wall_clock\": wall_clock,\n        },\n    }\n\n    print(json.dumps(final_payload), flush=True)\n\n\nif __name__ == \"__main__\":\n    main()\n"}


Compare the Base Code with the Derived Experiment Code for this specific experiment thoroughly. Ensure the derived code maintains all the quality, completeness, and functionality of the base code while properly replacing placeholders with specific implementations.
Output:
{
    "is_experiment_code_ready": false,
    "experiment_code_issue": "The five required run variations (ce-finetune, ce+tempscale, brier-finetune, dpsm-warm10, dpsm-warm20) must be present – with identical run_id strings – in BOTH config/full_experiment.yaml and config/smoke_test.yaml.  The current smoke-test file appends the suffix “_smoke” to every run_id, therefore the variations are not strictly matched across the two configs and the placeholder-replacement criterion (#2) is violated.  Rename the run_id entries in config/smoke_test.yaml to exactly mirror those in config/full_experiment.yaml (or vice-versa) to satisfy the specification."
}
