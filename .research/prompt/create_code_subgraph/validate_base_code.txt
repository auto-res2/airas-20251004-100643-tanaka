
Input:
You are an AI code reviewer specializing in validating base experiment foundations.

Analyze the provided CORE experiment code (which contains placeholders for datasets/models) and determine if it implements a solid foundation that follows the base code generation requirements.

# Instructions

## Core Validation Criteria
Check if the generated base code meets ALL of the following requirements:

1. **Complete Core Logic Implementation**:
   - Training loops are fully implemented (no placeholders in base training logic)
   - Evaluation framework is complete with proper metrics calculation
   - Model saving/loading mechanisms are implemented
   - Result visualization and figure generation is complete

2. **Proper Placeholder Strategy**:
   - Uses clear, descriptive placeholders like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`
   - Placeholders are ONLY used for dataset-specific and model-specific components
   - Core algorithm logic has NO placeholders
   - Includes comments explaining what each placeholder will be replaced with

3. **8-File Structure Compliance**:
   - Contains EXACTLY these 8 required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/smoke_test.yaml`
     * `config/full_experiment.yaml`
   - No additional utility files, helper modules, or separate components
   - All functionality is contained within the specified 8 files only

4. **Command Line Interface & Module Structure**:
   - main.py properly supports `--smoke-test` and `--full-experiment` flags with `--results-dir <path>` argument
   - main.py reads configuration YAML files and launches train.py for each run variation sequentially
   - main.py executes run variations one at a time in sequential order
   - main.py redirects each subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log` while forwarding to main stdout/stderr
   - train.py outputs JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py outputs JSON-formatted comparison results to stdout
   - Configuration YAML structure is ready to accept run variations (specific values will be added in derive_specific step)
   - Import statements are compatible with `uv run python -m src.main` execution

5. **Publication-Ready Infrastructure**:
   - Figure generation with proper formatting (PDF output to `{results_dir}/images/` directory, legends, annotations)
   - Consistent result formatting and comparison logic
   - Proper experimental description output

6. **PyTorch Framework Usage**:
   - Uses PyTorch exclusively for deep learning components
   - Proper model definition and training patterns
   - Appropriate use of existing Python libraries

7. **No Premature Specialization**:
   - Does NOT assume specific datasets or models (uses placeholders appropriately)
   - Does NOT contain real dataset loading code (should be placeholder)
   - Focuses on base algorithm and evaluation framework
   - Does NOT validate specific run_variation names (they will be provided later in derive_specific_experiments step)

## Output Format
Respond with a JSON object containing:
- `is_base_code_ready`: boolean - true if ALL base criteria are met, false otherwise
- `base_code_issue`: string - specific issues found if any criteria are not met, focusing on base foundation quality

# Current Research Method
{
    "Open Problems": "Strictly proper scoring rules such as the Brier and Spherical scores improve calibration and downstream generation quality, but: 1) they converge noticeably slower than the standard logarithmic (cross-entropy) loss when training from scratch; 2) practitioners must choose between fast learning (log-loss) and better calibration/generation (Brier/Spherical). A minimal change that preserves the speed of log-loss while inheriting the calibration benefits of Brier/Spherical is still missing.",
    "Methods": "Dynamic Proper-Score Mixing (DPSM)\n1. At every token we compute both the standard cross-entropy (CE) and the Brier loss.\n2. A single scalar weight α_t\\in[0,1] interpolates between them:\n   L_t = (1−α_t)·CE_t + α_t·Brier_t.\n3. α_t is scheduled to rise smoothly from 0 to 1 during training, e.g.\n   α_t = min(1, step / warmup_steps)  (linear)  or  α_t = 0.5·(1−cos(π·step/warmup_steps))  (cosine).\n   • Early training (α≈0): optimisation follows CE → rapid likelihood fitting.\n   • Late training (α≈1): optimisation follows Brier → improved calibration & generation quality.\n4. No extra hyper-parameters except warmup_steps; keeps the objective strictly proper throughout (convex combination of two strictly proper scores).",
    "Experimental Setup": "Dataset: WikiText-2 (small-scale) and CNN/DailyMail summarisation (mid-scale).\nModels:  \n• GPT-2-small trained from scratch on WikiText-2.  \n• BART-base fine-tuned on CNN/DailyMail.\nBaselines: (i) CE only, (ii) Brier only, (iii) DPSM (ours).\nMetrics:  \n• Perplexity (PPL) for language modelling.  \n• Expected Calibration Error (ECE, 10 bins).  \n• ROUGE-1/2/L for summarisation.\nTraining details: identical optimiser & learning-rate schedule for all runs; DPSM uses warmup_steps = 10% of total steps.",
    "Experimental Code": "import torch, torch.nn.functional as F\nclass DPSMLoss(torch.nn.Module):\n    def __init__(self, vocab_size:int, warmup_steps:int=10000, schedule:str='linear'):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.warmup = warmup_steps\n        self.schedule = schedule\n        self.register_buffer('global_step', torch.tensor(0.))\n    def _alpha(self):\n        s = self.global_step\n        if self.schedule=='linear':\n            return torch.clamp(s / self.warmup, 0., 1.)\n        # cosine schedule\n        x = torch.clamp(s / self.warmup, 0., 1.)\n        return 0.5 * (1 - torch.cos(torch.pi * x))\n    def forward(self, logits, targets):\n        self.global_step += 1\n        log_p = F.log_softmax(logits, -1)              # [B,T,V]\n        p = log_p.exp()\n        ce = F.nll_loss(log_p.view(-1,log_p.size(-1)), targets.view(-1), reduction='none')\n        one_hot = F.one_hot(targets, self.vocab_size).type_as(p)\n        brier = ((p - one_hot)**2).sum(-1).view_as(ce)\n        alpha = self._alpha()\n        loss = (1-alpha)*ce + alpha*brier\n        return loss.mean()",
    "Expected Result": "1. WikiText-2: DPSM matches CE perplexity after full training (±0.2 PPL) while reducing ECE by ~35% and converging 1.8× faster than pure Brier.\n2. CNN/DailyMail: DPSM improves ROUGE-L by ≈0.7 points over CE and ≈0.3 over Brier, with noticeably better calibration (-30% ECE).\n3. Training curves show CE-like rapid loss drop in early epochs and Brier-like calibration gains in later epochs.",
    "Expected Conclusion": "A single-line interpolation between cross-entropy and Brier scores delivers the best of both worlds: fast convergence and superior calibration/generation quality. Because it is strictly proper, easy to implement, and hyper-parameter-light (one warm-up length), DPSM can be dropped into any existing LLM training pipeline with minimal code change yet yields measurable accuracy and reliability gains."
}

# Experimental Design
## Experiment Strategy
Unified Experimental Strategy for Validating Dynamic Proper-Score Mixing (DPSM)

1. Core Hypotheses to Validate
   H1 – Performance: DPSM attains the same or better task performance (likelihood or downstream scores) as pure cross-entropy (CE).
   H2 – Calibration: DPSM matches or surpasses the calibration of pure Brier/Spherical scores.
   H3 – Convergence Speed: DPSM converges almost as fast as CE and markedly faster than Brier/Spherical.
   H4 – Efficiency: DPSM adds ≤2 % computational overhead (memory, GPU-hours) relative to CE.
   H5 – Robustness & Generalization: DPSM’s benefits hold across model sizes, tasks, datasets, optimisation settings, and random seeds.

2. Validation Dimensions & Metrics
   a. Quantitative Task Quality
      • Language modelling: Perplexity (PPL), NLL
      • Seq-to-seq (summarisation, translation, code): ROUGE/BLEU/CodeBLEU
   b. Calibration
      • ECE (10 & 100 bins), Brier score, reliability diagrams
   c. Learning Dynamics & Efficiency
      • Convergence curves (steps vs PPL/ECE) and wall-clock time
      • GPU utilisation, peak memory, throughput (tokens/s)
   d. Robustness & Generalization
      • Out-of-domain (OOD) datasets, different seeds (≥3), noisy labels (synthetic corruption)
   e. Qualitative Analysis
      • Human preference ratings on generated text, diversity (Distinct-n)

3. Comparison Grid (applied to every experiment)
   Baselines:
   • CE only (fast learning)
   • Brier only (strong calibration)
   • CE + temperature scaling post-hoc (industry standard fix)
   • Label smoothing & focal loss (alternative in-loss fixes)
   Ablations:
   • DPSM with fixed α (e.g., 0.25, 0.5, 0.75)
   • DPSM with linear vs cosine schedule
   • Warm-up length sweep (5 %, 10 %, 20 %)

4. Experimental Angles
   Angle 1 – From-scratch Training (small/medium models): proves H1–H4 under full optimisation control.
   Angle 2 – Fine-tuning (larger pretrained checkpoints): tests plug-and-play practicality and H5.
   Angle 3 – Stress Tests: OOD data, corrupted labels, low-resource splits; probes robustness.
   Angle 4 – Scaling: repeat Angle 1 at three model scales (≈125 M, 355 M, 1.3 B params) to expose trends.
   Angle 5 – Efficiency Audit: profile a fixed batch of 2 k tokens on the A100 for every loss variant.

5. Success Criteria (must satisfy simultaneously)
   • Task quality: ≤0.3 PPL gap or ≥+0.3 ROUGE/BLEU relative to CE; never worse than both CE & Brier.
   • Calibration: ≥25 % ECE reduction vs CE and within 5 % of Brier.
   • Convergence: reach 95 % of final CE performance in ≤110 % of CE wall-clock time and in ≤60 % of Brier time.
   • Efficiency: memory overhead ≤500 MB; throughput drop ≤2 %.
   • Robustness: advantages hold (same inequality signs) across ≥75 % of robustness settings and seeds.

6. Experimental Logistics
   • Hardware: single or multi-A100 nodes leveraging 80 GB VRAM and 2 TB RAM; mixed-precision (bfloat16) everywhere.
   • Repetition: 3 seeds per configuration; report mean±stdev; statistical tests (paired t-test p<0.05).
   • Tracking: use Weights & Biases for unified logging; standardised training scripts with a config flag selecting the loss.
   • Budget: cap total GPU time at 10 % above the equivalent CE-only research campaign by prioritising mid-scale runs and pruning under-performing ablations early.

7. Reporting Template (uniform across papers/sections)
   1. Learning curves (PPL & ECE) with wall-clock on x-axis
   2. Final metrics table (means, stdev, significance stars)
   3. Efficiency table (time/step, memory, tokens/s)
   4. Reliability diagrams (CE vs DPSM)
   5. Qualitative snippet comparison & human study results

This unified, multi-angle strategy ensures that every experiment—regardless of dataset, model, or training regime—jointly interrogates DPSM’s main claims: fast learning like CE, calibration like Brier, negligible overhead, and broad robustness.

# Generated Base Code Files
{"evaluate_py": "#!/usr/bin/env python\n\"\"\"src/evaluate.py\nAggregates results from multiple experimental runs (i.e. sub-directories of the\n`results_dir`) and generates comparison figures + a JSON summary printed to\nstdout.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List, Any\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nfrom .utils import save_pdf\n\n\n# --------------------------------------------------------------------------------------\n# CLI \u0026 helpers\n# --------------------------------------------------------------------------------------\n\ndef _collect_results(results_dir: Path) -\u003e pd.DataFrame:\n    rows: List[Dict[str, Any]] = []\n    for sub in results_dir.iterdir():\n        if not (sub / \"epoch_metrics.json\").exists():\n            continue  # skip non-run folders\n        with (sub / \"epoch_metrics.json\").open() as f:\n            epoch_metrics = json.load(f)\n        final = epoch_metrics[-1]\n        rows.append(\n            {\n                \"run_id\": sub.name,\n                \"val_ppl\": final[\"val_ppl\"],\n                \"val_ece\": final[\"val_ece\"],\n            }\n        )\n    if not rows:\n        raise RuntimeError(f\"No result folders found in {results_dir}\")\n    return pd.DataFrame(rows)\n\n\ndef _plot_bar(df: pd.DataFrame, metric: str, images_dir: Path):\n    plt.figure(figsize=(max(4, len(df) * 1.5), 4))\n    sns.barplot(x=\"run_id\", y=metric, data=df, palette=\"viridis\")\n    for i, v in enumerate(df[metric]):\n        plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.title(f\"Final {metric.upper()} Comparison\")\n    plt.xlabel(\"Run ID\")\n    plt.ylabel(metric.upper())\n    plt.tight_layout()\n    save_pdf(plt, images_dir / f\"{metric}_comparison.pdf\")\n\n\n# --------------------------------------------------------------------------------------\n# Main\n# --------------------------------------------------------------------------------------\n\ndef main() -\u003e None:\n    parser = argparse.ArgumentParser(description=\"Evaluate all experiment variations\")\n    parser.add_argument(\"--results-dir\", required=True, type=str, help=\"Directory with all runs\u0027 sub-folders\")\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir)\n    images_dir = results_dir / \"images\"\n    images_dir.mkdir(exist_ok=True)\n\n    df = _collect_results(results_dir)\n\n    # ----- Figures -----\n    for metric in [\"val_ppl\", \"val_ece\"]:\n        _plot_bar(df, metric, images_dir)\n\n    # ----- JSON summary -----\n    summary = df.to_dict(orient=\"list\")\n    print(json.dumps(summary, indent=None))\n\n\nif __name__ == \"__main__\":\n    main()\n", "full_experiment_yaml": "# config/full_experiment.yaml\n# TEMPLATE \u2014 will be filled with real datasets, models, and hyper-parameters in\na later pipeline stage. The placeholders *must* be replaced before running the\nfull experiment.\n\nexperiments:\n  - run_id: DATASET_PLACEHOLDER_ce\n    task_type: TASK_TYPE_PLACEHOLDER           # e.g. language_modeling | seq2seq\n    dataset:\n      name: DATASET_PLACEHOLDER                # PLACEHOLDER: specific dataset name\n      # PLACEHOLDER: add dataset-specific parameters here (paths, splits, ...)\n    model:\n      name: MODEL_PLACEHOLDER                  # PLACEHOLDER: specific model identifier\n      # PLACEHOLDER: model-specific hyper-parameters (layers, dims, ...)\n    training:\n      epochs: SPECIFIC_CONFIG_PLACEHOLDER\n      batch_size: SPECIFIC_CONFIG_PLACEHOLDER\n      learning_rate: SPECIFIC_CONFIG_PLACEHOLDER\n      loss: ce\n      warmup_steps: SPECIFIC_CONFIG_PLACEHOLDER\n    seed: SPECIFIC_CONFIG_PLACEHOLDER\n\n  - run_id: DATASET_PLACEHOLDER_dpsm\n    task_type: TASK_TYPE_PLACEHOLDER\n    dataset:\n      name: DATASET_PLACEHOLDER\n    model:\n      name: MODEL_PLACEHOLDER\n    training:\n      epochs: SPECIFIC_CONFIG_PLACEHOLDER\n      batch_size: SPECIFIC_CONFIG_PLACEHOLDER\n      learning_rate: SPECIFIC_CONFIG_PLACEHOLDER\n      loss: dpsm\n      warmup_steps: SPECIFIC_CONFIG_PLACEHOLDER\n      schedule: cosine     # example alternative schedule\n    seed: SPECIFIC_CONFIG_PLACEHOLDER\n\n  # Additional baselines / ablations can be appended following the same schema.\n", "main_py": "#!/usr/bin/env python\n\"\"\"src/main.py\nMaster orchestrator.  \nUsage:\n  uv run python -m src.main --smoke-test  --results-dir \u003cpath\u003e\n  uv run python -m src.main --full-experiment --results-dir \u003cpath\u003e\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport shutil\nimport subprocess\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List, Any\n\nimport yaml\n\n# --------------------------------------------------------------------------------------\n# IO helpers\n# --------------------------------------------------------------------------------------\n\ndef _tee_stream(stream, log_file):\n    \"\"\"Forward *stream* (stdout/stderr of subprocess) to both terminal and file.\"\"\"\n    for line in iter(stream.readline, b\"\"):\n        sys.stdout.buffer.write(line)\n        log_file.buffer.write(line)\n        sys.stdout.flush()\n        log_file.flush()\n\n\ndef _run_subprocess(cmd: List[str], env: Dict[str, str], stdout_path: Path, stderr_path: Path):\n    with stdout_path.open(\"wb\") as out_f, stderr_path.open(\"wb\") as err_f:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n        # Real-time tee\n        import threading\n\n        t_out = threading.Thread(target=_tee_stream, args=(proc.stdout, out_f))\n        t_err = threading.Thread(target=_tee_stream, args=(proc.stderr, err_f))\n        t_out.start(); t_err.start()\n        proc.wait()\n        t_out.join(); t_err.join()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Subprocess failed with code {proc.returncode}: {\u0027 \u0027.join(cmd)}\")\n\n\n# --------------------------------------------------------------------------------------\n# Main\n# --------------------------------------------------------------------------------------\n\ndef main() -\u003e None:\n    parser = argparse.ArgumentParser(description=\"Run all experiment variations sequentially\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\")\n    parser.add_argument(\"--results-dir\", required=True, type=str, help=\"Directory to store all outputs\")\n    args = parser.parse_args()\n\n    cfg_path = Path(\"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\")\n    with cfg_path.open() as f:\n        master_cfg = yaml.safe_load(f)\n\n    experiments: List[Dict[str, Any]] = master_cfg[\"experiments\"]\n    results_root = Path(args.results_dir)\n    if results_root.exists():\n        # Allow re-runs: remove previous contents\n        shutil.rmtree(results_root)\n    results_root.mkdir(parents=True)\n\n    # ------------------------------------------------------------------\n    # Run sequentially\n    # ------------------------------------------------------------------\n    for exp in experiments:\n        run_id = exp[\"run_id\"]\n        run_dir = results_root / run_id\n        run_dir.mkdir(parents=True, exist_ok=True)\n        # Dump per-run config (visible to train.py only)\n        run_cfg_path = run_dir / \"config.yaml\"\n        with run_cfg_path.open(\"w\") as f:\n            yaml.safe_dump(exp, f)\n\n        # Subprocess call\n        cmd = [\n            sys.executable,\n            \"-m\",\n            \"src.train\",\n            \"--config\",\n            str(run_cfg_path),\n            \"--results-dir\",\n            str(results_root),\n        ]\n        env = os.environ.copy()\n        _run_subprocess(cmd, env, run_dir / \"stdout.log\", run_dir / \"stderr.log\")\n\n    # ------------------------------------------------------------------\n    # After all runs \u2013 aggregate \u0026 evaluate\n    # ------------------------------------------------------------------\n    eval_cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.evaluate\",\n        \"--results-dir\",\n        str(results_root),\n    ]\n    _run_subprocess(eval_cmd, os.environ.copy(), results_root / \"evaluate_stdout.log\", results_root / \"evaluate_stderr.log\")\n\n    print(\"All experiments completed successfully.\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "#!/usr/bin/env python\n\"\"\"src/model.py\nCommon model architectures and loss functions, including the proposed DPSM loss.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom typing import Dict\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n# --------------------------------------------------------------------------------------\n# Utility functions\n# --------------------------------------------------------------------------------------\n\ndef expected_calibration_error(probs: torch.Tensor, labels: torch.Tensor, num_bins: int = 10) -\u003e torch.Tensor:\n    \"\"\"Compute Expected Calibration Error (vectorised, differentiable).\n    Args:\n        probs: [N, V] probability distribution over classes for each sample.\n        labels: [N] ground-truth indices.\n    Returns:\n        ECE scalar tensor (on same device as probs).\n    \"\"\"\n    confidences, predictions = probs.max(dim=1)\n    accuracies = predictions.eq(labels)\n\n    ece = torch.zeros(1, device=probs.device)\n    bin_boundaries = torch.linspace(0, 1, num_bins + 1, device=probs.device)\n    for i in range(num_bins):\n        lo, hi = bin_boundaries[i], bin_boundaries[i + 1]\n        mask = (confidences \u003e lo) \u0026 (confidences \u003c= hi)\n        if mask.any():\n            bin_acc = accuracies[mask].float().mean()\n            bin_conf = confidences[mask].mean()\n            ece += (mask.float().mean()) * torch.abs(bin_conf - bin_acc)\n    return ece\n\n\n# --------------------------------------------------------------------------------------\n# Dummy language model (small \u0026 dependency-free \u2013 perfect for smoke tests)\n# --------------------------------------------------------------------------------------\n\n\nclass DummyLanguageModel(nn.Module):\n    \"\"\"A tiny LSTM-based LM supporting *any* vocab size (\u22652).\"\"\"\n\n    def __init__(self, vocab_size: int, embedding_dim: int = 64, hidden_dim: int = 128, num_layers: int = 1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n        self.proj = nn.Linear(hidden_dim, vocab_size)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            nn.init.normal_(module.weight, mean=0, std=0.02)\n\n    def forward(self, input_ids: torch.Tensor):  # [B, T]\n        x = self.embed(input_ids)  # [B, T, D]\n        out, _ = self.lstm(x)\n        logits = self.proj(out)  # [B, T, V]\n        return logits\n\n\n# --------------------------------------------------------------------------------------\n# Loss functions\n# --------------------------------------------------------------------------------------\n\nclass CrossEntropyLoss(nn.Module):\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n        return F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n\n\nclass BrierLoss(nn.Module):\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n        p = logits.softmax(-1)\n        one_hot = F.one_hot(targets, logits.size(-1)).type_as(p)\n        loss = (p - one_hot).pow(2).sum(-1)\n        return loss.mean()\n\n\nclass DPSMLoss(nn.Module):\n    \"\"\"Dynamic Proper-Score Mixing (CE \u2194 Brier).\"\"\"\n\n    def __init__(self, vocab_size: int, warmup_steps: int = 1000, schedule: str = \"linear\"):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.warmup_steps = warmup_steps\n        self.schedule = schedule.lower()\n        self.register_buffer(\"global_step\", torch.tensor(0.0))\n\n    def _alpha(self):\n        x = torch.clamp(self.global_step / self.warmup_steps, 0.0, 1.0)\n        if self.schedule == \"linear\":\n            return x\n        elif self.schedule == \"cosine\":\n            return 0.5 * (1 - torch.cos(math.pi * x))\n        else:\n            raise ValueError(f\"Unknown schedule \u0027{self.schedule}\u0027\")\n\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n        self.global_step += 1.0\n        ce = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), reduction=\"none\")\n        p = logits.softmax(-1)\n        one_hot = F.one_hot(targets, logits.size(-1)).type_as(p)\n        brier = (p - one_hot).pow(2).sum(-1)\n        alpha = self._alpha()\n        loss = (1 - alpha) * ce + alpha * brier\n        return loss.mean()\n\n\n# --------------------------------------------------------------------------------------\n# Factories (public API)\n# --------------------------------------------------------------------------------------\n\ndef get_model(cfg: Dict, vocab_size: int) -\u003e nn.Module:\n    model_cfg = cfg[\"model\"]\n    name = model_cfg[\"name\"]\n    if name == \"dummy\":\n        return DummyLanguageModel(\n            vocab_size=vocab_size,\n            embedding_dim=model_cfg.get(\"embedding_dim\", 64),\n            hidden_dim=model_cfg.get(\"hidden_dim\", 128),\n            num_layers=model_cfg.get(\"num_layers\", 1),\n        )\n    else:\n        # PLACEHOLDER: Will be replaced with task- / model-specific implementation\n        raise NotImplementedError(f\"Model \u0027{name}\u0027 not implemented (placeholder)\")\n\n\ndef get_loss_fn(cfg: Dict, vocab_size: int, device: torch.device):\n    loss_name = cfg[\"training\"][\"loss\"].lower()\n    if loss_name == \"ce\":\n        return CrossEntropyLoss().to(device)\n    elif loss_name == \"brier\":\n        return BrierLoss().to(device)\n    elif loss_name == \"dpsm\":\n        schedule = cfg[\"training\"].get(\"schedule\", \"linear\")\n        warmup = cfg[\"training\"].get(\"warmup_steps\", 1000)\n        return DPSMLoss(vocab_size=vocab_size, warmup_steps=warmup, schedule=schedule).to(device)\n    else:\n        # PLACEHOLDER: Add focal loss, label smoothing, etc. in specialised phase.\n        raise NotImplementedError(f\"Loss \u0027{loss_name}\u0027 not implemented (placeholder)\")\n", "preprocess_py": "#!/usr/bin/env python\n\"\"\"src/preprocess.py\nCommon data-loading and preprocessing utilities with a **structured placeholder\nstrategy** for dataset-specific code.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport random\nfrom pathlib import Path\nfrom typing import Dict, Tuple\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\n\n# --------------------------------------------------------------------------------------\n# Dummy dataset (used for smoke tests) \u2013 always available\n# --------------------------------------------------------------------------------------\n\n\nclass DummyLanguageModelingDataset(Dataset):\n    \"\"\"Creates random token sequences for next-token prediction.\"\"\"\n\n    def __init__(self, num_samples: int, seq_length: int, vocab_size: int):\n        super().__init__()\n        self.num_samples = num_samples\n        self.seq_length = seq_length\n        self.vocab_size = vocab_size\n        # Pre-generate data for determinism \u0026 speed\n        rng = random.Random(0)\n        self.data = [\n            torch.tensor([rng.randint(1, vocab_size - 1) for _ in range(seq_length)])\n            for _ in range(num_samples)\n        ]\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        x = self.data[idx]\n        return x[:-1], x[1:]  # input, target\n\n\n# --------------------------------------------------------------------------------------\n# Public API\n# --------------------------------------------------------------------------------------\n\ndef load_dataset(cfg: Dict, split_ratio: float = 0.9) -\u003e Tuple[DataLoader, DataLoader, int]:\n    \"\"\"Returns (train_loader, val_loader, vocab_size).\n\n    All task-specific details are abstracted behind this function. REAL datasets\n    will be injected in the derive-specific phase by replacing the placeholders\n    marked below.\n    \"\"\"\n\n    task_type = cfg[\"task_type\"]\n    dscfg = cfg[\"dataset\"]\n    batch_size = cfg[\"training\"][\"batch_size\"]\n\n    # ------------------------------------------------------------------\n    # Placeholders for dataset-specific loading logic\n    # ------------------------------------------------------------------\n    if dscfg[\"name\"] == \"dummy\":\n        dataset = DummyLanguageModelingDataset(\n            num_samples=dscfg.get(\"num_samples\", 1024),\n            seq_length=dscfg.get(\"seq_length\", 32),\n            vocab_size=dscfg.get(\"vocab_size\", 256),\n        )\n        vocab_size = dscfg.get(\"vocab_size\", 256)\n    else:\n        # PLACEHOLDER: Will be replaced with specific dataset loading logic\n        raise NotImplementedError(\n            \"Dataset \u0027{}\u0027 not yet implemented (placeholder).\".format(dscfg[\"name\"])\n        )\n\n    # ------------------------------------------------------------------\n    # Split train/val\n    # ------------------------------------------------------------------\n    n_train = int(len(dataset) * split_ratio)\n    n_val = len(dataset) - n_train\n    train_set, val_set = torch.utils.data.random_split(dataset, [n_train, n_val])\n\n    # Simple collate (works for dummy LM)\n    def collate(batch):\n        inputs, targets = zip(*batch)\n        return torch.stack(inputs), torch.stack(targets)\n\n    train_loader = DataLoader(\n        train_set,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True,\n        collate_fn=collate,\n    )\n    val_loader = DataLoader(\n        val_set,\n        batch_size=batch_size,\n        shuffle=False,\n        drop_last=False,\n        collate_fn=collate,\n    )\n\n    return train_loader, val_loader, vocab_size\n", "pyproject_toml": "[build-system]\nrequires = [\"setuptools\u003e=65.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"dpsm-core\"\nversion = \"0.1.0\"\ndescription = \"Common Core Foundation for DPSM experiments\"\nrequires-python = \"\u003e=3.9\"\nlicense = {text = \"Apache-2.0\"}\nauthors = [\n    {name = \"OpenAI Research Automation\", email = \"noreply@example.com\"}\n]\n\n[project.dependencies]\ntorch = \"*\"\ntqdm = \"*\"\npyyaml = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\npandas = \"*\"\n\n# Optional (for future specialised experiments)\ntransformers = {version = \"*\", optional = true}\ndatasets = {version = \"*\", optional = true}\nrouge-score = {version = \"*\", optional = true}\n\n[project.optional-dependencies]\nhf = [\"transformers\", \"datasets\", \"sentencepiece\", \"rouge-score\"]\n", "smoke_test_yaml": "# config/smoke_test.yaml\n# Lightweight end-to-end sanity check. Runs on a tiny *synthetic* dataset so\n# that CI can finish within seconds.\n\nexperiments:\n  - run_id: dummy_ce\n    task_type: language_modeling\n    dataset:\n      name: dummy            # built-in dataset \u2013 no external downloads\n      vocab_size: 128\n      seq_length: 32\n      num_samples: 256\n    model:\n      name: dummy            # tiny LSTM defined in src/model.py\n      vocab_size: 128\n      embedding_dim: 64\n      hidden_dim: 128\n    training:\n      epochs: 1\n      batch_size: 8\n      learning_rate: 1e-3\n      loss: ce\n      warmup_steps: 10\n    seed: 42\n\n  - run_id: dummy_dpsm\n    task_type: language_modeling\n    dataset:\n      name: dummy\n      vocab_size: 128\n      seq_length: 32\n      num_samples: 256\n    model:\n      name: dummy\n      vocab_size: 128\n      embedding_dim: 64\n      hidden_dim: 128\n    training:\n      epochs: 1\n      batch_size: 8\n      learning_rate: 1e-3\n      loss: dpsm\n      warmup_steps: 10\n      schedule: linear\n    seed: 42\n", "train_py": "#!/usr/bin/env python\n\"\"\"src/train.py\nRuns a single experiment variation.  \nThis script is *invoked as a subprocess* by src/main.py so that every run has an\nisolated Python interpreter and clean GPU memory.  The CLI is intentionally\nminimal \u2013 **all run-specific information is provided through an on-disk YAML\nfile** produced by main.py.\n\nStandard-output protocol (MUST NOT CHANGE \u2013 relied upon by evaluate.py \u0026 CI)\n1. Human-readable experiment description (multi-line, free-form).\n2. A single **JSON line** with the structure documented below \u2013 this is parsed\n   by main.py \u0026 evaluate.py.\n\n{\n  \"run_id\": \"\u003cunique name from YAML\u003e\",\n  \"epoch_metrics\": [\n      {\"epoch\": 1, \"train_loss\": 4.83, \"val_ppl\": 125.1, \"val_ece\": 0.38},\n      ...\n  ],\n  \"final\":        {\"val_ppl\": 37.2, \"val_ece\": 0.09, \"wall_clock\": 713.4}\n}\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom .preprocess import load_dataset\nfrom .model import (\n    get_model,\n    get_loss_fn,\n    expected_calibration_error,\n)\nfrom .utils import set_seed, save_pdf\n\n# --------------------------------------------------------------------------------------\n# Helper functions\n# --------------------------------------------------------------------------------------\n\ndef _describe_experiment(cfg: Dict[str, Any]) -\u003e str:\n    ds = cfg[\"dataset\"][\"name\"]\n    model = cfg[\"model\"][\"name\"]\n    loss = cfg[\"training\"][\"loss\"]\n    epochs = cfg[\"training\"][\"epochs\"]\n    bs = cfg[\"training\"][\"batch_size\"]\n    return (\n        f\"Running experiment \u0027{cfg[\u0027run_id\u0027]}\u0027\\n\"\n        f\"  \u2022 Task       : {cfg[\u0027task_type\u0027]}\\n\"\n        f\"  \u2022 Dataset    : {ds}\\n\"\n        f\"  \u2022 Model      : {model}\\n\"\n        f\"  \u2022 Loss       : {loss}\\n\"\n        f\"  \u2022 Epochs     : {epochs}\\n\"\n        f\"  \u2022 Batch size : {bs}\\n\"\n    )\n\n\n# --------------------------------------------------------------------------------------\n# Training / Evaluation routines (model-agnostic)\n# --------------------------------------------------------------------------------------\n\ndef train_one_epoch(\n    model: nn.Module,\n    loss_fn: nn.Module,\n    data_loader: DataLoader,\n    optimizer: torch.optim.Optimizer,\n    device: torch.device,\n):\n    \"\"\"Train for exactly one epoch and return average training loss.\"\"\"\n\n    model.train()\n    total_loss = 0.0\n    total_tokens = 0\n\n    for batch in data_loader:\n        inputs, targets = [x.to(device) for x in batch]\n        optimizer.zero_grad(set_to_none=True)\n        logits = model(inputs)\n        loss = loss_fn(logits, targets)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * targets.numel()\n        total_tokens += targets.numel()\n\n    return total_loss / max(total_tokens, 1)\n\n\ndef evaluate(\n    model: nn.Module,\n    loss_fn: nn.Module,\n    data_loader: DataLoader,\n    device: torch.device,\n):\n    \"\"\"Return perplexity \u0026 ECE on the supplied validation / test split.\"\"\"\n\n    model.eval()\n    all_logits: List[torch.Tensor] = []\n    all_targets: List[torch.Tensor] = []\n    with torch.no_grad():\n        for batch in data_loader:\n            inputs, targets = [x.to(device) for x in batch]\n            logits = model(inputs)\n            _ = loss_fn(logits, targets)  # keep internal step counters consistent\n            all_logits.append(logits.detach())\n            all_targets.append(targets.detach())\n\n    logits = torch.cat(all_logits, dim=0)\n    targets = torch.cat(all_targets, dim=0)\n\n    ppl = torch.exp(\n        F.cross_entropy(\n            logits.view(-1, logits.size(-1)), targets.view(-1), reduction=\"mean\"\n        )\n    ).item()\n\n    probs = logits.softmax(-1).view(-1, logits.size(-1))\n    labels = targets.view(-1)\n    ece = expected_calibration_error(probs, labels, num_bins=10).item()\n\n    return ppl, ece\n\n\n# --------------------------------------------------------------------------------------\n# Main entry point\n# --------------------------------------------------------------------------------------\n\ndef main() -\u003e None:\n    parser = argparse.ArgumentParser(description=\"Train a single experimental run\")\n    parser.add_argument(\"--config\", type=str, required=True, help=\"Path to run config YAML\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory to save run-specific outputs\")\n    args = parser.parse_args()\n\n    import yaml  # local import to keep start-up time minimal\n\n    cfg: Dict[str, Any] = yaml.safe_load(Path(args.config).read_text())\n    run_id: str = cfg[\"run_id\"]\n    results_dir = Path(args.results_dir)\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n    images_dir = run_dir / \"images\"\n    images_dir.mkdir(exist_ok=True, parents=True)\n\n    # ------------------------------------------------------------------\n    # Reproducibility \u0026 device\n    # ------------------------------------------------------------------\n    seed = int(cfg.get(\"seed\", 42))\n    set_seed(seed)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ------------------------------------------------------------------\n    # Data\n    # ------------------------------------------------------------------\n    train_loader, val_loader, vocab_size = load_dataset(cfg)\n\n    # ------------------------------------------------------------------\n    # Model \u0026 Loss\n    # ------------------------------------------------------------------\n    model = get_model(cfg, vocab_size=vocab_size).to(device)\n    loss_fn = get_loss_fn(cfg, vocab_size=vocab_size, device=device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg[\"training\"][\"learning_rate\"])\n\n    # ------------------------------------------------------------------\n    # Training loop\n    # ------------------------------------------------------------------\n    num_epochs = int(cfg[\"training\"][\"epochs\"])\n    epoch_metrics: List[Dict[str, float]] = []\n    start_time = time.time()\n\n    print(_describe_experiment(cfg), flush=True)\n\n    for epoch in range(1, num_epochs + 1):\n        train_loss = train_one_epoch(model, loss_fn, train_loader, optimizer, device)\n        val_ppl, val_ece = evaluate(model, loss_fn, val_loader, device)\n\n        epoch_metrics.append(\n            {\n                \"epoch\": epoch,\n                \"train_loss\": train_loss,\n                \"val_ppl\": val_ppl,\n                \"val_ece\": val_ece,\n            }\n        )\n\n    wall_clock = time.time() - start_time\n\n    # ------------------------------------------------------------------\n    # Save artifacts\n    # ------------------------------------------------------------------\n    torch.save(model.state_dict(), run_dir / \"model.pt\")\n    with (run_dir / \"epoch_metrics.json\").open(\"w\") as f:\n        json.dump(epoch_metrics, f, indent=2)\n\n    # ----- Figures -----\n    import matplotlib.pyplot as plt\n\n    epochs = [m[\"epoch\"] for m in epoch_metrics]\n    losses = [m[\"train_loss\"] for m in epoch_metrics]\n    ppls = [m[\"val_ppl\"] for m in epoch_metrics]\n    eces = [m[\"val_ece\"] for m in epoch_metrics]\n\n    # Training loss curve\n    plt.figure(figsize=(6, 4))\n    plt.plot(epochs, losses, label=\"Train loss\")\n    plt.scatter(epochs[-1], losses[-1], color=\"red\")\n    plt.text(epochs[-1], losses[-1], f\"{losses[-1]:.2f}\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training Loss Curve\")\n    plt.legend()\n    plt.tight_layout()\n    save_pdf(plt, images_dir / \"training_loss.pdf\")\n\n    # Validation PPL \u0026 ECE (twin axes)\n    fig, ax1 = plt.subplots(figsize=(6, 4))\n    ax2 = ax1.twinx()\n    ax1.plot(epochs, ppls, color=\"blue\", label=\"PPL\")\n    ax2.plot(epochs, eces, color=\"orange\", label=\"ECE\")\n    ax1.set_xlabel(\"Epoch\")\n    ax1.set_ylabel(\"Perplexity\", color=\"blue\")\n    ax2.set_ylabel(\"ECE\", color=\"orange\")\n\n    # annotate final values\n    ax1.scatter(epochs[-1], ppls[-1], color=\"blue\")\n    ax1.text(epochs[-1], ppls[-1], f\"{ppls[-1]:.2f}\")\n    ax2.scatter(epochs[-1], eces[-1], color=\"orange\")\n    ax2.text(epochs[-1], eces[-1], f\"{eces[-1]:.3f}\")\n\n    fig.suptitle(\"Validation Metrics\")\n    fig.tight_layout()\n    save_pdf(plt, images_dir / \"validation_metrics.pdf\")\n\n    # ------------------------------------------------------------------\n    # Print final metrics to STDOUT (machine-readable part)\n    # ------------------------------------------------------------------\n    final_payload = {\n        \"run_id\": run_id,\n        \"epoch_metrics\": epoch_metrics,\n        \"final\": {\n            \"val_ppl\": ppls[-1],\n            \"val_ece\": eces[-1],\n            \"wall_clock\": wall_clock,\n        },\n    }\n\n    print(json.dumps(final_payload), flush=True)\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Analyze the Base code thoroughly, focusing on whether it provides a solid, consistent foundation for ALL future experimental variations while properly using placeholders for dataset/model-specific components.
Output:
{
    "is_base_code_ready": false,
    "base_code_issue": "Violates 8-file structure – code relies on a separate `src/utils.py` (imported in train.py & evaluate.py) which is not one of the allowed files. This extra dependency both exceeds the mandated file list and would cause runtime errors because the file is absent. All core utilities (save_pdf, set_seed, etc.) must be implemented inside the permitted files or removed."
}
